<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Day 6 &mdash; Intro to Language Modeling &mdash; The Bigram Model</title>
<link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;1,400&family=Source+Serif+4:ital,wght@0,300;0,400;0,600;1,300;1,400&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
<style>
:root{--ink:#1a1a2e;--cream:#faf8f4;--cream-dark:#f0ece4;--accent:#c0392b;--blue:#2c3e6b;--green:#27654a;--gold:#d4a843;--gray:#7f8c8d;--gray-light:#bdc3c7;--purple:#5b2c6f;--orange:#d35400;}
*{margin:0;padding:0;box-sizing:border-box;}
body{background:var(--cream);color:var(--ink);font-family:'Source Serif 4',Georgia,serif;line-height:1.7;}
.progress-strip{position:sticky;top:0;z-index:100;background:var(--ink);padding:0.6rem 2rem;display:flex;align-items:center;gap:0.4rem;font-family:'JetBrains Mono',monospace;font-size:0.65rem;color:#888;}
.p-dot{width:8px;height:8px;border-radius:50%;border:1.5px solid #555;flex-shrink:0;}
.p-dot.done{background:var(--green);border-color:var(--green);}
.p-dot.now{background:var(--accent);border-color:var(--accent);box-shadow:0 0 6px rgba(192,57,43,0.5);}
.p-line{width:24px;height:1.5px;background:#444;flex-shrink:0;}.p-line.done{background:var(--green);}
.p-label{color:#666;margin-left:0.3rem;margin-right:0.5rem;}.p-label.done{color:var(--green);}.p-label.now{color:var(--accent);font-weight:600;}
.back-link{display:block;font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:.18em;text-transform:uppercase;color:var(--gray);text-decoration:none;padding:1.2rem 2rem 0;max-width:1100px;margin:0 auto;transition:color .2s;}.back-link:hover{color:var(--accent);}
.header{padding:2rem 2rem 2rem;max-width:1100px;margin:0 auto;border-bottom:3px double var(--ink);}
.header-meta{font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:0.15em;text-transform:uppercase;color:var(--gray);margin-bottom:0.5rem;}
.phase-tag{background:var(--ink);color:white;padding:0.15rem 0.5rem;border-radius:2px;font-size:0.62rem;margin-right:0.4rem;}
.header h1{font-family:'Playfair Display',serif;font-size:clamp(2rem,5vw,3.1rem);font-weight:700;line-height:1.15;margin-bottom:0.5rem;}
.header h1 span{color:var(--accent);}
.header-sub{font-style:italic;font-size:1.05rem;color:var(--blue);max-width:700px;}
.content{max-width:1100px;margin:0 auto;padding:2rem 2rem 3rem;}
.memo-quote{background:var(--cream-dark);border-left:4px solid var(--accent);padding:1.5rem 2rem;margin:0 0 3rem;font-style:italic;font-size:1.02rem;color:var(--blue);position:relative;}
.memo-quote::before{content:'"\27';font-family:'Playfair Display',serif;font-size:4rem;color:var(--accent);position:absolute;top:-0.5rem;left:0.5rem;opacity:0.25;}
.memo-quote .attr{display:block;margin-top:0.6rem;font-style:normal;font-size:0.8rem;color:var(--gray);font-family:'JetBrains Mono',monospace;}
.sh{font-family:'Playfair Display',serif;font-size:1.55rem;font-weight:700;margin:3rem 0 1.25rem;padding-bottom:0.5rem;border-bottom:1px solid var(--gray-light);}
.sh .n{color:var(--accent);font-style:italic;margin-right:0.2rem;}
.prose{font-size:1rem;max-width:720px;margin-bottom:1.25rem;}
.prose strong{color:var(--blue);}
.prose code,code{font-family:'JetBrains Mono',monospace;font-size:0.83rem;background:var(--cream-dark);padding:0.1rem 0.35rem;border-radius:2px;color:var(--accent);}
.vf{background:white;border:1px solid var(--gray-light);border-radius:2px;padding:2rem 1.5rem 1.5rem;margin:1.5rem 0 2rem;position:relative;overflow:hidden;}
.vf::before{content:'';position:absolute;top:0;left:0;right:0;height:3px;}
.vf.green::before{background:linear-gradient(90deg,var(--green),var(--blue));}
.vf.gold::before{background:linear-gradient(90deg,var(--gold),var(--accent));}
.vf.purple::before{background:linear-gradient(90deg,var(--purple),var(--blue));}
.vf-label{font-family:'JetBrains Mono',monospace;font-size:0.68rem;letter-spacing:0.18em;text-transform:uppercase;color:var(--gray);margin-bottom:1rem;}
svg text{font-family:'Source Serif 4',Georgia,serif;}
svg .m{font-family:'JetBrains Mono',monospace;}
.box{border-radius:2px;padding:1.4rem 1.8rem;margin:2rem 0;position:relative;}
.box::before{position:absolute;top:-0.75rem;left:1rem;font-size:1.2rem;background:var(--cream);padding:0 0.5rem;}
.box h4{font-family:'Playfair Display',serif;margin-bottom:0.4rem;font-size:1rem;}
.box p{font-size:0.93rem;color:#555;}
.box.insight{background:linear-gradient(135deg,#fdf6e8,#fef9f0);border:1px solid var(--gold);}.box.insight::before{content:'\1F4A1';}.box.insight h4{color:var(--gold);}
.box.danger{background:linear-gradient(135deg,#fdf0ef,#fef5f4);border:1px solid var(--accent);}.box.danger::before{content:'\26A0\FE0F';}.box.danger h4{color:var(--accent);}
.code-block{background:#1e1e2e;color:#cdd6f4;border-radius:4px;padding:1.5rem;margin:1.5rem 0;overflow-x:auto;font-family:'JetBrains Mono',monospace;font-size:0.82rem;line-height:1.8;}
.code-block .comment{color:#6c7086;}.code-block .keyword{color:#cba6f7;}.code-block .string{color:#a6e3a1;}.code-block .number{color:#fab387;}.code-block .func{color:#89b4fa;}.code-block .class-name{color:#f9e2af;}.code-block .op{color:#89dceb;}.code-block .self{color:#f38ba8;}
.mx{display:grid;grid-template-columns:auto 1fr 1fr;grid-template-rows:auto 1fr 1fr;gap:0;margin:2rem 0;background:white;border:1px solid var(--gray-light);border-radius:2px;overflow:hidden;}
.mx-corner{background:var(--ink);padding:0.8rem;}
.mx-ch{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;text-align:center;display:flex;align-items:center;justify-content:center;}
.mx-rh{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;writing-mode:vertical-lr;text-orientation:mixed;transform:rotate(180deg);display:flex;align-items:center;justify-content:center;}
.mx-cell{padding:1.2rem;border:1px solid var(--cream-dark);}.mx-cell h4{font-family:'Playfair Display',serif;font-size:0.95rem;margin-bottom:0.4rem;}.mx-cell p{font-size:0.83rem;color:#555;line-height:1.5;}.mx-cell.best{background:#f0faf4;}.mx-cell.worst{background:#fdf0ef;}.mx-cell .e{font-size:1.4rem;display:block;margin-bottom:0.4rem;}
.mx-cell code{font-family:'JetBrains Mono',monospace;font-size:0.8rem;background:var(--cream-dark);padding:0.1rem 0.3rem;border-radius:2px;color:var(--accent);}
.cl{list-style:none;margin:1.5rem 0;}.cl li{padding:0.55rem 0 0.55rem 2rem;position:relative;font-size:0.93rem;border-bottom:1px dotted var(--gray-light);}.cl li::before{content:'\2610';position:absolute;left:0;color:var(--accent);font-size:1.1rem;}.cl li strong{font-family:'JetBrains Mono',monospace;font-size:0.8rem;color:var(--blue);}
.footer{max-width:1100px;margin:0 auto;padding:2rem;border-top:3px double var(--ink);display:flex;justify-content:space-between;align-items:center;font-family:'JetBrains Mono',monospace;font-size:0.68rem;color:var(--gray);text-transform:uppercase;letter-spacing:0.1em;flex-wrap:wrap;gap:0.5rem;}
.fi{opacity:0;transform:translateY(16px);animation:fu 0.5s ease forwards;}
@keyframes fu{to{opacity:1;transform:translateY(0);}}
.fi:nth-child(2){animation-delay:0.08s;}.fi:nth-child(3){animation-delay:0.16s;}.fi:nth-child(4){animation-delay:0.24s;}
@media(max-width:600px){.header{padding:1.5rem 1rem;}.content{padding:1.5rem 1rem;}.footer{flex-direction:column;text-align:center;}}
</style>
</head>
<body>

<div class="progress-strip">
  <span class="p-dot done"></span><span class="p-label done">1</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">2</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">3</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">4</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">5</span><span class="p-line done"></span>
  <span class="p-dot now"></span><span class="p-label now">6</span><span class="p-line"></span>
  <span class="p-dot"></span><span class="p-label">7</span>
  <span style="color:#555;margin-left:0.5rem;">&middot; &middot; &middot;</span>
  <span class="p-dot" style="margin-left:0.5rem;"></span><span class="p-label">80</span>
</div>

<a href="index.html" class="back-link">&larr; Back to index</a>

<header class="header fi">
  <div class="header-meta"><span class="phase-tag">PHASE 1</span> Foundations &middot; Day 6 of 80 &middot; Neural Networks &amp; Backprop</div>
  <h1>Intro to <span>Language Modeling</span> &mdash; The Bigram Model</h1>
  <p class="header-sub">The simplest possible language model: predict the next character using only the current one. From counting pairs to generating names.</p>
</header>

<main class="content">

  <div class="memo-quote fi">
    The best predictions come from understanding what has happened before. In markets, the simplest model is
    &ldquo;what did this asset do yesterday?&rdquo; In language, it is &ldquo;what letter usually follows this one?&rdquo;
    The bigram model is this idea made precise &mdash; a 27&times;27 lookup table that captures the statistical
    structure of character-level English. Crude, but revelatory.
    <span class="attr">&mdash; Day 6 Principle, adapted from the Marks framework</span>
  </div>

  <h2 class="sh fi"><span class="n">I.</span> What is Language Modeling?</h2>
  <p class="prose fi">
    A <strong>language model</strong> assigns probabilities to sequences. Given a prefix, it predicts what comes next.
    The bigram model is the simplest version: <em>P(next | current)</em> &mdash; the probability of the next character
    depends <strong>only</strong> on the current character. We train it on a dataset of names (Karpathy uses 32,000+
    names from <code>names.txt</code>).
  </p>

  <div class="fi">
    <div class="vf green">
      <div class="vf-label">Exhibit A &mdash; Bigram Frequency Matrix (Sample: Characters a&ndash;e)</div>
      <svg viewBox="0 0 700 280" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
        <text x="80" y="25" class="m" font-size="10" fill="#888">NEXT CHARACTER &rarr;</text>
        <text x="15" y="125" class="m" font-size="10" fill="#888" transform="rotate(-90,15,125)">CURRENT &rarr;</text>

        <text x="165" y="48" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">.</text>
        <text x="265" y="48" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">a</text>
        <text x="365" y="48" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">b</text>
        <text x="465" y="48" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">c</text>
        <text x="565" y="48" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">d</text>

        <text x="90" y="95" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">.</text>
        <text x="90" y="145" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">a</text>
        <text x="90" y="195" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">b</text>
        <text x="90" y="245" class="m" font-size="11" fill="#2c3e6b" text-anchor="middle">c</text>

        <rect x="120" y="60" width="90" height="45" rx="2" fill="#e8f5e9" stroke="#ddd" stroke-width="0.5"/>
        <text x="165" y="88" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">0</text>
        <rect x="220" y="60" width="90" height="45" rx="2" fill="#a5d6a7" stroke="#ddd" stroke-width="0.5"/>
        <text x="265" y="88" class="m" font-size="12" fill="#1b5e20" text-anchor="middle" font-weight="600">4410</text>
        <rect x="320" y="60" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="365" y="88" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">1306</text>
        <rect x="420" y="60" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="465" y="88" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">1542</text>
        <rect x="520" y="60" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="565" y="88" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">1690</text>

        <rect x="120" y="110" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="165" y="138" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">556</text>
        <rect x="220" y="110" width="90" height="45" rx="2" fill="#e8f5e9" stroke="#ddd" stroke-width="0.5"/>
        <text x="265" y="138" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">556</text>
        <rect x="320" y="110" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="365" y="138" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">541</text>
        <rect x="420" y="110" width="90" height="45" rx="2" fill="#e8f5e9" stroke="#ddd" stroke-width="0.5"/>
        <text x="465" y="138" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">470</text>
        <rect x="520" y="110" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="565" y="138" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">1407</text>

        <rect x="120" y="160" width="90" height="45" rx="2" fill="#e8f5e9" stroke="#ddd" stroke-width="0.5"/>
        <text x="165" y="188" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">114</text>
        <rect x="220" y="160" width="90" height="45" rx="2" fill="#a5d6a7" stroke="#ddd" stroke-width="0.5"/>
        <text x="265" y="188" class="m" font-size="12" fill="#1b5e20" text-anchor="middle" font-weight="600">2093</text>
        <rect x="320" y="160" width="90" height="45" rx="2" fill="#c8e6c9" stroke="#ddd" stroke-width="0.5"/>
        <text x="365" y="188" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">168</text>
        <rect x="420" y="160" width="90" height="45" rx="2" fill="#e8f5e9" stroke="#ddd" stroke-width="0.5"/>
        <text x="465" y="188" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">0</text>
        <rect x="520" y="160" width="90" height="45" rx="2" fill="#e8f5e9" stroke="#ddd" stroke-width="0.5"/>
        <text x="565" y="188" class="m" font-size="12" fill="#27654a" text-anchor="middle" font-weight="600">0</text>

        <text x="350" y="265" text-anchor="middle" font-size="11" fill="#555" font-style="italic">Darker green = higher count. Row normalization &rarr; probability distribution.</text>
      </svg>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">II.</span> Building the Model &mdash; Count &amp; Normalize</h2>

  <div class="code-block fi">
<span class="keyword">import</span> torch

<span class="comment"># Build bigram counts from names dataset</span>
words = <span class="func">open</span>(<span class="string">&#39;names.txt&#39;</span>).<span class="func">read</span>().<span class="func">splitlines</span>()
chars = <span class="func">sorted</span>(<span class="func">list</span>(<span class="func">set</span>(<span class="string">&#39;&#39;</span>.<span class="func">join</span>(words))))
stoi = {s:i<span class="op">+</span><span class="number">1</span> <span class="keyword">for</span> i,s <span class="keyword">in</span> <span class="func">enumerate</span>(chars)}
stoi[<span class="string">&#39;.&#39;</span>] = <span class="number">0</span>
itos = {i:s <span class="keyword">for</span> s,i <span class="keyword">in</span> stoi.<span class="func">items</span>()}

N = torch.<span class="func">zeros</span>((<span class="number">27</span>, <span class="number">27</span>), dtype=torch.int32)
<span class="keyword">for</span> w <span class="keyword">in</span> words:
    chs = [<span class="string">&#39;.&#39;</span>] <span class="op">+</span> <span class="func">list</span>(w) <span class="op">+</span> [<span class="string">&#39;.&#39;</span>]
    <span class="keyword">for</span> ch1, ch2 <span class="keyword">in</span> <span class="func">zip</span>(chs, chs[<span class="number">1</span>:]):
        N[stoi[ch1], stoi[ch2]] <span class="op">+=</span> <span class="number">1</span>

<span class="comment"># Normalize to probabilities (add smoothing)</span>
P = (N <span class="op">+</span> <span class="number">1</span>).<span class="func">float</span>()
P <span class="op">=</span> P <span class="op">/</span> P.<span class="func">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)
  </div>

  <h2 class="sh fi"><span class="n">III.</span> Sampling &amp; Evaluating &mdash; Generate Names, Measure Quality</h2>

  <div class="code-block fi">
<span class="comment"># Generate names by sampling from bigram distribution</span>
<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="func">range</span>(<span class="number">5</span>):
    out = []
    ix = <span class="number">0</span>  <span class="comment"># start token</span>
    <span class="keyword">while</span> <span class="keyword">True</span>:
        p = P[ix]
        ix = torch.<span class="func">multinomial</span>(p, num_samples=<span class="number">1</span>).<span class="func">item</span>()
        <span class="keyword">if</span> ix <span class="op">==</span> <span class="number">0</span>: <span class="keyword">break</span>
        out.<span class="func">append</span>(itos[ix])
    <span class="func">print</span>(<span class="string">&#39;&#39;</span>.<span class="func">join</span>(out))

<span class="comment"># Evaluate with negative log-likelihood</span>
log_likelihood = <span class="number">0.0</span>
n = <span class="number">0</span>
<span class="keyword">for</span> w <span class="keyword">in</span> words:
    chs = [<span class="string">&#39;.&#39;</span>] <span class="op">+</span> <span class="func">list</span>(w) <span class="op">+</span> [<span class="string">&#39;.&#39;</span>]
    <span class="keyword">for</span> ch1, ch2 <span class="keyword">in</span> <span class="func">zip</span>(chs, chs[<span class="number">1</span>:]):
        prob = P[stoi[ch1], stoi[ch2]]
        log_likelihood <span class="op">+=</span> torch.<span class="func">log</span>(prob)
        n <span class="op">+=</span> <span class="number">1</span>
nll = <span class="op">-</span>log_likelihood <span class="op">/</span> n
<span class="func">print</span>(f<span class="string">"avg NLL: {nll:.4f}"</span>)  <span class="comment"># ~2.45</span>
  </div>

  <div class="box insight fi">
    <h4>Why Negative Log-Likelihood?</h4>
    <p>NLL converts probabilities into a loss: high probability &rarr; low loss, low probability &rarr; high loss. A perfect model (always assigns probability 1.0 to the correct next char) has NLL = 0. A uniform random model (1/27 chance) has NLL = log(27) &asymp; 3.30. Our bigram achieves &sim;2.45 &mdash; significantly better than random, but far from perfect. This is the baseline every future model must beat.</p>
  </div>

  <h2 class="sh fi"><span class="n">IV.</span> The Matrix &mdash; What Matters Today</h2>
  <div class="mx fi">
    <div class="mx-corner"></div>
    <div class="mx-ch">Builds Deep Intuition</div>
    <div class="mx-ch">Surface-Level Only</div>
    <div class="mx-rh">Quick to Do</div>
    <div class="mx-cell best">
      <span class="e">&#127919;</span>
      <h4>DO FIRST</h4>
      <p>Build the 27&times;27 count matrix. Normalize to probabilities. Sample 10 names. Compute NLL.</p>
    </div>
    <div class="mx-cell">
      <span class="e">&#9197;&#65039;</span>
      <h4>DO IF TIME</h4>
      <p>Visualize the count matrix with <code>plt.imshow()</code>. The patterns reveal English phonotactics &mdash; which letters follow which.</p>
    </div>
    <div class="mx-rh">Slow but Worth It</div>
    <div class="mx-cell">
      <span class="e">&#128400;</span>
      <h4>DO CAREFULLY</h4>
      <p>Understand smoothing: why adding 1 to all counts prevents log(0) and what it implies about rare character pairs.</p>
    </div>
    <div class="mx-cell worst">
      <span class="e">&#128683;</span>
      <h4>AVOID TODAY</h4>
      <p>Using neural networks for language modeling. Today is purely counting-based. The neural version comes on Day 9.</p>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">V.</span> Today&rsquo;s Deliverables</h2>
  <ul class="cl fi">
    <li><strong>Character mapping:</strong> Build <code>stoi</code> and <code>itos</code> for 27 characters (a-z + special token)</li>
    <li><strong>Count matrix:</strong> Populate a 27&times;27 tensor from the names dataset</li>
    <li><strong>Probability matrix:</strong> Row-normalize with +1 smoothing</li>
    <li><strong>Sampling:</strong> Generate 10 names by sampling character by character</li>
    <li><strong>Evaluation:</strong> Compute average NLL over the full dataset (&sim;2.45)</li>
    <li><strong>Baseline:</strong> Record the NLL &mdash; every future model must beat this number</li>
  </ul>

  <div class="memo-quote fi" style="margin-top:3rem;">
    The bigram model is the &ldquo;index fund&rdquo; of language modeling: simple, well-understood, and surprisingly hard
    to beat with naive approaches. It sets a floor. Everything from here &mdash; MLPs, RNNs, Transformers &mdash; is an
    attempt to capture longer-range dependencies that bigrams miss. Tomorrow: tensors &amp; broadcasting, the tools you need.
    <span class="attr">&mdash; Day 6 Closing Principle</span>
  </div>

</main>

<footer class="footer">
  <span>RAG &amp; LLM Engineer &middot; 80-Day Plan</span>
  <span>Day 6 of 80 &middot; Phase 1: Foundations</span>
  <span>&larr; <a href="llm-day5.html" style="color:inherit;">Day 5</a> &nbsp;|&nbsp; &rarr; Day 7: Tensors &amp; Broadcasting</span>
</footer>
</body>
</html>