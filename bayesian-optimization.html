<!DOCTYPE html>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-62YF7Y81BS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-62YF7Y81BS');
</script>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Bayesian Optimisation ‚Äî Mathematical Deep Dive</title>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,500&family=JetBrains+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

<style>
:root {
  --bg:      #f8f7f4;
  --ink:     #1c1c1c;
  --muted:   #6e6e6e;
  --faint:   #b8b3a8;
  --rule:    #d8d3c8;
  --card:    #f2f0eb;
  --red:     #9b2335;
  --blue:    #1a4a7a;
  --green:   #1e6b3c;
  --amber:   #8a5a00;
  --teal:    #1a5a6b;
  --purple:  #5a2080;
}

* { box-sizing: border-box; margin: 0; padding: 0; }

body {
  font-family: 'EB Garamond', serif;
  background: var(--bg);
  color: var(--ink);
  max-width: 1020px;
  margin: 0 auto;
  padding: 56px 44px;
  line-height: 1.7;
  font-size: 15px;
}

.doc-header {
  border-top: 2px solid var(--ink);
  padding-top: 20px;
  margin-bottom: 48px;
}
.doc-meta {
  display: flex;
  justify-content: space-between;
  align-items: flex-start;
  margin-bottom: 28px;
}
.doc-id {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: .12em;
  text-transform: uppercase;
  color: var(--muted);
  line-height: 1.9;
}
.doc-classification {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: .15em;
  color: var(--muted);
  text-transform: uppercase;
  text-align: right;
  border: 1px solid var(--faint);
  padding: 6px 12px;
}
h1 {
  font-family: 'EB Garamond', serif;
  font-size: 46px;
  font-weight: 500;
  line-height: 1.08;
  letter-spacing: -.01em;
}
h1 .sub {
  display: block;
  font-size: 20px;
  font-weight: 400;
  font-style: italic;
  color: var(--muted);
  margin-top: 4px;
  letter-spacing: 0;
}
.abstract {
  margin-top: 20px;
  padding: 18px 24px;
  border-left: 3px solid var(--ink);
  font-size: 14px;
  font-style: italic;
  color: var(--muted);
  line-height: 1.8;
  max-width: 740px;
}
.abstract strong { font-style: normal; color: var(--ink); }

.section {
  margin: 60px 0 24px;
  display: grid;
  grid-template-columns: 56px 1fr;
  gap: 12px;
  align-items: baseline;
  border-bottom: 1px solid var(--ink);
  padding-bottom: 8px;
}
.sec-n {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  color: var(--muted);
  letter-spacing: .1em;
  text-transform: uppercase;
}
.sec-title { font-size: 22px; font-weight: 500; }
.sec-sub {
  grid-column: 2;
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  color: var(--muted);
  letter-spacing: .1em;
  text-transform: uppercase;
  margin-top: -4px;
}

p.body {
  font-size: 15px;
  line-height: 1.8;
  color: var(--ink);
  margin-bottom: 16px;
  max-width: 760px;
}
p.body strong { font-weight: 600; }
p.body em { font-style: italic; }

.def {
  margin: 20px 0;
  padding: 16px 20px 16px 0;
  border-top: 1px solid var(--rule);
  border-bottom: 1px solid var(--rule);
  display: grid;
  grid-template-columns: 110px 1fr;
  gap: 16px;
}
.def-tag {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  text-transform: uppercase;
  letter-spacing: .12em;
  color: var(--muted);
  padding-top: 2px;
}
.def-body { font-size: 14px; line-height: 1.75; }
.def-body strong { font-weight: 600; }

.math-block {
  font-family: 'JetBrains Mono', monospace;
  font-size: 13px;
  background: var(--card);
  border: 1px solid var(--rule);
  padding: 16px 22px;
  margin: 16px 0;
  color: var(--ink);
  line-height: 2;
  overflow-x: auto;
}
.math-block .label {
  font-size: 9px;
  text-transform: uppercase;
  letter-spacing: .12em;
  color: var(--muted);
  display: block;
  margin-bottom: 8px;
  border-bottom: 1px dashed var(--rule);
  padding-bottom: 6px;
}
.math-block .eq { display: block; margin: 4px 0 4px 20px; }
.math-block .comment { color: var(--muted); }
.m { font-family: 'JetBrains Mono', monospace; font-size: 12px; background: rgba(0,0,0,.05); padding: 1px 4px; border-radius: 2px; color: var(--red); }

.diagram {
  margin: 20px 0;
  border: 1px solid var(--rule);
  background: var(--card);
  overflow: hidden;
}
.diagram-hdr {
  padding: 10px 18px;
  background: var(--ink);
  color: var(--bg);
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  text-transform: uppercase;
  letter-spacing: .12em;
  display: flex;
  justify-content: space-between;
  align-items: center;
}
.diagram-hdr span { opacity: .5; }
.diagram-body { padding: 24px; }
.diagram-note {
  padding: 10px 18px;
  border-top: 1px dashed var(--rule);
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  color: var(--muted);
  line-height: 1.7;
}

.ex-table { width: 100%; border-collapse: collapse; font-size: 13px; }
.ex-table th {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  text-transform: uppercase;
  letter-spacing: .1em;
  padding: 10px 14px;
  background: var(--ink);
  color: var(--bg);
  text-align: left;
  font-weight: 500;
}
.ex-table td {
  padding: 10px 14px;
  border-bottom: 1px solid var(--rule);
  vertical-align: top;
  line-height: 1.6;
  color: var(--muted);
}
.ex-table td.key { color: var(--ink); font-weight: 500; }
.ex-table tr:nth-child(even) td { background: rgba(0,0,0,.02); }

.cmp-grid {
  display: grid;
  gap: 1px;
  background: var(--rule);
}
.cmp-cell { background: var(--card); padding: 18px; }
.cmp-name {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  font-weight: 600;
  letter-spacing: .06em;
  margin-bottom: 6px;
}
.cmp-formula {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--red);
  background: rgba(155,35,53,.06);
  padding: 5px 8px;
  margin-bottom: 10px;
  line-height: 1.6;
}
.cmp-prop {
  font-size: 12px;
  color: var(--muted);
  margin-bottom: 3px;
  padding-left: 10px;
  position: relative;
  line-height: 1.5;
}
.cmp-prop::before { content: '‚Äî'; position: absolute; left: 0; color: var(--faint); }

.spectrum { display: flex; flex-direction: column; gap: 0; }
.sp-row {
  display: grid;
  grid-template-columns: 160px 1fr 180px;
  border: 1px solid var(--rule);
  margin-bottom: -1px;
}
.sp-lbl {
  padding: 14px 16px;
  border-right: 1px solid var(--rule);
  background: rgba(0,0,0,.025);
  font-family: 'JetBrains Mono', monospace;
}
.sp-lbl strong { display: block; font-size: 12px; color: var(--ink); margin-bottom: 3px; }
.sp-lbl span { font-size: 9px; color: var(--muted); text-transform: uppercase; letter-spacing: .08em; }
.sp-viz { padding: 10px 16px; display: flex; align-items: center; }
.sp-note { padding: 12px 14px; border-left: 1px solid var(--rule); font-size: 12px; color: var(--muted); line-height: 1.5; }
.sp-note strong { color: var(--ink); font-weight: 600; }
.bar-track { width: 100%; height: 20px; background: var(--rule); border-radius: 1px; overflow: hidden; }
.bar-fill  { height: 100%; display: flex; align-items: center; padding-left: 8px; font-family: 'JetBrains Mono', monospace; font-size: 9px; font-weight: 600; color: white; }

.pyramid { max-width: 660px; margin: 0 auto; display: flex; flex-direction: column; }
.pyr-row {
  display: flex; align-items: center; justify-content: space-between;
  border: 1px solid var(--rule); margin-bottom: -1px; padding: 14px 20px; gap: 16px;
}
.pyr-t { font-size: 14px; font-weight: 500; margin-bottom: 3px; }
.pyr-d { font-family: 'JetBrains Mono', monospace; font-size: 10px; color: var(--muted); line-height: 1.5; }
.pyr-b {
  font-family: 'JetBrains Mono', monospace; font-size: 9px;
  padding: 3px 9px; border-radius: 1px; white-space: nowrap; flex-shrink: 0; border: 1px solid;
}

hr.rule { border: none; border-top: 1px solid var(--rule); margin: 36px 0; }

.two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin: 16px 0; }
.note-box { background: var(--card); border: 1px solid var(--rule); padding: 18px; }
.note-box h4 { font-size: 15px; font-weight: 500; margin-bottom: 8px; font-family: 'EB Garamond', serif; }
.note-box p { font-size: 13px; color: var(--muted); line-height: 1.65; }

.regret-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 1px;
  background: var(--rule);
  margin: 16px 0;
}
.rg-cell { background: var(--card); padding: 20px; }
.rg-name { font-family: 'JetBrains Mono', monospace; font-size: 11px; font-weight: 600; color: var(--ink); margin-bottom: 6px; letter-spacing: .05em; }
.rg-formula { font-family: 'JetBrains Mono', monospace; font-size: 11px; color: var(--red); background: rgba(155,35,53,.06); padding: 6px 10px; margin-bottom: 10px; line-height: 1.5; }
.rg-body { font-size: 13px; color: var(--muted); line-height: 1.6; }

.doc-footer {
  margin-top: 56px;
  padding-top: 16px;
  border-top: 1px solid var(--rule);
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  color: var(--muted);
  display: flex;
  justify-content: space-between;
  letter-spacing: .06em;
  line-height: 1.8;
}

.notebook-card {
  margin: 24px 0;
  border: 2px solid var(--ink);
  background: white;
  overflow: hidden;
}
.notebook-card-hdr {
  padding: 14px 20px;
  background: var(--ink);
  color: var(--bg);
  display: flex;
  justify-content: space-between;
  align-items: center;
}
.notebook-card-hdr .nb-title {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  letter-spacing: .08em;
  text-transform: uppercase;
  font-weight: 600;
}
.notebook-card-hdr .nb-badge {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  padding: 3px 10px;
  border: 1px solid rgba(255,255,255,.3);
  letter-spacing: .08em;
  text-transform: uppercase;
  opacity: .7;
}
.notebook-card-body {
  padding: 20px;
}
.notebook-card-body p {
  font-size: 14px;
  color: var(--muted);
  line-height: 1.7;
  margin-bottom: 12px;
}
.notebook-card-body ul {
  list-style: none;
  padding: 0;
  margin: 0 0 16px 0;
}
.notebook-card-body ul li {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  color: var(--muted);
  padding: 4px 0 4px 16px;
  position: relative;
  line-height: 1.6;
}
.notebook-card-body ul li::before {
  content: '‚Üí';
  position: absolute;
  left: 0;
  color: var(--faint);
}
.notebook-links {
  display: flex;
  gap: 12px;
  flex-wrap: wrap;
}
.notebook-links a {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: .06em;
  text-transform: uppercase;
  padding: 8px 16px;
  text-decoration: none;
  border: 1.5px solid;
  transition: all .15s;
  font-weight: 600;
}
.nb-colab {
  background: #f9ab00;
  color: #1c1c1c;
  border-color: #f9ab00;
}
.nb-colab:hover { background: #e09800; border-color: #e09800; }
.nb-github {
  background: white;
  color: var(--ink);
  border-color: var(--ink);
}
.nb-github:hover { background: var(--ink); color: white; }
.nb-nbviewer {
  background: white;
  color: var(--blue);
  border-color: var(--blue);
}
.nb-nbviewer:hover { background: var(--blue); color: white; }
</style>
</head>
<body>

<!-- HEADER -->
<div class="doc-header">
  <div class="doc-meta">
    <div class="doc-id">
      Technical Memorandum<br>
      Optimisation Theory ¬∑ Probabilistic Methods<br>
      February 2026
    </div>
    <div class="doc-classification">
      Mathematical Deep Dive<br>
      No PM framing
    </div>
  </div>
  <h1>
    Bayesian Optimisation
    <span class="sub">A complete mathematical treatment ‚Äî surrogate models, acquisition functions,<br>Gaussian processes, convergence, and the geometry of sequential decisions</span>
  </h1>
  <div class="abstract">
    <strong>Abstract.</strong> Bayesian optimisation is a sequential, model-based strategy for the global optimisation of an expensive black-box function <span class="m">f : ùí≥ ‚Üí ‚Ñù</span>. It maintains a probabilistic surrogate model ‚Äî typically a Gaussian process ‚Äî over <span class="m">f</span>, and uses this model to define an acquisition function that directs sampling toward regions of high expected improvement. We derive the core mathematics: the Gaussian process posterior, the three canonical acquisition functions (EI, PI, UCB), kernel design and its relationship to function priors, marginalisation of hyperparameters, convergence in terms of cumulative regret, and the precise conditions under which BO outperforms gradient-based methods.
  </div>
</div>

<!-- ¬ß1 -->
<div class="section">
  <div class="sec-n">¬ß 1</div>
  <div>
    <div class="sec-title">The Problem Formulation</div>
    <div class="sec-sub">What BO solves and why derivatives are unavailable</div>
  </div>
</div>

<p class="body">We seek the global maximiser of a function <span class="m">f</span> over a compact domain <span class="m">ùí≥ ‚äÜ ‚Ñù·µà</span>:</p>

<div class="math-block">
  <span class="label">Problem statement</span>
  <span class="eq">x* = arg max_{x ‚àà ùí≥}  f(x)</span>
</div>

<p class="body">The function <span class="m">f</span> has three properties that make gradient descent inapplicable:</p>

<div class="def">
  <div class="def-tag">Property 1</div>
  <div class="def-body"><strong>Expensive to evaluate.</strong> Each query <span class="m">f(x)</span> costs significantly ‚Äî training a neural network, running a physics simulation, a laboratory experiment. We have a budget of <span class="m">T</span> evaluations, typically <span class="m">T ‚àà [10, 500]</span>. This is many orders of magnitude fewer than the iterations available to gradient descent.</div>
</div>
<div class="def">
  <div class="def-tag">Property 2</div>
  <div class="def-body"><strong>Black-box and non-differentiable.</strong> We cannot compute <span class="m">‚àÇf/‚àÇx</span>. The function may be computed by an external process, involve discrete operations, or simply not be expressed in closed form. Gradient information is unavailable by definition.</div>
</div>
<div class="def">
  <div class="def-tag">Property 3</div>
  <div class="def-body"><strong>Possibly multi-modal.</strong> <span class="m">f</span> may have multiple local maxima. Methods that follow local curvature will get trapped. We require a strategy with global coverage guarantees.</div>
</div>

<p class="body">Bayesian optimisation addresses all three by building a <em>probabilistic model</em> of <span class="m">f</span> from past evaluations, then using this model to choose the next query point ‚Äî the one that best balances exploring unknown regions against exploiting regions known to be promising.</p>

<div class="diagram">
  <div class="diagram-hdr">Exhibit 1 ‚Äî The Bayesian Optimisation Loop <span>4-component sequential cycle</span></div>
  <div class="diagram-body">
    <svg viewBox="0 0 860 280" xmlns="http://www.w3.org/2000/svg" width="100%" style="display:block;">
      <rect width="860" height="280" fill="#f2f0eb"/>
      <rect x="20" y="90" width="175" height="100" fill="white" stroke="#1c1c1c" stroke-width="1.5"/>
      <rect x="20" y="90" width="175" height="22" fill="#1c1c1c"/>
      <text x="107" y="106" font-family="'JetBrains Mono',monospace" font-size="9" fill="white" text-anchor="middle" letter-spacing="1">SURROGATE MODEL</text>
      <text x="107" y="130" font-family="'EB Garamond',serif" font-size="13" fill="#1c1c1c" text-anchor="middle">Gaussian Process</text>
      <text x="107" y="148" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">p(f | x‚ÇÅ..x‚Çô, y‚ÇÅ..y‚Çô)</text>
      <text x="107" y="166" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">posterior mean Œº‚Çô(x)</text>
      <text x="107" y="181" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">posterior var œÉ¬≤‚Çô(x)</text>
      <rect x="245" y="90" width="175" height="100" fill="white" stroke="#1c1c1c" stroke-width="1.5"/>
      <rect x="245" y="90" width="175" height="22" fill="#1c1c1c"/>
      <text x="332" y="106" font-family="'JetBrains Mono',monospace" font-size="9" fill="white" text-anchor="middle" letter-spacing="1">ACQUISITION FN</text>
      <text x="332" y="130" font-family="'EB Garamond',serif" font-size="13" fill="#1c1c1c" text-anchor="middle">Œ±(x) ‚Äî cheap to optimise</text>
      <text x="332" y="148" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">EI, UCB, or PI</text>
      <text x="332" y="166" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">x_{n+1} = arg max Œ±(x)</text>
      <text x="332" y="181" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">gradient-free inner opt</text>
      <rect x="470" y="90" width="175" height="100" fill="white" stroke="#1c1c1c" stroke-width="1.5"/>
      <rect x="470" y="90" width="175" height="22" fill="#1c1c1c"/>
      <text x="557" y="106" font-family="'JetBrains Mono',monospace" font-size="9" fill="white" text-anchor="middle" letter-spacing="1">TRUE EVALUATION</text>
      <text x="557" y="130" font-family="'EB Garamond',serif" font-size="13" fill="#1c1c1c" text-anchor="middle">y_{n+1} = f(x_{n+1}) + Œµ</text>
      <text x="557" y="148" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">Œµ ~ ùí©(0, œÉ¬≤_noise)</text>
      <text x="557" y="166" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">expensive oracle call</text>
      <text x="557" y="181" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">(T budget total)</text>
      <rect x="695" y="90" width="150" height="100" fill="white" stroke="#1c1c1c" stroke-width="1.5"/>
      <rect x="695" y="90" width="150" height="22" fill="#1c1c1c"/>
      <text x="770" y="106" font-family="'JetBrains Mono',monospace" font-size="9" fill="white" text-anchor="middle" letter-spacing="1">POSTERIOR UPDATE</text>
      <text x="770" y="130" font-family="'EB Garamond',serif" font-size="13" fill="#1c1c1c" text-anchor="middle">Bayes' rule update</text>
      <text x="770" y="148" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">D‚Çô ‚Üê D‚Çô ‚à™ {x‚Çô‚Çä‚ÇÅ, y‚Çô‚Çä‚ÇÅ}</text>
      <text x="770" y="166" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">O(n¬≥) GP refit</text>
      <text x="770" y="181" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">n ‚Üê n + 1</text>
      <line x1="196" y1="140" x2="243" y2="140" stroke="#1c1c1c" stroke-width="1.5" marker-end="url(#arr)"/>
      <line x1="421" y1="140" x2="468" y2="140" stroke="#1c1c1c" stroke-width="1.5" marker-end="url(#arr)"/>
      <line x1="646" y1="140" x2="693" y2="140" stroke="#1c1c1c" stroke-width="1.5" marker-end="url(#arr)"/>
      <path d="M770,191 L770,240 L107,240 L107,191" fill="none" stroke="#1c1c1c" stroke-width="1.5" marker-end="url(#arr)"/>
      <text x="438" y="258" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle" letter-spacing="1">REPEAT UNTIL BUDGET EXHAUSTED</text>
      <line x1="107" y1="40" x2="107" y2="88" stroke="#6e6e6e" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#arrgrey)"/>
      <text x="107" y="28" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">INITIALISE</text>
      <text x="107" y="40" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">D‚ÇÄ = Latin hypercube</text>
      <line x1="557" y1="40" x2="557" y2="88" stroke="#1a4a7a" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#arrblue)"/>
      <text x="557" y="28" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1a4a7a" text-anchor="middle">OUTPUT: xÃÇ* = arg max f(x·µ¢)</text>
      <text x="557" y="40" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1a4a7a" text-anchor="middle">over all observed points</text>
      <defs>
        <marker id="arr" markerWidth="8" markerHeight="8" refX="4" refY="4" orient="auto"><path d="M0,0 L8,4 L0,8 Z" fill="#1c1c1c"/></marker>
        <marker id="arrgrey" markerWidth="7" markerHeight="7" refX="3.5" refY="3.5" orient="auto"><path d="M0,0 L7,3.5 L0,7 Z" fill="#6e6e6e"/></marker>
        <marker id="arrblue" markerWidth="7" markerHeight="7" refX="3.5" refY="3.5" orient="auto"><path d="M0,0 L7,3.5 L0,7 Z" fill="#1a4a7a"/></marker>
      </defs>
    </svg>
  </div>
  <div class="diagram-note">The loop has two optimisations: the outer loop (expensive, querying <span class="m">f</span>) and an inner loop (cheap, maximising <span class="m">Œ±(x)</span> over the surrogate). The inner optimisation uses gradient-based or evolutionary methods ‚Äî it is cheap because <span class="m">Œ±</span> is analytic and differentiable even when <span class="m">f</span> is not.</div>
</div>
<!-- ¬ß2 GP SURROGATE -->
<div class="section">
  <div class="sec-n">¬ß 2</div>
  <div>
    <div class="sec-title">The Gaussian Process Surrogate</div>
    <div class="sec-sub">Prior specification, posterior update, and the predictive distribution</div>
  </div>
</div>

<p class="body">A Gaussian process (GP) is a distribution over functions. It is the canonical surrogate for Bayesian optimisation because it provides a principled, closed-form posterior that quantifies both the <em>predicted value</em> and <em>uncertainty</em> at every unobserved point ‚Äî precisely what the acquisition function requires.</p>

<div class="def">
  <div class="def-tag">Definition</div>
  <div class="def-body"><strong>Gaussian Process.</strong> We say <span class="m">f ~ GP(m, k)</span> if any finite collection of function values <span class="m">[f(x‚ÇÅ), ‚Ä¶, f(x‚Çô)]</span> is jointly Gaussian distributed with mean vector <span class="m">[m(x‚ÇÅ), ‚Ä¶, m(x‚Çô)]</span> and covariance matrix <span class="m">[k(x·µ¢, x‚±º)]</span>. The mean function <span class="m">m(x) = ùîº[f(x)]</span> and kernel <span class="m">k(x, x') = Cov(f(x), f(x'))</span> completely specify the distribution.</div>
</div>

<p class="body">In practice we set <span class="m">m(x) = 0</span> (absorbed into the kernel's variance parameter) and focus on kernel design. Given <span class="m">n</span> observations <span class="m">D‚Çô = {(x·µ¢, y·µ¢)}·µ¢‚Çå‚ÇÅ‚Åø</span> with <span class="m">y·µ¢ = f(x·µ¢) + Œµ·µ¢</span>, <span class="m">Œµ·µ¢ ~ ùí©(0, œÉ¬≤‚Çô)</span>, the posterior over <span class="m">f</span> at any test point <span class="m">x*</span> is Gaussian with closed-form parameters:</p>

<div class="math-block">
  <span class="label">GP Posterior ‚Äî Predictive Distribution at x*</span>
  <span class="eq">f(x*) | D‚Çô  ~  ùí©( Œº‚Çô(x*),  œÉ¬≤‚Çô(x*) )</span>
  <span class="eq"> </span>
  <span class="eq">Œº‚Çô(x*)  =  k(x*, X)  [K(X,X) + œÉ¬≤‚Çô I]‚Åª¬π  y      <span class="comment">    ‚Üê posterior mean (prediction)</span></span>
  <span class="eq">œÉ¬≤‚Çô(x*) =  k(x*,x*) ‚àí k(x*,X) [K(X,X) + œÉ¬≤‚Çô I]‚Åª¬π k(X,x*) <span class="comment">‚Üê posterior variance (uncertainty)</span></span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">where:  K(X,X)·µ¢‚±º = k(x·µ¢,x‚±º)  is the n√ón kernel matrix (Gram matrix)</span></span>
  <span class="eq"><span class="comment">        k(x*,X)  is the 1√ón cross-covariance vector</span></span>
  <span class="eq"><span class="comment">        y  =  [y‚ÇÅ,‚Ä¶,y‚Çô]·µÄ  is the vector of observations</span></span>
</div>

<p class="body">The <strong>posterior mean</strong> <span class="m">Œº‚Çô(x*)</span> is the GP's best prediction of <span class="m">f(x*)</span> given all data seen so far. The <strong>posterior variance</strong> <span class="m">œÉ¬≤‚Çô(x*)</span> quantifies uncertainty: it equals the prior variance at <span class="m">x*</span> minus the information gained from observations. Points near observed data have low variance; unexplored regions retain high variance.</p>

<div class="notebook-card">
  <div class="notebook-card-hdr">
    <span class="nb-title">Notebook 1 ‚Äî GP Surrogate: From Prior to Posterior</span>
    <span class="nb-badge">Runnable Python</span>
  </div>
  <div class="notebook-card-body">
    <p>Implements a Gaussian Process from scratch (NumPy only). Visualises the prior ‚Üí posterior progression as observations collapse uncertainty, and demonstrates the effect of noise on the posterior.</p>
    <ul>
      <li>RBF kernel implementation and Cholesky sampling from the GP prior</li>
      <li>Closed-form posterior: Œº‚Çô(x*) and œÉ¬≤‚Çô(x*) with 0, 3, and 7 observations</li>
      <li>Effect of noise variance œÉ¬≤‚Çô on posterior smoothness</li>
      <li>O(n¬≥) computational cost benchmark</li>
    </ul>
    <div class="notebook-links">
      <a href="https://colab.research.google.com/github/OmkarRayAI/omkarray/blob/main/notebooks/01_gp_surrogate.ipynb" target="_blank" class="nb-colab">‚ñ∂ Open in Colab</a>
      <a href="https://github.com/OmkarRayAI/omkarray/blob/main/notebooks/01_gp_surrogate.ipynb" target="_blank" class="nb-github">View on GitHub</a>
      <a href="https://nbviewer.org/github/OmkarRayAI/omkarray/blob/main/notebooks/01_gp_surrogate.ipynb" target="_blank" class="nb-nbviewer">nbviewer</a>
    </div>
  </div>
</div>

<p class="body"><strong>Computational cost.</strong> The critical bottleneck is matrix inversion: computing <span class="m">[K(X,X) + œÉ¬≤‚Çô I]‚Åª¬π</span> costs <span class="m">O(n¬≥)</span> time and <span class="m">O(n¬≤)</span> space. This is acceptable for small <span class="m">n</span> (say, <span class="m">n ‚â§ 10‚Å¥</span>), but becomes prohibitive as the evaluation budget grows. Sparse GPs, inducing point methods (SGPR), and Cholesky decompositions with rank-1 updates mitigate this in practice.</p>

<!-- ¬ß3 KERNELS -->
<div class="section">
  <div class="sec-n">¬ß 3</div>
  <div>
    <div class="sec-title">Kernel Design and Function Priors</div>
    <div class="sec-sub">The kernel encodes all prior beliefs about f ‚Äî smoothness, periodicity, scale</div>
  </div>
</div>

<p class="body">The kernel function <span class="m">k(x, x')</span> is the only place where prior knowledge about <span class="m">f</span> enters the model. It must be <em>positive semi-definite</em> (so that <span class="m">K(X,X)</span> is always a valid covariance matrix). Every valid kernel corresponds to a specific class of functions that the GP prior places mass on.</p>

<div class="notebook-card">
  <div class="notebook-card-hdr">
    <span class="nb-title">Notebook 2 ‚Äî Kernel Design & Function Priors</span>
    <span class="nb-badge">Runnable Python</span>
  </div>
  <div class="notebook-card-body">
    <p>Implements RBF, Mat√©rn 5/2, Mat√©rn 3/2, and Periodic kernels from scratch. Visualises how each kernel shapes the GP prior, how length-scale controls smoothness, and how kernels compose.</p>
    <ul>
      <li>Kernel correlation profiles: k(r) vs distance for all four kernels</li>
      <li>GP prior samples under each kernel ‚Äî same randomness, different function classes</li>
      <li>Length-scale effect: ‚Ñì = 0.3 vs 1.0 vs 3.0</li>
      <li>Kernel composition: RBF + Periodic, RBF √ó Periodic, multi-scale</li>
      <li>Log marginal likelihood for hyperparameter selection</li>
    </ul>
    <div class="notebook-links">
      <a href="https://colab.research.google.com/github/OmkarRayAI/omkarray/blob/main/notebooks/02_kernels.ipynb" target="_blank" class="nb-colab">‚ñ∂ Open in Colab</a>
      <a href="https://github.com/OmkarRayAI/omkarray/blob/main/notebooks/02_kernels.ipynb" target="_blank" class="nb-github">View on GitHub</a>
      <a href="https://nbviewer.org/github/OmkarRayAI/omkarray/blob/main/notebooks/02_kernels.ipynb" target="_blank" class="nb-nbviewer">nbviewer</a>
    </div>
  </div>
</div>
<!-- ¬ß4 ACQUISITION FUNCTIONS -->
<div class="section">
  <div class="sec-n">¬ß 4</div>
  <div>
    <div class="sec-title">Acquisition Functions</div>
    <div class="sec-sub">The mathematical encoding of exploration vs. exploitation</div>
  </div>
</div>

<p class="body">The acquisition function <span class="m">Œ± : ùí≥ ‚Üí ‚Ñù</span> is a function of the GP posterior ‚Äî cheap to evaluate and to optimise ‚Äî that quantifies how useful querying <span class="m">f</span> at a candidate point <span class="m">x</span> would be. All canonical acquisition functions are derived from the posterior predictive distribution <span class="m">ùí©(Œº‚Çô(x), œÉ¬≤‚Çô(x))</span>.</p>

<p class="body">Define <span class="m">f* = max{y‚ÇÅ,‚Ä¶,y‚Çô}</span> as the current best observed value. Then:</p>

<div class="math-block">
  <span class="label">The three canonical acquisition functions ‚Äî full derivations</span>
  <span class="eq"> </span>
  <span class="eq"><strong>1.  Probability of Improvement (PI)  ‚Äî Kushner, 1964</strong></span>
  <span class="eq"> </span>
  <span class="eq">   Œ±_PI(x) = P( f(x) &gt; f* + Œæ )  =  Œ¶( (Œº‚Çô(x) ‚àí f* ‚àí Œæ) / œÉ‚Çô(x) )</span>
  <span class="eq"> </span>
  <span class="eq">   <span class="comment">Œ¶ = CDF of standard normal. Œæ ‚â• 0 is a trade-off parameter.</span></span>
  <span class="eq">   <span class="comment">PI only measures probability of improvement, ignoring improvement magnitude.</span></span>
  <span class="eq">   <span class="comment">Greedy (Œæ=0) ‚Üí exploits; large Œæ ‚Üí forces exploration.</span></span>
  <span class="eq"> </span>
  <span class="eq"><strong>2.  Expected Improvement (EI)  ‚Äî Mockus, 1978</strong></span>
  <span class="eq"> </span>
  <span class="eq">   Œ±_EI(x) = ùîº[ max(f(x) ‚àí f*, 0) ]</span>
  <span class="eq"> </span>
  <span class="eq">   =  (Œº‚Çô(x) ‚àí f*) ¬∑ Œ¶(Z)  +  œÉ‚Çô(x) ¬∑ œÜ(Z)         <span class="comment">Z = (Œº‚Çô(x)‚àíf*)/œÉ‚Çô(x)</span></span>
  <span class="eq"> </span>
  <span class="eq">   <span class="comment">œÜ = PDF of standard normal.  This closed form arises from integrating</span></span>
  <span class="eq">   <span class="comment">max(f‚àíf*,0) over the Gaussian predictive distribution ùí©(Œº‚Çô,œÉ¬≤‚Çô).</span></span>
  <span class="eq">   <span class="comment">First term: exploitation (high mean ‚Üí high reward). </span></span>
  <span class="eq">   <span class="comment">Second term: exploration (high œÉ ‚Üí high reward).</span></span>
  <span class="eq">   <span class="comment">EI = 0 iff œÉ‚Çô(x) = 0 (already observed exactly) or Œº‚Çô(x) &lt;&lt; f*.</span></span>
  <span class="eq"> </span>
  <span class="eq"><strong>3.  Upper Confidence Bound (UCB)  ‚Äî Srinivas et al., 2010</strong></span>
  <span class="eq"> </span>
  <span class="eq">   Œ±_UCB(x) = Œº‚Çô(x)  +  Œ≤¬Ω ¬∑ œÉ‚Çô(x)</span>
  <span class="eq"> </span>
  <span class="eq">   <span class="comment">Œ≤ &gt; 0 is the exploration parameter (controls confidence width).</span></span>
  <span class="eq">   <span class="comment">Œ≤‚Üí0: pure exploitation (follow mean). Œ≤‚Üí‚àû: pure exploration (follow œÉ).</span></span>
  <span class="eq">   <span class="comment">Optimal Œ≤ schedule (Srinivas 2010): Œ≤_t = 2log(|ùí≥|t¬≤œÄ¬≤/6Œ¥) for finite ùí≥,</span></span>
  <span class="eq">   <span class="comment">or Œ≤_t = 2log(t^(d/2+2) œÄ¬≤/3Œ¥) for continuous ùí≥ ‚äÜ ‚Ñù·µà.</span></span>
  <span class="eq">   <span class="comment">UCB has the strongest theoretical convergence guarantees of the three.</span></span>
</div>

<div class="notebook-card">
  <div class="notebook-card-hdr">
    <span class="nb-title">Notebook 3 ‚Äî Acquisition Functions & Full BO Loop</span>
    <span class="nb-badge">Runnable Python</span>
  </div>
  <div class="notebook-card-body">
    <p>Implements EI, PI, and UCB from scratch. Visualises all three on the same GP posterior, runs a complete BO loop on a multi-modal function, compares BO vs random search, and demonstrates the exploration‚Äìexploitation spectrum.</p>
    <ul>
      <li>PI, EI, UCB: same GP posterior ‚Üí three different next points</li>
      <li>Full Bayesian Optimisation loop with EI (step-by-step visualization)</li>
      <li>BO vs Random Search convergence (20 runs, shaded IQR)</li>
      <li>UCB Œ≤ spectrum: Œ≤=0.01 (exploit) to Œ≤=20 (explore)</li>
      <li>Simple regret and cumulative regret curves</li>
    </ul>
    <div class="notebook-links">
      <a href="https://colab.research.google.com/github/OmkarRayAI/omkarray/blob/main/notebooks/03_acquisition_functions.ipynb" target="_blank" class="nb-colab">‚ñ∂ Open in Colab</a>
      <a href="https://github.com/OmkarRayAI/omkarray/blob/main/notebooks/03_acquisition_functions.ipynb" target="_blank" class="nb-github">View on GitHub</a>
      <a href="https://nbviewer.org/github/OmkarRayAI/omkarray/blob/main/notebooks/03_acquisition_functions.ipynb" target="_blank" class="nb-nbviewer">nbviewer</a>
    </div>
  </div>
</div>
<!-- ¬ß5 EXPLORATION-EXPLOITATION -->
<div class="section">
  <div class="sec-n">¬ß 5</div>
  <div>
    <div class="sec-title">The Exploration‚ÄìExploitation Spectrum</div>
    <div class="sec-sub">How Œ≤ in UCB and Œæ in EI control the trade-off ‚Äî and the optimal schedule</div>
  </div>
</div>

<p class="body">Every acquisition function embeds a trade-off between querying where the mean is high (exploitation) and where uncertainty is high (exploration). The key insight is that <em>both are necessary</em>: a purely exploitative strategy converges to a local optimum near the initial observations; a purely exploratory strategy wastes evaluations uniformly.</p>

<div class="diagram">
  <div class="diagram-hdr">Exhibit 5 ‚Äî Exploration‚ÄìExploitation Spectrum via UCB Œ≤ Parameter <span>Œ≤ controls the confidence width</span></div>
  <div class="diagram-body">
    <div class="spectrum">
      <div class="sp-row">
        <div class="sp-lbl"><strong>Œ≤ ‚Üí 0</strong><span>pure exploitation</span></div>
        <div class="sp-viz"><div class="bar-track"><div class="bar-fill" style="width:8%;background:#9b2335;">exploit</div></div></div>
        <div class="sp-note"><span class="m">Œ±_UCB ‚âà Œº‚Çô(x)</span> ‚Äî always query where the posterior mean is highest. Greedy. Converges quickly to a local optimum near initial observations. <strong>Will miss global maximum if initialisation is poor.</strong></div>
      </div>
      <div class="sp-row">
        <div class="sp-lbl"><strong>Œ≤ = 0.1‚Äì1.0</strong><span>mild exploration</span></div>
        <div class="sp-viz"><div class="bar-track"><div class="bar-fill" style="width:30%;background:#8a5a00;">exploit / explore</div></div></div>
        <div class="sp-note">Slight preference for unexplored regions when mean values are comparable. Good for smooth, well-behaved functions where exploration cost is low relative to improvement gained.</div>
      </div>
      <div class="sp-row" style="border-left:3px solid #1e6b3c;">
        <div class="sp-lbl" style="background:rgba(30,107,60,0.06);"><strong>Œ≤ ‚âà 2 log(t)</strong><span>‚òÖ optimal schedule</span></div>
        <div class="sp-viz"><div class="bar-track"><div class="bar-fill" style="width:55%;background:#1e6b3c;">balanced ‚Äî increases with t</div></div></div>
        <div class="sp-note"><strong>Srinivas et al. (2010):</strong> setting <span class="m">Œ≤_t = 2 log(t^(d/2+2)œÄ¬≤/3Œ¥)</span> yields sublinear cumulative regret <span class="m">R_T = O(‚àöT ¬∑ Œ≥_T log T)</span> with probability <span class="m">1‚àíŒ¥</span>. Œ≤ grows with time to maintain coverage as <span class="m">ùí≥</span> is increasingly explored.</div>
      </div>
      <div class="sp-row">
        <div class="sp-lbl"><strong>Œ≤ = 5‚Äì10</strong><span>heavy exploration</span></div>
        <div class="sp-viz"><div class="bar-track"><div class="bar-fill" style="width:78%;background:#1a4a7a;">explore</div></div></div>
        <div class="sp-note">Strongly prefers uncertain regions. Useful in early iterations (small <span class="m">n</span>, poor coverage) or when <span class="m">f</span> is highly multi-modal and missing the global optimum is catastrophic.</div>
      </div>
      <div class="sp-row">
        <div class="sp-lbl"><strong>Œ≤ ‚Üí ‚àû</strong><span>pure exploration</span></div>
        <div class="sp-viz"><div class="bar-track"><div class="bar-fill" style="width:100%;background:#5a2080;">explore only</div></div></div>
        <div class="sp-note"><span class="m">Œ±_UCB ‚âà œÉ‚Çô(x)</span> ‚Äî always query the most uncertain point. Equivalent to maximum variance sampling. Wastes evaluations in regions confirmed to have low <span class="m">f</span>. <strong>Exploration without exploitation is random search.</strong></div>
      </div>
    </div>
  </div>
  <div class="diagram-note">The EI analogue: <span class="m">Œæ</span> in <span class="m">Œ±_EI(x; Œæ) = (Œº‚Çô(x) ‚àí f* ‚àí Œæ)Œ¶(Z) + œÉ‚Çô(x)œÜ(Z)</span>. Large <span class="m">Œæ</span> requires <span class="m">f(x)</span> to exceed <span class="m">f*</span> by more than <span class="m">Œæ</span> to be worth exploring ‚Äî forces exploration of uncertain regions. In practice, <span class="m">Œæ = 0.01</span> is the most common default for EI (Lizotte 2008).</div>
</div>

<!-- ¬ß6 HYPERPARAMETER MARGINALISATION -->
<div class="section">
  <div class="sec-n">¬ß 6</div>
  <div>
    <div class="sec-title">Hyperparameter Marginalisation</div>
    <div class="sec-sub">Why integrating out Œ∏ is correct ‚Äî and how it's approximated in practice</div>
  </div>
</div>

<p class="body">The GP has hyperparameters <span class="m">Œ∏ = {‚Ñì, œÉ_f, œÉ_n, ‚Ä¶}</span> (length-scale, signal variance, noise variance, etc.). Most implementations fit <span class="m">Œ∏</span> by maximum marginal likelihood (type-II MLE) and treat it as fixed. This is a <em>point estimate</em>, which can be overconfident when data is sparse.</p>

<p class="body">The fully Bayesian approach places a prior <span class="m">p(Œ∏)</span> over hyperparameters and <em>integrates them out</em>:</p>

<div class="math-block">
  <span class="label">Fully Marginalised Acquisition ‚Äî integrating over GP hyperparameters</span>
  <span class="eq"> </span>
  <span class="eq">Œ±(x | D‚Çô)  =  ‚à´  Œ±(x | D‚Çô, Œ∏) ¬∑ p(Œ∏ | D‚Çô)  dŒ∏</span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">where:  p(Œ∏ | D‚Çô) ‚àù p(y | X, Œ∏) ¬∑ p(Œ∏)   ‚Üê posterior over hyperparameters</span></span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">This integral has no closed form. Approximated by:</span></span>
  <span class="eq">  (a)  MCMC sampling:  Œ±(x) ‚âà (1/S) Œ£‚Çõ Œ±(x | D‚Çô, Œ∏À¢),   Œ∏À¢ ~ p(Œ∏|D‚Çô)  via HMC</span>
  <span class="eq">  (b)  Slice sampling (SPEARMINT, Snoek et al. 2012)</span>
  <span class="eq">  (c)  Laplace approximation:  p(Œ∏|D‚Çô) ‚âà ùí©(Œ∏_MAP, [‚àí‚àá¬≤ log p]‚Åª¬π)</span>
</div>

<div class="two-col">
  <div class="note-box">
    <h4>Point Estimate (Type-II MLE)</h4>
    <p><span class="m">Œ∏* = arg max_Œ∏ log p(y | X, Œ∏)</span>. Fast: single L-BFGS run. Used in BoTorch, GPyOpt defaults. Overconfident: the acquisition function behaves as if <span class="m">Œ∏*</span> is known exactly, which underestimates uncertainty in <span class="m">Œ∏</span> ‚Äî especially dangerous with few observations (<span class="m">n &lt; 20</span>). Can lead to convergence to a suboptimal local maximum.</p>
  </div>
  <div class="note-box">
    <h4>Full Marginalisation (MCMC)</h4>
    <p>Treats GP hyperparameters as random variables with a prior. Averages the acquisition function over the posterior <span class="m">p(Œ∏ | D‚Çô)</span>. More expensive but provides correct uncertainty quantification. Particularly important when the length-scale is poorly identified from few points. SPEARMINT used this; GPflowOpt and BoTorch support it via NUTS/HMC.</p>
  </div>
</div>

<p class="body"><strong>When marginalisation matters most:</strong> in very low-data regimes (<span class="m">n ‚â§ 10</span>), when the kernel structure is uncertain, or when the acquisition function is used to make high-stakes decisions. In typical hyperparameter tuning with budgets of <span class="m">T = 50‚Äì200</span>, Type-II MLE performs comparably to full marginalisation and is far cheaper.</p>

<!-- ¬ß7 CONVERGENCE -->
<div class="section">
  <div class="sec-n">¬ß 7</div>
  <div>
    <div class="sec-title">Convergence Theory ‚Äî Regret Analysis</div>
    <div class="sec-sub">What theoretical guarantees actually say ‚Äî and what they do not</div>
  </div>
</div>

<p class="body">Convergence of BO is analysed through the lens of <em>regret</em> ‚Äî the gap between the values queried and the true optimum. There are two distinct notions:</p>

<div class="regret-grid">
  <div class="rg-cell">
    <div class="rg-name">Simple Regret  r_T</div>
    <div class="rg-formula">r_T  =  f(x*)  ‚àí  max_{t‚â§T} f(x_t)</div>
    <div class="rg-body">The gap between the true maximum and the best point found after <span class="m">T</span> evaluations. Measures final quality. A Bayesian optimiser has converged when <span class="m">r_T ‚Üí 0</span>. This is the quantity optimised in the recommendation step: return <span class="m">xÃÇ* = arg max_{t‚â§T} f(x_t)</span>.</div>
  </div>
  <div class="rg-cell">
    <div class="rg-name">Cumulative Regret  R_T</div>
    <div class="rg-formula">R_T  =  Œ£_{t=1}^T [ f(x*)  ‚àí  f(x_t) ]</div>
    <div class="rg-body">The total suboptimality accumulated over all <span class="m">T</span> queries. Used to bound simple regret: <span class="m">r_T ‚â§ R_T / T</span>. An algorithm is <em>no-regret</em> if <span class="m">R_T / T ‚Üí 0</span>. This requires that the algorithm continues to improve ‚Äî not just one lucky query.</div>
  </div>
</div>

<div class="math-block">
  <span class="label">UCB Regret Bound ‚Äî Srinivas, Krause, Kakade, Seeger (2010) ‚Äî ICML Best Paper</span>
  <span class="eq"> </span>
  <span class="eq">With probability ‚â• 1 ‚àí Œ¥, UCB-BO with Œ≤_t = 2 log(|ùí≥|t¬≤œÄ¬≤/6Œ¥) satisfies:</span>
  <span class="eq"> </span>
  <span class="eq">   R_T  ‚â§  ‚àö( C‚ÇÅ ¬∑ T ¬∑ Œ≤_T ¬∑ Œ≥_T )</span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">where: Œ≥_T = max information gain achievable by T points about f</span></span>
  <span class="eq"><span class="comment">       C‚ÇÅ = 8/log(1+œÉ‚Åª¬≤_n)   (constant depending on noise level)</span></span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">Œ≥_T depends on the kernel:</span></span>
  <span class="eq">  RBF kernel:      Œ≥_T  =  O( (log T)^(d+1) )         <span class="comment">‚Üê sublinear, fast convergence</span></span>
  <span class="eq">  Mat√©rn ŒΩ kernel: Œ≥_T  =  O( T^(d/2ŒΩ+d) ¬∑ (log T)^s ) <span class="comment">‚Üê depends on smoothness ŒΩ</span></span>
  <span class="eq">  Linear kernel:   Œ≥_T  =  O( d log T )                <span class="comment">‚Üê d-dimensional linear function</span></span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">For RBF:  R_T = O( ‚àöT ¬∑ (log T)^(d+1) ),  so  R_T/T ‚Üí 0  (no-regret)</span></span>
  <span class="eq"><span class="comment">Simple regret: r_T ‚â§ R_T/T = O( (log T)^(d+1) / ‚àöT ) ‚Üí 0 as T ‚Üí ‚àû</span></span>
</div>

<p class="body"><strong>What the theorem does not say:</strong> it does not provide useful bounds for the finite budgets typical in practice (<span class="m">T = 50</span>). The constants hidden in the <span class="m">O(¬∑)</span> notation are large and problem-dependent. The bound holds for the <em>exact</em> GP posterior with known hyperparameters ‚Äî in practice, hyperparameters are estimated, which violates the theoretical assumptions. Convergence guarantees are asymptotic results; practical performance is empirical.</p>

<div class="math-block">
  <span class="label">Maximum Information Gain Œ≥_T ‚Äî the kernel-specific bottleneck</span>
  <span class="eq"> </span>
  <span class="eq">Œ≥_T  =  max_{A‚äÇùí≥, |A|=T}  I(y_A ; f)  =  ¬Ω log |I + œÉ‚Åª¬≤_n K(A,A)|</span>
  <span class="eq"> </span>
  <span class="eq"><span class="comment">I(y_A ; f) is the mutual information between T observations y_A and the function f.</span></span>
  <span class="eq"><span class="comment">Œ≥_T measures how quickly observations "fill in" the function. Smoother kernels</span></span>
  <span class="eq"><span class="comment">(RBF) have low Œ≥_T ‚Äî a few points reveal a lot. Rough kernels have high Œ≥_T.</span></span>
  <span class="eq"><span class="comment">The d-dimensional scaling Œ≥_T ‚àù d is why BO degrades in high dimensions.</span></span>
</div>
<!-- ¬ß8 WHEN BO BEATS GD -->
<div class="section">
  <div class="sec-n">¬ß 8</div>
  <div>
    <div class="sec-title">When BO Dominates Gradient Descent ‚Äî and When It Doesn't</div>
    <div class="sec-sub">The decision boundary is a function of dimensionality, budget, and differentiability</div>
  </div>
</div>

<p class="body">This is a mathematical decision, not a heuristic one. The crossover point between BO and gradient-based methods is determined by three quantities: the <strong>evaluation budget</strong> <span class="m">T</span>, the <strong>input dimension</strong> <span class="m">d</span>, and <strong>gradient availability</strong>.</p>

<div class="diagram">
  <div class="diagram-hdr">Exhibit 6 ‚Äî The BO vs. GD Decision Boundary <span>as a function of evaluation budget and dimensionality</span></div>
  <div class="diagram-body">
    <svg viewBox="0 0 800 280" xmlns="http://www.w3.org/2000/svg" width="100%" style="display:block;">
      <rect width="800" height="280" fill="white"/>
      <line x1="60" y1="20" x2="60" y2="240" stroke="#1c1c1c" stroke-width="1.5"/>
      <line x1="60" y1="240" x2="780" y2="240" stroke="#1c1c1c" stroke-width="1.5"/>
      <text x="420" y="270" font-family="'JetBrains Mono',monospace" font-size="10" fill="#1c1c1c" text-anchor="middle" letter-spacing="1">INPUT DIMENSION  d  ‚Üí</text>
      <text x="20" y="130" font-family="'JetBrains Mono',monospace" font-size="10" fill="#1c1c1c" text-anchor="middle" transform="rotate(-90,20,130)" letter-spacing="1">BUDGET  T  ‚Üí</text>
      <text x="100" y="255" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">d=2</text>
      <text x="200" y="255" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">d=5</text>
      <text x="340" y="255" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">d=10</text>
      <text x="540" y="255" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">d=20</text>
      <text x="740" y="255" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">d=50+</text>
      <text x="55" y="220" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="end">T=10</text>
      <text x="55" y="180" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="end">T=50</text>
      <text x="55" y="130" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="end">T=200</text>
      <text x="55" y="70" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="end">T=1000+</text>
      <path d="M60,240 C100,240 200,238 300,235 C380,232 430,225 470,210 C500,200 520,180 530,160 C535,140 535,80 530,30 L60,30 Z" fill="rgba(30,107,60,0.1)" stroke="none"/>
      <path d="M530,30 C535,80 535,140 530,160 C520,180 500,200 470,210 C430,225 380,232 300,235 C200,238 100,240 60,240 L780,240 L780,30 Z" fill="rgba(26,74,122,0.07)" stroke="none"/>
      <path d="M60,30 C60,80 100,140 200,185 C280,218 380,232 500,238 C600,242 680,241 780,240" fill="none" stroke="#1c1c1c" stroke-width="2"/>
      <text x="150" y="80" font-family="'EB Garamond',serif" font-size="15" fill="#1e6b3c" font-weight="500">BO dominates</text>
      <text x="150" y="98" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1e6b3c">low d, expensive f</text>
      <text x="150" y="110" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1e6b3c">T ‚â§ ~500 realistic</text>
      <text x="150" y="122" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1e6b3c">no gradient available</text>
      <text x="600" y="80" font-family="'EB Garamond',serif" font-size="15" fill="#1a4a7a" font-weight="500">GD dominates</text>
      <text x="600" y="98" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1a4a7a">high d, cheap f</text>
      <text x="600" y="110" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1a4a7a">T &gt;&gt; 10‚Å¥ feasible</text>
      <text x="600" y="122" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1a4a7a">gradient available</text>
      <circle cx="200" cy="180" r="6" fill="white" stroke="#1e6b3c" stroke-width="2"/>
      <text x="215" y="172" font-family="'JetBrains Mono',monospace" font-size="8" fill="#1e6b3c">hyperparameter</text>
      <text x="215" y="182" font-family="'JetBrains Mono',monospace" font-size="8" fill="#1e6b3c">tuning (d‚â§10)</text>
      <circle cx="340" cy="130" r="6" fill="white" stroke="#8a5a00" stroke-width="2"/>
      <text x="355" y="122" font-family="'JetBrains Mono',monospace" font-size="8" fill="#8a5a00">NAS / drug</text>
      <text x="355" y="132" font-family="'JetBrains Mono',monospace" font-size="8" fill="#8a5a00">discovery</text>
      <circle cx="740" cy="60" r="6" fill="white" stroke="#1a4a7a" stroke-width="2"/>
      <text x="688" y="46" font-family="'JetBrains Mono',monospace" font-size="8" fill="#1a4a7a">NN weight</text>
      <text x="688" y="56" font-family="'JetBrains Mono',monospace" font-size="8" fill="#1a4a7a">training</text>
      <text x="450" y="195" font-family="'JetBrains Mono',monospace" font-size="9" fill="#1c1c1c" text-anchor="middle">crossover boundary</text>
      <text x="450" y="206" font-family="'JetBrains Mono',monospace" font-size="9" fill="#6e6e6e" text-anchor="middle">approx: BO effective when d ‚â§ 20, T ‚â§ 500</text>
    </svg>
  </div>
  <div class="diagram-note">The curse of dimensionality strikes BO through <span class="m">Œ≥_T</span>: in high dimensions, the number of observations needed to adequately cover <span class="m">ùí≥</span> grows exponentially. The practical threshold is approximately <span class="m">d ‚â§ 20</span> for standard GP-BO; for <span class="m">d &gt; 20</span>, random embeddings (REMBO), additive decompositions, or gradient-enhanced GPs become necessary.</div>
</div>

<table class="ex-table">
  <thead>
    <tr>
      <th style="width:200px;">Regime</th>
      <th style="width:120px;">Use BO?</th>
      <th>Mathematical Reason</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td class="key">d ‚â§ 20, T ‚â§ 500, ‚àáf unavailable</td>
      <td style="color:#1e6b3c;font-weight:500;">‚úì Yes, GP-BO</td>
      <td>Œ≥_T sublinear in T; GP posterior informative; acquisition function cheaper than f per query</td>
    </tr>
    <tr>
      <td class="key">d ‚â§ 5, T ‚â§ 30</td>
      <td style="color:#1e6b3c;font-weight:500;">‚úì Yes, strong</td>
      <td>Low d ‚Üí GP posterior tight with few points; EI efficiently concentrates evaluations near x*</td>
    </tr>
    <tr>
      <td class="key">20 &lt; d ‚â§ 100, T ‚â§ 1000</td>
      <td style="color:#8a5a00;font-weight:500;">‚ö† BO with tricks</td>
      <td>REMBO (random embedding), HESBO, or additive GP decompositions extend BO to moderate d. Vanilla GP-BO fails.</td>
    </tr>
    <tr>
      <td class="key">d &gt; 100, ‚àáf available</td>
      <td style="color:#9b2335;font-weight:500;">‚úó Use GD</td>
      <td>GD amortises per-iteration cost over the full gradient; BO GP cost O(n¬≥) cannot compete at scale</td>
    </tr>
    <tr>
      <td class="key">f differentiable, T &gt; 10‚Å¥ feasible</td>
      <td style="color:#9b2335;font-weight:500;">‚úó Use GD</td>
      <td>Each GD step is O(d) vs. BO's O(n¬≥) + inner optimisation. The gradient is an exponentially more informative signal.</td>
    </tr>
    <tr>
      <td class="key">f noisy (œÉ_n large), d ‚â§ 10</td>
      <td style="color:#1e6b3c;font-weight:500;">‚úì BO handles noise</td>
      <td>GP naturally models observation noise œÉ¬≤_n. The posterior mean is an automatic smoothed estimate. GD on noisy f is ill-posed without careful variance reduction (SVRG, SAG).</td>
    </tr>
    <tr>
      <td class="key">f is a neural network (d = 10‚Å∂+)</td>
      <td style="color:#9b2335;font-weight:500;">‚úó Definitively GD</td>
      <td>Training a GP with 10‚Å∂-dimensional inputs is computationally impossible. Adam + backprop is the only viable strategy. BO is restricted to the hyperparameter space of the network, not its weights.</td>
    </tr>
  </tbody>
</table>

<!-- ¬ß9 EXTENSIONS -->
<div class="section">
  <div class="sec-n">¬ß 9</div>
  <div>
    <div class="sec-title">Extensions and Active Research Directions</div>
    <div class="sec-sub">Beyond the standard formulation ‚Äî the mathematical frontier</div>
  </div>
</div>

<div class="diagram">
  <div class="diagram-hdr">Exhibit 7 ‚Äî Insights Pyramid: Standard Formulation to Open Research <span>layered from known to frontier</span></div>
  <div class="diagram-body">
    <div class="pyramid">
      <div class="pyr-row" style="background:rgba(0,0,0,.02);">
        <div class="pyr-main">
          <div class="pyr-t">Standard GP-BO: GP surrogate + EI/UCB acquisition + MLE hyperparameters</div>
          <div class="pyr-d">The formulation in ¬ß2‚Äì5. Works for d‚â§10, T‚â§200, noise-free or mildly noisy f.</div>
        </div>
        <div class="pyr-b" style="background:rgba(0,0,0,.06);border-color:var(--faint);color:var(--muted);">STANDARD</div>
      </div>
      <div class="pyr-row" style="background:rgba(26,74,122,.04);">
        <div class="pyr-main">
          <div class="pyr-t">Batch BO: selecting q &gt; 1 points per iteration (parallel evaluations)</div>
          <div class="pyr-d">q-EI: ùîº[max(f(x‚ÇÅ)‚Ä¶f(xq)) ‚àí f*] ‚Äî analytically intractable for q&gt;1; approximated by Monte Carlo or fantasisation. Used when evaluations can run in parallel (e.g., k8s job farm).</div>
        </div>
        <div class="pyr-b" style="border-color:#1a4a7a;color:#1a4a7a;">KNOWN EXTENSION</div>
      </div>
      <div class="pyr-row" style="background:rgba(30,107,60,.04);">
        <div class="pyr-main">
          <div class="pyr-t">Constrained BO: optimise f(x) subject to c·µ¢(x) ‚â• 0</div>
          <div class="pyr-d">Model each constraint with a separate GP: p(c·µ¢(x) ‚â• 0 | D‚Çô). Acquisition becomes Œ±(x) ¬∑ P(feasible | x). SCBO (scalable constrained BO) extends to trust regions for stability.</div>
        </div>
        <div class="pyr-b" style="border-color:#1e6b3c;color:#1e6b3c;">ACTIVE AREA</div>
      </div>
      <div class="pyr-row" style="background:rgba(138,90,0,.04);">
        <div class="pyr-main">
          <div class="pyr-t">High-dimensional BO: REMBO, ALEBO, Additive BO, TuRBO (trust regions)</div>
          <div class="pyr-d">REMBO: embed ùí≥ ‚äÜ ‚Ñù·µà into a low-dimensional subspace ‚Ñù·µè (k&lt;&lt;d) via random matrix A ‚àà ‚Ñù^{d√ók}. Effective if f has low intrinsic dimensionality. TuRBO: restricts BO to a trust region that shrinks/grows based on success ‚Äî avoids the exploration-exploitation pathology in high d.</div>
        </div>
        <div class="pyr-b" style="border-color:#8a5a00;color:#8a5a00;">ACTIVE AREA</div>
      </div>
      <div class="pyr-row" style="background:rgba(90,32,128,.04);">
        <div class="pyr-main">
          <div class="pyr-t">Non-GP surrogates: Random Forests (SMAC), TPE, Deep Kernel Learning</div>
          <div class="pyr-d">SMAC (Hutter 2011) uses random forests as surrogate ‚Äî handles categorical inputs and conditional hyperparameters natively. TPE (Bergstra 2011) models p(x|y) rather than p(y|x). DKL (Wilson 2016): deep neural network feature extractor + GP, combining scalability with posterior quality. These abandon closed-form posteriors for scalability.</div>
        </div>
        <div class="pyr-b" style="border-color:#5a2080;color:#5a2080;">FRONTIER</div>
      </div>
      <div class="pyr-row" style="background:rgba(155,35,53,.06);border-left:3px solid var(--red);">
        <div class="pyr-main">
          <div class="pyr-t">BO for LLMs: gradient-free fine-tuning, prompt optimisation, RLHF loop design</div>
          <div class="pyr-d">As LLM evaluation becomes the expensive oracle ‚Äî A/B test, human evaluation, downstream task performance ‚Äî BO over the prompt/adapter/configuration space is an active area. Key challenge: the response variable is itself a distribution, not a scalar. Multi-objective BO (Pareto front) handles simultaneous optimisation of capability, safety, and efficiency metrics without reduction to a single scalar.</div>
        </div>
        <div class="pyr-b" style="border-color:var(--red);color:var(--red);">OPEN FRONTIER</div>
      </div>
    </div>
  </div>
  <div class="diagram-note">The deepest open question in BO theory: for what function classes does the cumulative regret of EI satisfy the same sublinear bound as UCB? UCB has O(‚àöT¬∑Œ≥_T log T) guarantees under mild conditions. EI's convergence is harder to prove because it does not directly control the exploration-exploitation trade-off through an explicit parameter ‚Äî yet empirically EI often outperforms UCB. The gap between theory and practice here is substantial.</div>
</div>

<!-- FOOTER -->
<hr class="rule">
<div class="doc-footer">
  <div>
    References: Kushner 1964 ¬∑ Mockus 1978 ¬∑ Srinivas, Krause, Kakade, Seeger 2010 (ICML Best Paper) ¬∑<br>
    Snoek, Larochelle, Adams 2012 ¬∑ Bergstra &amp; Bengio 2012 ¬∑ Hutter, Hoos, Leyton-Brown 2011 ¬∑<br>
    Wilson &amp; Adams 2013 ¬∑ Wang et al. (REMBO) 2013 ¬∑ Eriksson et al. (TuRBO) 2019 ¬∑ Garnett 2023
  </div>
  <div style="text-align:right;">
    Mathematical Deep Dive ‚Äî Bayesian Optimisation<br>
    February 2026
  </div>
</div>

</body>
</html>