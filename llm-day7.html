<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Day 7 &mdash; Tensors, Broadcasting &amp; torch.Tensor Deep Dive</title>
<link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;1,400&family=Source+Serif+4:ital,wght@0,300;0,400;0,600;1,300;1,400&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
<style>
:root{--ink:#1a1a2e;--cream:#faf8f4;--cream-dark:#f0ece4;--accent:#c0392b;--blue:#2c3e6b;--green:#27654a;--gold:#d4a843;--gray:#7f8c8d;--gray-light:#bdc3c7;--purple:#5b2c6f;--orange:#d35400;}
*{margin:0;padding:0;box-sizing:border-box;}
body{background:var(--cream);color:var(--ink);font-family:'Source Serif 4',Georgia,serif;line-height:1.7;}
.progress-strip{position:sticky;top:0;z-index:100;background:var(--ink);padding:0.6rem 2rem;display:flex;align-items:center;gap:0.4rem;font-family:'JetBrains Mono',monospace;font-size:0.65rem;color:#888;}
.p-dot{width:8px;height:8px;border-radius:50%;border:1.5px solid #555;flex-shrink:0;}
.p-dot.done{background:var(--green);border-color:var(--green);}
.p-dot.now{background:var(--accent);border-color:var(--accent);box-shadow:0 0 6px rgba(192,57,43,0.5);}
.p-line{width:24px;height:1.5px;background:#444;flex-shrink:0;}.p-line.done{background:var(--green);}
.p-label{color:#666;margin-left:0.3rem;margin-right:0.5rem;}.p-label.done{color:var(--green);}.p-label.now{color:var(--accent);font-weight:600;}
.back-link{display:block;font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:.18em;text-transform:uppercase;color:var(--gray);text-decoration:none;padding:1.2rem 2rem 0;max-width:1100px;margin:0 auto;transition:color .2s;}.back-link:hover{color:var(--accent);}
.header{padding:2rem 2rem 2rem;max-width:1100px;margin:0 auto;border-bottom:3px double var(--ink);}
.header-meta{font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:0.15em;text-transform:uppercase;color:var(--gray);margin-bottom:0.5rem;}
.phase-tag{background:var(--ink);color:white;padding:0.15rem 0.5rem;border-radius:2px;font-size:0.62rem;margin-right:0.4rem;}
.header h1{font-family:'Playfair Display',serif;font-size:clamp(2rem,5vw,3.1rem);font-weight:700;line-height:1.15;margin-bottom:0.5rem;}
.header h1 span{color:var(--accent);}
.header-sub{font-style:italic;font-size:1.05rem;color:var(--blue);max-width:700px;}
.content{max-width:1100px;margin:0 auto;padding:2rem 2rem 3rem;}
.memo-quote{background:var(--cream-dark);border-left:4px solid var(--accent);padding:1.5rem 2rem;margin:0 0 3rem;font-style:italic;font-size:1.02rem;color:var(--blue);position:relative;}
.memo-quote::before{content:'"\27';font-family:'Playfair Display',serif;font-size:4rem;color:var(--accent);position:absolute;top:-0.5rem;left:0.5rem;opacity:0.25;}
.memo-quote .attr{display:block;margin-top:0.6rem;font-style:normal;font-size:0.8rem;color:var(--gray);font-family:'JetBrains Mono',monospace;}
.sh{font-family:'Playfair Display',serif;font-size:1.55rem;font-weight:700;margin:3rem 0 1.25rem;padding-bottom:0.5rem;border-bottom:1px solid var(--gray-light);}
.sh .n{color:var(--accent);font-style:italic;margin-right:0.2rem;}
.prose{font-size:1rem;max-width:720px;margin-bottom:1.25rem;}
.prose strong{color:var(--blue);}
.prose code,code{font-family:'JetBrains Mono',monospace;font-size:0.83rem;background:var(--cream-dark);padding:0.1rem 0.35rem;border-radius:2px;color:var(--accent);}
.vf{background:white;border:1px solid var(--gray-light);border-radius:2px;padding:2rem 1.5rem 1.5rem;margin:1.5rem 0 2rem;position:relative;overflow:hidden;}
.vf::before{content:'';position:absolute;top:0;left:0;right:0;height:3px;}
.vf.green::before{background:linear-gradient(90deg,var(--green),var(--blue));}
.vf.gold::before{background:linear-gradient(90deg,var(--gold),var(--accent));}
.vf.purple::before{background:linear-gradient(90deg,var(--purple),var(--blue));}
.vf-label{font-family:'JetBrains Mono',monospace;font-size:0.68rem;letter-spacing:0.18em;text-transform:uppercase;color:var(--gray);margin-bottom:1rem;}
svg text{font-family:'Source Serif 4',Georgia,serif;}
svg .m{font-family:'JetBrains Mono',monospace;}
.box{border-radius:2px;padding:1.4rem 1.8rem;margin:2rem 0;position:relative;}
.box::before{position:absolute;top:-0.75rem;left:1rem;font-size:1.2rem;background:var(--cream);padding:0 0.5rem;}
.box h4{font-family:'Playfair Display',serif;margin-bottom:0.4rem;font-size:1rem;}
.box p{font-size:0.93rem;color:#555;}
.box.insight{background:linear-gradient(135deg,#fdf6e8,#fef9f0);border:1px solid var(--gold);}.box.insight::before{content:'\1F4A1';}.box.insight h4{color:var(--gold);}
.box.danger{background:linear-gradient(135deg,#fdf0ef,#fef5f4);border:1px solid var(--accent);}.box.danger::before{content:'\26A0\FE0F';}.box.danger h4{color:var(--accent);}
.code-block{background:#1e1e2e;color:#cdd6f4;border-radius:4px;padding:1.5rem;margin:1.5rem 0;overflow-x:auto;font-family:'JetBrains Mono',monospace;font-size:0.82rem;line-height:1.8;}
.code-block .comment{color:#6c7086;}.code-block .keyword{color:#cba6f7;}.code-block .string{color:#a6e3a1;}.code-block .number{color:#fab387;}.code-block .func{color:#89b4fa;}.code-block .class-name{color:#f9e2af;}.code-block .op{color:#89dceb;}.code-block .self{color:#f38ba8;}
.mx{display:grid;grid-template-columns:auto 1fr 1fr;grid-template-rows:auto 1fr 1fr;gap:0;margin:2rem 0;background:white;border:1px solid var(--gray-light);border-radius:2px;overflow:hidden;}
.mx-corner{background:var(--ink);padding:0.8rem;}
.mx-ch{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;text-align:center;display:flex;align-items:center;justify-content:center;}
.mx-rh{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;writing-mode:vertical-lr;text-orientation:mixed;transform:rotate(180deg);display:flex;align-items:center;justify-content:center;}
.mx-cell{padding:1.2rem;border:1px solid var(--cream-dark);}.mx-cell h4{font-family:'Playfair Display',serif;font-size:0.95rem;margin-bottom:0.4rem;}.mx-cell p{font-size:0.83rem;color:#555;line-height:1.5;}.mx-cell.best{background:#f0faf4;}.mx-cell.worst{background:#fdf0ef;}.mx-cell .e{font-size:1.4rem;display:block;margin-bottom:0.4rem;}
.mx-cell code{font-family:'JetBrains Mono',monospace;font-size:0.8rem;background:var(--cream-dark);padding:0.1rem 0.3rem;border-radius:2px;color:var(--accent);}
.cl{list-style:none;margin:1.5rem 0;}.cl li{padding:0.55rem 0 0.55rem 2rem;position:relative;font-size:0.93rem;border-bottom:1px dotted var(--gray-light);}.cl li::before{content:'\2610';position:absolute;left:0;color:var(--accent);font-size:1.1rem;}.cl li strong{font-family:'JetBrains Mono',monospace;font-size:0.8rem;color:var(--blue);}
.footer{max-width:1100px;margin:0 auto;padding:2rem;border-top:3px double var(--ink);display:flex;justify-content:space-between;align-items:center;font-family:'JetBrains Mono',monospace;font-size:0.68rem;color:var(--gray);text-transform:uppercase;letter-spacing:0.1em;flex-wrap:wrap;gap:0.5rem;}
.fi{opacity:0;transform:translateY(16px);animation:fu 0.5s ease forwards;}
@keyframes fu{to{opacity:1;transform:translateY(0);}}
.fi:nth-child(2){animation-delay:0.08s;}.fi:nth-child(3){animation-delay:0.16s;}.fi:nth-child(4){animation-delay:0.24s;}
@media(max-width:600px){.header{padding:1.5rem 1rem;}.content{padding:1.5rem 1rem;}.footer{flex-direction:column;text-align:center;}}

.notebook-card{margin:2.5rem 0;border:2px solid var(--ink);background:white;overflow:hidden;}
.notebook-card-hdr{padding:14px 20px;background:var(--ink);color:white;display:flex;justify-content:space-between;align-items:center;}
.notebook-card-hdr .nb-title{font-family:'JetBrains Mono',monospace;font-size:11px;letter-spacing:.08em;text-transform:uppercase;font-weight:600;}
.notebook-card-hdr .nb-badge{font-family:'JetBrains Mono',monospace;font-size:9px;padding:3px 10px;border:1px solid rgba(255,255,255,.3);letter-spacing:.08em;text-transform:uppercase;opacity:.7;}
.notebook-card-body{padding:20px;}
.notebook-card-body p{font-size:0.95rem;color:#555;line-height:1.7;margin-bottom:12px;}
.notebook-links{display:flex;gap:12px;flex-wrap:wrap;}
.notebook-links a{font-family:'JetBrains Mono',monospace;font-size:10px;letter-spacing:.06em;text-transform:uppercase;padding:8px 16px;text-decoration:none;border:1.5px solid;transition:all .15s;font-weight:600;}
.nb-colab{background:#f9ab00;color:#1c1c1c;border-color:#f9ab00;}.nb-colab:hover{background:#e09800;border-color:#e09800;}
.nb-github{background:white;color:var(--ink);border-color:var(--ink);}.nb-github:hover{background:var(--ink);color:white;}
.nb-nbviewer{background:white;color:#2c3e6b;border-color:#2c3e6b;}.nb-nbviewer:hover{background:#2c3e6b;color:white;}

</style>
</head>
<body>

<div class="progress-strip">
  <span class="p-dot done"></span><span class="p-label done">1</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">2</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">3</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">4</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">5</span><span class="p-line done"></span>
  <span class="p-dot done"></span><span class="p-label done">6</span><span class="p-line done"></span>
  <span class="p-dot now"></span><span class="p-label now">7</span><span class="p-line"></span>
  <span class="p-dot"></span><span class="p-label">8</span>
  <span style="color:#555;margin-left:0.5rem;">&middot; &middot; &middot;</span>
  <span class="p-dot" style="margin-left:0.5rem;"></span><span class="p-label">80</span>
</div>

<a href="index.html" class="back-link">&larr; Back to index</a>

<header class="header fi">
  <div class="header-meta"><span class="phase-tag">PHASE 1</span> Foundations &middot; Day 7 of 80 &middot; Neural Networks &amp; Backprop</div>
  <h1>Tensors, <span>Broadcasting</span> &amp; torch.Tensor Deep Dive</h1>
  <p class="header-sub">Master the data structure at the heart of deep learning. Understand shapes, strides, views, and the broadcasting rules that make vectorized code possible.</p>
</header>

<main class="content">

  <div class="memo-quote fi">
    A spreadsheet is a 2D grid of numbers. A tensor is an N-dimensional grid of numbers. Just as portfolio
    analytics requires fluency with spreadsheets, neural network work requires fluency with tensors. Broadcasting &mdash;
    the automatic expansion of shapes during arithmetic &mdash; is the single most important concept to internalize.
    Get it wrong, and silent shape mismatches corrupt your results. Get it right, and you write code that runs
    100&times; faster than loops.
    <span class="attr">&mdash; Day 7 Principle, adapted from the Marks framework</span>
  </div>

  <h2 class="sh fi"><span class="n">I.</span> Tensor Fundamentals &mdash; Shape, Stride, View</h2>
  <p class="prose fi">
    A tensor is a multi-dimensional array stored as a contiguous block of memory. Its <strong>shape</strong> tells you
    the dimensions (e.g., <code>[3, 4]</code> means 3 rows, 4 columns). Its <strong>stride</strong> tells you how many
    elements to skip to reach the next position along each dimension. A <strong>view</strong> creates a new tensor that
    shares the same underlying data but interprets it with a different shape.
  </p>

  <div class="fi">
    <div class="vf green">
      <div class="vf-label">Exhibit A &mdash; Tensor Shapes: Scalar &rarr; Vector &rarr; Matrix &rarr; 3D Tensor</div>
      <svg viewBox="0 0 800 180" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
        <rect x="20" y="60" width="40" height="40" rx="3" fill="#eef5ee" stroke="#27654a" stroke-width="1.5"/>
        <text x="40" y="85" text-anchor="middle" class="m" font-size="11" fill="#27654a">42</text>
        <text x="40" y="120" text-anchor="middle" class="m" font-size="9" fill="#888">shape: []</text>
        <text x="40" y="135" text-anchor="middle" class="m" font-size="8" fill="#aaa">0-D scalar</text>

        <rect x="120" y="40" width="30" height="90" rx="3" fill="#eef5ee" stroke="#27654a" stroke-width="1.5"/>
        <text x="135" y="62" text-anchor="middle" class="m" font-size="9" fill="#27654a">1</text>
        <text x="135" y="80" text-anchor="middle" class="m" font-size="9" fill="#27654a">2</text>
        <text x="135" y="98" text-anchor="middle" class="m" font-size="9" fill="#27654a">3</text>
        <text x="135" y="120" text-anchor="middle" class="m" font-size="9" fill="#888">shape: [3]</text>
        <text x="135" y="135" text-anchor="middle" class="m" font-size="8" fill="#aaa">1-D vector</text>

        <rect x="220" y="30" width="120" height="90" rx="3" fill="#eef5ee" stroke="#27654a" stroke-width="1.5"/>
        <text x="250" y="55" class="m" font-size="9" fill="#27654a">1</text>
        <text x="280" y="55" class="m" font-size="9" fill="#27654a">2</text>
        <text x="310" y="55" class="m" font-size="9" fill="#27654a">3</text>
        <text x="250" y="80" class="m" font-size="9" fill="#27654a">4</text>
        <text x="280" y="80" class="m" font-size="9" fill="#27654a">5</text>
        <text x="310" y="80" class="m" font-size="9" fill="#27654a">6</text>
        <text x="250" y="105" class="m" font-size="9" fill="#27654a">7</text>
        <text x="280" y="105" class="m" font-size="9" fill="#27654a">8</text>
        <text x="310" y="105" class="m" font-size="9" fill="#27654a">9</text>
        <text x="280" y="140" text-anchor="middle" class="m" font-size="9" fill="#888">shape: [3,3]</text>
        <text x="280" y="155" text-anchor="middle" class="m" font-size="8" fill="#aaa">2-D matrix</text>

        <rect x="420" y="20" width="110" height="80" rx="3" fill="#eef5ee" stroke="#27654a" stroke-width="1.5" opacity="0.5"/>
        <rect x="440" y="35" width="110" height="80" rx="3" fill="#eef5ee" stroke="#27654a" stroke-width="1.5" opacity="0.7"/>
        <rect x="460" y="50" width="110" height="80" rx="3" fill="#eef5ee" stroke="#27654a" stroke-width="1.5"/>
        <text x="515" y="80" text-anchor="middle" class="m" font-size="9" fill="#27654a">data</text>
        <text x="515" y="95" text-anchor="middle" class="m" font-size="9" fill="#27654a">here</text>
        <text x="490" y="150" text-anchor="middle" class="m" font-size="9" fill="#888">shape: [3,3,3]</text>
        <text x="490" y="165" text-anchor="middle" class="m" font-size="8" fill="#aaa">3-D tensor</text>

        <text x="700" y="80" text-anchor="middle" font-size="11" fill="#555" font-style="italic">N-D: same idea,</text>
        <text x="700" y="98" text-anchor="middle" font-size="11" fill="#555" font-style="italic">more dimensions</text>
      </svg>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">II.</span> Broadcasting Rules &mdash; The Three-Step Check</h2>
  <p class="prose fi">
    Broadcasting lets you perform arithmetic between tensors of different shapes without explicit copying.
    PyTorch follows NumPy&rsquo;s broadcasting rules. Align shapes from the <strong>right</strong>. At each dimension,
    sizes must be <strong>equal</strong> or one of them must be <strong>1</strong>.
  </p>

  <div class="fi" style="display:grid;grid-template-columns:1fr 1fr;gap:2px;border:1px solid var(--gray-light);border-radius:2px;overflow:hidden;margin:2rem 0;">
    <div style="background:var(--green);color:white;padding:0.8rem 1.2rem;font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:0.12em;text-transform:uppercase;text-align:center;font-weight:600;">&#10003; Compatible Shapes</div>
    <div style="background:var(--accent);color:white;padding:0.8rem 1.2rem;font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:0.12em;text-transform:uppercase;text-align:center;font-weight:600;">&#10007; Incompatible</div>
    <div style="padding:1rem 1.2rem;font-family:'JetBrains Mono',monospace;font-size:0.82rem;line-height:2;background:#f0faf4;">
      [3,4] + [&nbsp;&nbsp;&nbsp;4] &rarr; [3,4]<br>
      [3,4] + [3,1] &rarr; [3,4]<br>
      [3,1] + [1,4] &rarr; [3,4]<br>
      [2,3,4] + [4] &rarr; [2,3,4]
    </div>
    <div style="padding:1rem 1.2rem;font-family:'JetBrains Mono',monospace;font-size:0.82rem;line-height:2;background:#fef5f4;">
      [3,4] + [3] &rarr; ERROR<br>
      [3,4] + [2,4] &rarr; ERROR<br>
      <span style="color:#999">sizes 3 vs 2 &ne; 1</span><br>
      <span style="color:#999">must be equal or 1</span>
    </div>
  </div>

  <div class="code-block fi">
<span class="keyword">import</span> torch

<span class="comment"># Broadcasting in action</span>
a = torch.<span class="func">tensor</span>([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],
                  [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])     <span class="comment"># shape [2, 3]</span>
b = torch.<span class="func">tensor</span>([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>])  <span class="comment"># shape [3]</span>

c = a <span class="op">+</span> b  <span class="comment"># b broadcasts to [2, 3]</span>
<span class="comment"># tensor([[11, 22, 33],</span>
<span class="comment">#         [14, 25, 36]])</span>

<span class="comment"># Row-wise normalization (sum to 1 per row)</span>
P = a.<span class="func">float</span>()
P = P <span class="op">/</span> P.<span class="func">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># keepdim=True is crucial!</span>
<span class="comment"># P.sum(1, keepdim=True) shape: [2, 1] &mdash; broadcasts against [2, 3]</span>
  </div>

  <div class="box danger fi">
    <h4>The keepdim Trap</h4>
    <p>If you write <code>P.sum(dim=1)</code> without <code>keepdim=True</code>, the result has shape <code>[2]</code> instead of <code>[2, 1]</code>. Division then broadcasts incorrectly &mdash; silently producing wrong results. This is the #1 tensor bug. Always use <code>keepdim=True</code> when you need the result to broadcast back against the original tensor.</p>
  </div>

  <h2 class="sh fi"><span class="n">III.</span> One-Hot Encoding &mdash; Integers to Tensors</h2>
  <p class="prose fi">
    Neural networks operate on continuous numbers, not discrete integers. <strong>One-hot encoding</strong> converts
    a character index (e.g., 5) into a vector of 27 zeros with a single 1 at position 5. This is the bridge
    between discrete tokens and the continuous world of matrix multiplication.
  </p>

  <div class="code-block fi">
<span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F

<span class="comment"># One-hot encode character indices</span>
xenc = F.<span class="func">one_hot</span>(torch.<span class="func">tensor</span>([<span class="number">5</span>, <span class="number">13</span>, <span class="number">1</span>]), num_classes=<span class="number">27</span>).<span class="func">float</span>()
<span class="comment"># shape: [3, 27] &mdash; three chars, each a 27-dim vector</span>

<span class="comment"># Multiply by weight matrix: this IS the neural net&rsquo;s first layer</span>
W = torch.<span class="func">randn</span>((<span class="number">27</span>, <span class="number">27</span>), requires_grad=<span class="keyword">True</span>)
logits = xenc <span class="op">@</span> W  <span class="comment"># [3, 27] @ [27, 27] = [3, 27]</span>
  </div>

  <div class="box insight fi">
    <h4>One-Hot @ W = Table Lookup</h4>
    <p>Multiplying a one-hot vector by a weight matrix is equivalent to selecting a row from the matrix. So <code>one_hot(5) @ W</code> just returns <code>W[5]</code>. This is why embedding layers exist &mdash; they skip the one-hot multiplication and directly index into the weight matrix. Same result, zero wasted computation.</p>
  </div>

  <h2 class="sh fi"><span class="n">IV.</span> The Matrix &mdash; What Matters Today</h2>
  <div class="mx fi">
    <div class="mx-corner"></div>
    <div class="mx-ch">Builds Deep Intuition</div>
    <div class="mx-ch">Surface-Level Only</div>
    <div class="mx-rh">Quick to Do</div>
    <div class="mx-cell best">
      <span class="e">&#127919;</span>
      <h4>DO FIRST</h4>
      <p>Practice broadcasting: create tensors of shapes [3,4], [4], [3,1] and verify all arithmetic combinations. Break one on purpose.</p>
    </div>
    <div class="mx-cell">
      <span class="e">&#9197;&#65039;</span>
      <h4>DO IF TIME</h4>
      <p>Explore <code>.view()</code>, <code>.reshape()</code>, <code>.unsqueeze()</code>, <code>.squeeze()</code>. Understand which share memory (views) vs. copy.</p>
    </div>
    <div class="mx-rh">Slow but Worth It</div>
    <div class="mx-cell">
      <span class="e">&#128400;</span>
      <h4>DO CAREFULLY</h4>
      <p>Re-implement the Day 6 bigram model using one-hot encoding + matrix multiply. Verify you get the same NLL as the counting approach.</p>
    </div>
    <div class="mx-cell worst">
      <span class="e">&#128683;</span>
      <h4>AVOID TODAY</h4>
      <p>GPU tensors, CUDA operations, or distributed tensor ops. CPU tensors are sufficient for all Phase 1 work.</p>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">V.</span> Today&rsquo;s Deliverables</h2>
  <ul class="cl fi">
    <li><strong>Shape manipulation:</strong> Create, reshape, view, squeeze, unsqueeze tensors fluently</li>
    <li><strong>Broadcasting:</strong> Verify 4+ broadcasting examples by hand, then in code</li>
    <li><strong>One-hot encoding:</strong> Convert character indices to one-hot vectors with <code>F.one_hot()</code></li>
    <li><strong>Matrix multiply:</strong> <code>xenc @ W</code> to produce logits from one-hot inputs</li>
    <li><strong>keepdim:</strong> Demonstrate the bug without <code>keepdim=True</code> and the fix with it</li>
    <li><strong>Equivalence:</strong> Show that <code>one_hot(i) @ W == W[i]</code> (embedding = table lookup)</li>
  </ul>

  <div class="memo-quote fi" style="margin-top:3rem;">
    Tensors are the language of neural networks, and broadcasting is its grammar. Every forward pass, every loss
    computation, every gradient update is a tensor operation. Fluency here is not optional &mdash; it is the difference
    between writing code that works and code that silently corrupts. Tomorrow, you put it all together in a training loop.
    <span class="attr">&mdash; Day 7 Closing Principle</span>
  </div>

  <div class="notebook-card fade-in">
    <div class="notebook-card-hdr">
      <span class="nb-title">Day 7 Notebook â€” Tensors, Broadcasting & torch.Tensor</span>
      <span class="nb-badge">Runnable Python</span>
    </div>
    <div class="notebook-card-body">
      <p>PyTorch tensor deep dive: shapes, broadcasting rules, one-hot encoding, matrix multiply for neural nets, and softmax.</p>
      <div class="notebook-links">
        <a href="https://colab.research.google.com/github/OmkarRayAI/omkarray/blob/main/notebooks/llm_day07_tensors.ipynb" target="_blank" class="nb-colab">&#9654; Open in Colab</a>
        <a href="https://github.com/OmkarRayAI/omkarray/blob/main/notebooks/llm_day07_tensors.ipynb" target="_blank" class="nb-github">View on GitHub</a>
        <a href="https://nbviewer.org/github/OmkarRayAI/omkarray/blob/main/notebooks/llm_day07_tensors.ipynb" target="_blank" class="nb-nbviewer">nbviewer</a>
      </div>
    </div>
  </div>
</main>

<footer class="footer">
  <span>RAG &amp; LLM Engineer &middot; 80-Day Plan</span>
  <span>Day 7 of 80 &middot; Phase 1: Foundations</span>
  <span>&larr; <a href="llm-day6.html" style="color:inherit;">Day 6</a> &nbsp;|&nbsp; &rarr; Day 8: Training Loops &amp; Splits</span>
</footer>
</body>
</html>