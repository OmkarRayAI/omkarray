{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 16: Self-Attention — The Core of Transformers\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Every architecture so far (MLP, WaveNet) processes the context window in a **fixed, position-blind** way. A token at position 5 has no idea which other tokens are most relevant to it.\n",
    "\n",
    "**Self-attention** solves this: each token **queries** all other tokens to find relevant ones, then **aggregates** their information weighted by relevance.\n",
    "\n",
    "The mechanism:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "- $Q$ (queries): what this token is looking for\n",
    "- $K$ (keys): what each token has to offer  \n",
    "- $V$ (values): the actual content to aggregate\n",
    "- $\\sqrt{d_k}$: scaling to prevent softmax saturation\n",
    "\n",
    "For language modeling, we use **causal (masked) self-attention**: token $i$ can only attend to tokens $\\leq i$ (no peeking at the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Intuition: Weighted Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start simple: uniform averaging (\"bag of words\" baseline)\n",
    "B, T, C = 2, 6, 8  # batch=2, seq_len=6, channels=8\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Version 1: simple loop — each position averages all PAST tokens\n",
    "xbow = torch.zeros_like(x)  # \"bag of words\"\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]        # (t+1, C)\n",
    "        xbow[b, t] = xprev.mean(0) # average over past\n",
    "\n",
    "print(\"xbow[0, 3] (avg of tokens 0-3):\", xbow[0, 3])\n",
    "print(\"Manual check:\", x[0, :4].mean(0))\n",
    "print(\"Match:\", torch.allclose(xbow[0, 3], x[0, :4].mean(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efficient Averaging via Lower-Triangular Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: matrix multiply trick — no loop needed\n",
    "wei = torch.tril(torch.ones(T, T))  # lower triangular\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)  # normalize rows\n",
    "\n",
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) broadcasts to (B, T, C)\n",
    "\n",
    "print(\"Weight matrix (T=6 x T=6):\")\n",
    "print(wei.round(decimals=2))\n",
    "print(f\"\\nxbow2 matches xbow: {torch.allclose(xbow, xbow2)}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.imshow(wei.numpy(), cmap='Blues')\n",
    "ax.set_title('Causal Attention Mask (uniform weights)')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "plt.colorbar(ax.images[0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Softmax Masking — Data-Dependent Weights\n",
    "\n",
    "Uniform averaging treats all past tokens equally. Self-attention makes the weights **data-dependent** via Q·K dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: softmax masking (step toward real attention)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # future tokens -> -inf\n",
    "wei = F.softmax(wei, dim=-1)                     # softmax -> uniform over past\n",
    "\n",
    "print(\"Masked softmax weights (uniform, since all zeros before masking):\")\n",
    "print(wei.round(decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single-Head Self-Attention from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"One head of causal self-attention.\"\"\"\n",
    "    def __init__(self, embed_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        # Register causal mask as buffer (not a parameter)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(256, 256)))\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "\n",
    "        # Attention scores\n",
    "        scale = self.head_size ** -0.5\n",
    "        scores = q @ k.transpose(-2, -1) * scale  # (B, T, T)\n",
    "\n",
    "        # Causal mask: zero out future\n",
    "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(scores, dim=-1)        # (B, T, T)\n",
    "\n",
    "        # Weighted aggregation\n",
    "        out = weights @ v  # (B, T, head_size)\n",
    "        return out, weights\n",
    "\n",
    "\n",
    "# Test with a small example\n",
    "embed_dim, head_size = 16, 8\n",
    "attn = SingleHeadAttention(embed_dim, head_size)\n",
    "x = torch.randn(2, 6, embed_dim)\n",
    "out, weights = attn(x)\n",
    "\n",
    "print(f\"Input:   {x.shape}\")\n",
    "print(f\"Output:  {out.shape}\")\n",
    "print(f\"Weights: {weights.shape}\")\n",
    "print(f\"\\nAttention weights for batch=0 (causal = lower triangular):\")\n",
    "print(weights[0].detach().round(decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualising Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention weight matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Single example attention weights\n",
    "w = weights[0].detach().numpy()\n",
    "im = axes[0].imshow(w, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title('Self-Attention Weights\\n(causal: upper triangle = 0)')\n",
    "axes[0].set_xlabel('Key (which token to attend to)')\n",
    "axes[0].set_ylabel('Query (which token is asking)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Show how the weights change with temperature (scaling factor)\n",
    "x_single = torch.randn(1, 6, embed_dim)\n",
    "k = attn.key(x_single)\n",
    "q = attn.query(x_single)\n",
    "raw_scores = (q @ k.transpose(-2, -1))[0].detach()\n",
    "\n",
    "tril = torch.tril(torch.ones(6, 6))\n",
    "for scale_factor, label in [(0.1, 'No scaling (peaked)'), (head_size**-0.5, f'Scaled (1/√{head_size}), default)')]:\n",
    "    pass  # just using the default weights plot\n",
    "\n",
    "axes[1].bar(range(6), weights[0, 4].detach().numpy(), color='steelblue')\n",
    "axes[1].set_title('Token 4 attends to tokens 0-4\\n(what it finds most relevant)')\n",
    "axes[1].set_xlabel('Token position')\n",
    "axes[1].set_ylabel('Attention weight')\n",
    "axes[1].set_xticks(range(6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why Scale by √d_k?\n",
    "\n",
    "Without scaling, dot products grow with `head_size`. Large values push softmax into saturation — one token gets weight ≈ 1, all others ≈ 0. Gradients vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate saturation effect\n",
    "q = torch.randn(1, 6, 64)\n",
    "k = torch.randn(1, 6, 64)\n",
    "\n",
    "scores_unscaled = (q @ k.transpose(-2, -1))[0, 0].detach()\n",
    "scores_scaled   = scores_unscaled / (64 ** 0.5)\n",
    "\n",
    "w_unscaled = F.softmax(scores_unscaled, dim=-1)\n",
    "w_scaled   = F.softmax(scores_scaled, dim=-1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].bar(range(6), w_unscaled.numpy(), color='tomato')\n",
    "axes[0].set_title(f'Unscaled: max = {w_unscaled.max().item():.3f}\\n(nearly one-hot, vanishing gradients)')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].bar(range(6), w_scaled.numpy(), color='steelblue')\n",
    "axes[1].set_title(f'Scaled by 1/√64: max = {w_scaled.max().item():.3f}\\n(spread out, healthy gradients)')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Effect of √d_k Scaling on Attention Weights')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unscaled entropy: {-(w_unscaled * w_unscaled.log()).sum():.4f}\")\n",
    "print(f\"Scaled entropy:   {-(w_scaled * w_scaled.log()).sum():.4f}\")\n",
    "print(\"(Higher entropy = more spread out = better gradients)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building a Simple Attention-Based LM\n",
    "\n",
    "One attention head + a feedforward layer = a baby transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLM(nn.Module):\n",
    "    \"\"\"Minimal language model with a single self-attention head.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=32, head_size=32, block_size=8):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, embed_dim)   # learned positions\n",
    "        self.attn    = SingleHeadAttention(embed_dim, head_size)\n",
    "        self.proj    = nn.Linear(head_size, embed_dim)\n",
    "        self.ff      = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.ln1  = nn.LayerNorm(embed_dim)\n",
    "        self.ln2  = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok = self.tok_emb(idx)                          # (B, T, C)\n",
    "        pos = self.pos_emb(torch.arange(T))              # (T, C)\n",
    "        x = tok + pos                                    # broadcast\n",
    "        # Attention with residual\n",
    "        attn_out, _ = self.attn(self.ln1(x))\n",
    "        x = x + self.proj(attn_out)\n",
    "        # FFN with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return self.head(x)                              # (B, T, vocab_size)\n",
    "\n",
    "\n",
    "# Quick test\n",
    "vocab_size = 27  # letters + special\n",
    "lm = AttentionLM(vocab_size=vocab_size)\n",
    "idx = torch.randint(0, vocab_size, (2, 8))\n",
    "logits = lm(idx)\n",
    "print(f\"Input shape:  {idx.shape}\")\n",
    "print(f\"Output shape: {logits.shape}  (B, T, vocab_size)\")\n",
    "print(f\"Parameters:   {sum(p.numel() for p in lm.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 16: Self-Attention](https://omkarray.com/llm-day16.html) | [← Prev](llm_day15_wavenet.ipynb) | [Next →](llm_day17_multihead_attention.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
