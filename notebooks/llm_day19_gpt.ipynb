{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 19: GPT — Putting It All Together\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) is the architecture behind ChatGPT, GPT-4, and most modern LLMs. Today we build the complete GPT from scratch using everything we've learned:\n",
    "\n",
    "- Token + positional embeddings (Day 17)\n",
    "- Multi-head causal self-attention (Day 16-17)\n",
    "- Transformer blocks with residuals + LayerNorm (Day 18)\n",
    "- Language modeling head\n",
    "\n",
    "**GPT architecture:**\n",
    "```\n",
    "Input tokens\n",
    "  → Token Embedding + Positional Embedding\n",
    "  → Block 1 (MHA + FFN)\n",
    "  → Block 2 (MHA + FFN)\n",
    "  → ...\n",
    "  → Block N (MHA + FFN)\n",
    "  → LayerNorm\n",
    "  → Linear head → logits over vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete GPT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 65          # character-level: 65 chars\n",
    "    block_size: int = 64          # context window\n",
    "    n_layer:    int = 4           # number of transformer blocks\n",
    "    n_head:     int = 4           # attention heads\n",
    "    n_embd:     int = 128         # embedding dimension\n",
    "    dropout:    float = 0.1\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        # Q, K, V all at once\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        H, hs = self.n_head, self.head_size\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, H, hs).transpose(1, 2)\n",
    "        k = k.view(B, T, H, hs).transpose(1, 2)\n",
    "        v = v.view(B, T, H, hs).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2,-1)) * (hs ** -0.5)\n",
    "        att = att.masked_fill(self.bias[:T,:T] == 0, float('-inf'))\n",
    "        att = self.attn_dropout(F.softmax(att, dim=-1))\n",
    "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.act    = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp  = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte  = nn.Embedding(config.vocab_size, config.n_embd),   # token embeddings\n",
    "            wpe  = nn.Embedding(config.block_size, config.n_embd),   # positional embeddings\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # Weight tying: share token embedding and lm_head weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"GPT-2 style initialization.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        tok_emb = self.transformer.wte(idx)   # (B, T, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)   # (T, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)              # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Autoregressive generation.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.config.block_size:]  # crop to block_size\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature      # last token only\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_tok], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"GPT config: {config}\")\n",
    "print(f\"Parameters: {params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "idx = torch.randint(0, config.vocab_size, (2, 32))\n",
    "targets = torch.randint(0, config.vocab_size, (2, 32))\n",
    "logits, loss = model(idx, targets)\n",
    "print(f\"\\nlogits: {logits.shape}, loss: {loss.item():.4f}\")\n",
    "print(f\"Expected initial loss ≈ {torch.log(torch.tensor(config.vocab_size)):.4f} (random baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Tying — Why Share Token + Output Weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight tying: wte.weight == lm_head.weight\n",
    "# Intuition: the embedding of token i should be similar to the output vector\n",
    "# that causes token i to be predicted — they're dual representations\n",
    "# Also saves ~vocab_size * n_embd parameters\n",
    "\n",
    "print(\"Weight tying verification:\")\n",
    "print(f\"  wte.weight is lm_head.weight: {model.transformer.wte.weight is model.lm_head.weight}\")\n",
    "print(f\"  Shape: {model.transformer.wte.weight.shape}\")\n",
    "print(f\"  Saves: {config.vocab_size * config.n_embd:,} parameters\")\n",
    "\n",
    "# Parameter breakdown\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"  {name:<40} {str(p.shape):<25} {p.numel():>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generation — Before Training (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character vocabulary\n",
    "chars = [chr(i) for i in range(32, 97)]  # space + uppercase + some punctuation = 65 chars\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "# Generate from untrained model\n",
    "model.eval()\n",
    "context = torch.zeros(1, 1, dtype=torch.long)  # start with token 0\n",
    "generated = model.generate(context, max_new_tokens=100, temperature=1.0)[0].tolist()\n",
    "text = ''.join(itos.get(i, '?') for i in generated)\n",
    "print(\"Before training (random noise):\")\n",
    "print(repr(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing GPT to Earlier Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture comparison table\n",
    "architectures = [\n",
    "    (\"Bigram (Day 6)\",        \"None\",         \"None\",    \"1-gram\",   \"<1K\"),\n",
    "    (\"MLP (Day 9)\",           \"None\",         \"Concat\",  \"fixed\",    \"~50K\"),\n",
    "    (\"WaveNet (Day 15)\",      \"None\",         \"Hierarch\",\"log(T)\",   \"~150K\"),\n",
    "    (\"GPT (today)\",           \"Self-Attn\",    \"Residual\",\"full T\",   f\"{params:,}\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<25} {'Context Mixing':<15} {'Aggregation':<12} {'Receptive Field':<18} {'Params':<12}\")\n",
    "print(\"-\" * 85)\n",
    "for row in architectures:\n",
    "    print(f\"{row[0]:<25} {row[1]:<15} {row[2]:<12} {row[3]:<18} {row[4]:<12}\")\n",
    "\n",
    "print(\"\\nKey insight: GPT's self-attention lets EVERY token attend to EVERY other token\")\n",
    "print(\"with learned, data-dependent weights — the most expressive mixing possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top-K and Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Demonstrate temperature and top-k effects\n",
    "logits = torch.tensor([3.0, 2.0, 1.0, 0.5, 0.1, -0.5, -1.0, -2.0])\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Temperature effects\n",
    "temps = [0.5, 1.0, 2.0]\n",
    "for i, T in enumerate(temps):\n",
    "    probs = F.softmax(logits / T, dim=0)\n",
    "    axes[0, i].bar(range(len(probs)), probs.numpy(), color='steelblue')\n",
    "    axes[0, i].set_title(f'Temperature = {T}\\n{\"peaked\" if T < 1 else (\"balanced\" if T == 1 else \"flat\")}')\n",
    "    axes[0, i].set_xlabel('Token')\n",
    "    axes[0, i].set_ylabel('Probability')\n",
    "    axes[0, i].set_ylim(0, 1)\n",
    "\n",
    "# Top-K effects\n",
    "ks = [2, 4, 8]\n",
    "for i, k in enumerate(ks):\n",
    "    l = logits.clone()\n",
    "    v, _ = torch.topk(l, k)\n",
    "    l[l < v[-1]] = float('-inf')\n",
    "    probs = F.softmax(l, dim=0)\n",
    "    axes[1, i].bar(range(len(probs)), probs.numpy(), color='tomato')\n",
    "    axes[1, i].set_title(f'Top-K = {k}\\n(zeros out {len(logits)-k} lowest tokens)')\n",
    "    axes[1, i].set_xlabel('Token')\n",
    "    axes[1, i].set_ylabel('Probability')\n",
    "    axes[1, i].set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Sampling Strategies: Temperature (top) vs Top-K (bottom)', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Temperature < 1: more deterministic (good for code/facts)\")\n",
    "print(\"Temperature > 1: more random (good for creative text)\")\n",
    "print(\"Top-K: only sample from K most likely tokens (prevents rare token sampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 19: GPT](https://omkarray.com/llm-day19.html) | [← Prev](llm_day18_transformer_block.ipynb) | [Next →](llm_day20_training_gpt.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
