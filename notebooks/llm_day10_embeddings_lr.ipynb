{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10: Embeddings, Learning Rate Schedules & Hyperparameters\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Finding the right learning rate is one of the most impactful hyperparameter choices. Too low and training crawls; too high and the optimizer diverges. Today we use a **learning rate finder** — sweeping LR in log-space and plotting loss — to identify the sweet spot before committing to full training.\n",
    "\n",
    "We also explore **embedding spaces**: the learned character vectors in `C` organize themselves during training. Similar characters (e.g., vowels) cluster together. Visualizing these embeddings reveals what the network has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup\n",
    "\n",
    "Same names list, stoi/itos, and (X, Y) pairs with block_size=3 — reused from Day 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'mia', 'charlotte', 'amelia', 'harper', 'evelyn',\n",
    "         'abigail', 'emily', 'ella', 'elizabeth', 'camila', 'luna', 'sofia', 'avery', 'mila', 'aria']\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {'.': 0, **{c: i + 1 for i, c in enumerate(chars)}}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs) - block_size):\n",
    "        ctx = [stoi[c] for c in chs[i:i + block_size]]\n",
    "        tgt = stoi[chs[i + block_size]]\n",
    "        X.append(ctx)\n",
    "        Y.append(tgt)\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(f\"Dataset: {len(words)} names\")\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"Context pairs: {len(X)}\")\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split (80/20)\n",
    "torch.manual_seed(42)\n",
    "perm = torch.randperm(len(X))\n",
    "n_train = int(0.8 * len(X))\n",
    "Xtr, Ytr = X[perm[:n_train]], Y[perm[:n_train]]\n",
    "Xval, Yval = X[perm[n_train:]], Y[perm[n_train:]]\n",
    "print(f\"Train: {len(Xtr)}, Val: {len(Xval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup\n",
    "\n",
    "Same MLP architecture as Day 9: C(27,10), W1(30,200), b1(200), W2(200,27), b2(27)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    C = torch.randn(27, 10) * 0.1\n",
    "    W1 = torch.randn(30, 200) * 0.2\n",
    "    b1 = torch.randn(200) * 0.01\n",
    "    W2 = torch.randn(200, 27) * 0.01\n",
    "    b2 = torch.randn(27) * 0\n",
    "    params = [C, W1, b1, W2, b2]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "def forward(X, params):\n",
    "    C, W1, b1, W2, b2 = params\n",
    "    emb = C[X]  # (N, block_size, emb_dim)\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    return logits\n",
    "\n",
    "params = build_model()\n",
    "print(\"Params: C(27,10), W1(30,200), b1(200), W2(200,27), b2(27)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Finder\n",
    "\n",
    "Sweep learning rate from 1e-4 to 10 in log-space over 1000 steps. Record loss at each LR. Plot loss vs log10(lr) to find the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = build_model()\n",
    "lre = torch.linspace(-4, 1, 1000)  # log10(lr) from -4 to 1\n",
    "lrs = 10 ** lre\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    ix = torch.randint(0, len(Xtr), (32,))\n",
    "    logits = forward(Xtr[ix], params)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = lrs[i].item()\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(lre.tolist(), lossi, 'b-', linewidth=0.8)\n",
    "plt.xlabel('log10(learning rate)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate Finder: Loss vs LR (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sweet spot: where loss drops fastest before rising\n",
    "min_idx = min(range(len(lossi)), key=lambda i: lossi[i])\n",
    "print(f\"Lowest loss at step {min_idx}: lr={lrs[min_idx].item():.4f}, loss={lossi[min_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step Decay Schedule\n",
    "\n",
    "Train with a schedule: start at lr=0.1, decay to 0.01 after 5000 steps. Compare to fixed LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_schedule(params, steps, decay_step=5000, lr_high=0.1, lr_low=0.01, use_decay=True):\n",
    "    train_losses, val_losses = [], []\n",
    "    for step in range(steps):\n",
    "        ix = torch.randint(0, len(Xtr), (32,))\n",
    "        logits = forward(Xtr[ix], params)\n",
    "        loss = F.cross_entropy(logits, Ytr[ix])\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        if use_decay:\n",
    "            lr = lr_high if step < decay_step else lr_low\n",
    "        else:\n",
    "            lr = lr_low  # fixed low LR\n",
    "        for p in params:\n",
    "            p.data -= lr * p.grad\n",
    "        if step % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                tloss = F.cross_entropy(forward(Xtr, params), Ytr).item()\n",
    "                vloss = F.cross_entropy(forward(Xval, params), Yval).item()\n",
    "            train_losses.append(tloss)\n",
    "            val_losses.append(vloss)\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "decay_step = 5000\n",
    "\n",
    "# With step decay (0.1 -> 0.01 at 5000)\n",
    "params_decay = build_model()\n",
    "tr_decay, val_decay = train_with_schedule(params_decay, steps, decay_step, use_decay=True)\n",
    "\n",
    "# Fixed LR = 0.01\n",
    "params_fixed = build_model()\n",
    "tr_fixed, val_fixed = train_with_schedule(params_fixed, steps, decay_step, use_decay=False)\n",
    "\n",
    "steps_plot = list(range(0, steps + 1, 500))\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(steps_plot, tr_decay, 'b-', label='Train (decay)')\n",
    "plt.plot(steps_plot, val_decay, 'b--', label='Val (decay)')\n",
    "plt.plot(steps_plot, tr_fixed, 'g-', alpha=0.7, label='Train (fixed 0.01)')\n",
    "plt.plot(steps_plot, val_fixed, 'g--', alpha=0.7, label='Val (fixed 0.01)')\n",
    "plt.axvline(decay_step, color='gray', linestyle=':', alpha=0.7, label='Decay step')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Step Decay (0.1→0.01) vs Fixed LR')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"With decay  - Final train: {tr_decay[-1]:.4f}, val: {val_decay[-1]:.4f}\")\n",
    "print(f\"Fixed 0.01 - Final train: {tr_fixed[-1]:.4f}, val: {val_fixed[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Embeddings\n",
    "\n",
    "After training, extract the 2D PCA (or first 2 dims) of C. Plot characters in embedding space, label each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = params_decay[0].detach()\n",
    "\n",
    "# Use first 2 dims (C is 27x10, so we have 10 dims)\n",
    "# For 2D viz: either first 2 dims or PCA\n",
    "if C.shape[1] >= 2:\n",
    "    # Option: first 2 dims\n",
    "    emb_2d = C[:, :2].numpy()\n",
    "else:\n",
    "    emb_2d = C.numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(27):\n",
    "    plt.scatter(emb_2d[i, 0], emb_2d[i, 1], s=80, alpha=0.8)\n",
    "    plt.annotate(itos[i], (emb_2d[i, 0], emb_2d[i, 1]), fontsize=10, ha='center', va='bottom')\n",
    "plt.xlabel('Embedding dim 0')\n",
    "plt.ylabel('Embedding dim 1')\n",
    "plt.title('Character Embeddings (first 2 dims of C)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: PCA for better 2D projection when emb_dim > 2\n",
    "def pca_2d(X, n_components=2):\n",
    "    X_centered = X - X.mean(0)\n",
    "    cov = X_centered.T @ X_centered / (X.shape[0] - 1)\n",
    "    eigvals, eigvecs = torch.linalg.eigh(cov)\n",
    "    idx = eigvals.argsort(descending=True)\n",
    "    V = eigvecs[:, idx[:n_components]]\n",
    "    return (X_centered @ V).numpy()\n",
    "\n",
    "emb_pca = pca_2d(C)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(27):\n",
    "    plt.scatter(emb_pca[i, 0], emb_pca[i, 1], s=80, alpha=0.8)\n",
    "    plt.annotate(itos[i], (emb_pca[i, 0], emb_pca[i, 1]), fontsize=10, ha='center', va='bottom')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Character Embeddings (PCA projection)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Table\n",
    "\n",
    "Print a summary: embedding_dim, hidden_size, block_size, total parameters, final train loss, final val loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(params):\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_train = F.cross_entropy(forward(Xtr, params_decay), Ytr).item()\n",
    "    final_val = F.cross_entropy(forward(Xval, params_decay), Yval).item()\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_size = 200\n",
    "total_params = count_params(params_decay)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HYPERPARAMETER SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"embedding_dim:  {embedding_dim}\")\n",
    "print(f\"hidden_size:    {hidden_size}\")\n",
    "print(f\"block_size:    {block_size}\")\n",
    "print(f\"total_params:  {total_params:,}\")\n",
    "print(f\"final_train_loss: {final_train:.4f}\")\n",
    "print(f\"final_val_loss:   {final_val:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Blog:** [Day 10 — Embeddings, LR Schedules & Hyperparameters](https://omkarray.com/llm-day10.html)\n",
    "\n",
    "**Prev:** [Day 9 — MLP Language Model](llm_day09_mlp.ipynb) · **Next:** [Day 11 — Activations & Gradients](llm_day11_activations.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
