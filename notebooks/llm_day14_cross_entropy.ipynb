{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 14: Cross-Entropy, Softmax & Classification Gradients\n",
        "\n",
        "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "**Softmax** converts logits (unnormalized scores) into a probability distribution over classes. **Cross-entropy** measures how well predicted probabilities match the true target. Together they form the standard loss for classification.\n",
        "\n",
        "**The math:**\n",
        "- Softmax: $p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
        "- Cross-entropy (for target class $y$): $\\mathcal{L} = -\\log(p_y)$\n",
        "\n",
        "**Numerical stability:** Raw $e^{z_i}$ overflows for large $z$. We subtract $\\max(z)$ first: $p_i = \\frac{e^{z_i - \\max(z)}}{\\sum_j e^{z_j - \\max(z)}}$ — mathematically equivalent, numerically stable.\n",
        "\n",
        "**The gradient:** When softmax is followed by cross-entropy, the gradient w.r.t. logits simplifies beautifully to $\\frac{\\partial \\mathcal{L}}{\\partial z_i} = p_i - \\mathbb{1}[i = y]$. That is: **prediction minus truth**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Naive Softmax\n",
        "\n",
        "Implement `softmax_naive(logits)`: `exp(logits) / sum(exp(logits))`. Works for small values but overflows for large logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def softmax_naive(logits):\n",
        "    \"\"\"Naive softmax: exp(logits) / sum(exp(logits)). Overflows for large logits.\"\"\"\n",
        "    exp_logits = torch.exp(logits)\n",
        "    return exp_logits / exp_logits.sum()\n",
        "\n",
        "\n",
        "# Works for small values\n",
        "logits_small = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
        "probs_small = softmax_naive(logits_small)\n",
        "print(\"Small logits [1, 2, 3]:\")\n",
        "print(f\"  softmax_naive = {probs_small.tolist()}\")\n",
        "print(f\"  sum = {probs_small.sum().item():.6f}\")\n",
        "\n",
        "# Overflows for large values\n",
        "logits_large = torch.tensor([1000.0, 1001.0, 1002.0], dtype=torch.float32)\n",
        "print(\"\\nLarge logits [1000, 1001, 1002]:\")\n",
        "try:\n",
        "    probs_large = softmax_naive(logits_large)\n",
        "    print(f\"  softmax_naive = {probs_large.tolist()}\")\n",
        "except Exception as e:\n",
        "    print(f\"  ERROR: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stable Softmax\n",
        "\n",
        "Implement `softmax_stable(logits)`: subtract max first. `logits -= logits.max()` then exponentiate. Handles large values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def softmax_stable(logits):\n",
        "    \"\"\"Numerically stable softmax: subtract max before exp.\"\"\"\n",
        "    logits = logits - logits.max()\n",
        "    exp_logits = torch.exp(logits)\n",
        "    return exp_logits / exp_logits.sum()\n",
        "\n",
        "\n",
        "# Same result for small values\n",
        "probs_stable_small = softmax_stable(logits_small)\n",
        "print(\"Small logits [1, 2, 3]:\")\n",
        "print(f\"  softmax_stable = {probs_stable_small.tolist()}\")\n",
        "print(f\"  Match naive? {torch.allclose(probs_small, probs_stable_small)}\")\n",
        "\n",
        "# Now handles large values!\n",
        "probs_stable_large = softmax_stable(logits_large)\n",
        "print(\"\\nLarge logits [1000, 1001, 1002]:\")\n",
        "print(f\"  softmax_stable = {probs_stable_large.tolist()}\")\n",
        "print(f\"  sum = {probs_stable_large.sum().item():.6f}\")\n",
        "print(\"  (After subtracting max, we get [-2, -1, 0] → same relative probs as [1, 2, 3])\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Cross-Entropy Loss\n",
        "\n",
        "Implement manually: `loss = -log(softmax[target_class])`. Show equivalence with `F.cross_entropy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cross_entropy_manual(logits, target):\n",
        "    \"\"\"Cross-entropy: -log(softmax[target]).\"\"\"\n",
        "    probs = softmax_stable(logits)\n",
        "    return -torch.log(probs[target])\n",
        "\n",
        "\n",
        "# Single example\n",
        "logits = torch.tensor([2.0, 1.0, 0.1], dtype=torch.float32)\n",
        "target = 0  # true class is index 0\n",
        "\n",
        "loss_manual = cross_entropy_manual(logits, target)\n",
        "loss_torch = F.cross_entropy(logits.unsqueeze(0), torch.tensor([target]))\n",
        "\n",
        "print(f\"Logits: {logits.tolist()}\")\n",
        "print(f\"Target class: {target}\")\n",
        "print(f\"Manual CE loss: {loss_manual.item():.6f}\")\n",
        "print(f\"F.cross_entropy: {loss_torch.item():.6f}\")\n",
        "print(f\"Match? {torch.allclose(loss_manual, loss_torch)}\")\n",
        "\n",
        "# Batch version\n",
        "logits_batch = torch.tensor([[2.0, 1.0, 0.1], [0.5, 2.5, 0.3]], dtype=torch.float32)\n",
        "targets_batch = torch.tensor([0, 1])\n",
        "\n",
        "losses_manual = torch.stack([cross_entropy_manual(logits_batch[i], targets_batch[i]) for i in range(2)])\n",
        "loss_batch_torch = F.cross_entropy(logits_batch, targets_batch)\n",
        "\n",
        "print(f\"\\nBatch: manual mean = {losses_manual.mean().item():.6f}, F.cross_entropy = {loss_batch_torch.item():.6f}\")\n",
        "print(f\"Match? {torch.allclose(losses_manual.mean(), loss_batch_torch)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Temperature Scaling\n",
        "\n",
        "Implement `softmax(logits / T)` for different T. Low T = peaked/confident, high T = uniform/uncertain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def softmax_temperature(logits, T=1.0):\n",
        "    \"\"\"Softmax with temperature: softmax(logits / T).\"\"\"\n",
        "    return softmax_stable(logits / T)\n",
        "\n",
        "\n",
        "logits = torch.tensor([2.0, 1.0, 0.5, 0.1], dtype=torch.float32)\n",
        "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(14, 4), sharey=True)\n",
        "\n",
        "for ax, T in zip(axes, temperatures):\n",
        "    probs = softmax_temperature(logits, T)\n",
        "    bars = ax.bar(range(len(probs)), probs.tolist(), color='steelblue', edgecolor='black')\n",
        "    ax.set_xlabel('Class')\n",
        "    ax.set_title(f'T = {T}')\n",
        "    ax.set_xticks(range(len(probs)))\n",
        "    if T == 0.1:\n",
        "        ax.set_ylabel('Probability')\n",
        "\n",
        "plt.suptitle('Temperature Scaling: Low T = peaked, High T = uniform')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Low T (0.1): nearly one-hot, very confident\")\n",
        "print(\"High T (5.0): nearly uniform, uncertain\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. The Gradient of Cross-Entropy + Softmax\n",
        "\n",
        "Derive and implement: `dlogits = softmax(logits) - one_hot(target)`. Verify against PyTorch autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cross_entropy_softmax_gradient(logits, target):\n",
        "    \"\"\"\n",
        "    Gradient of cross-entropy + softmax w.r.t. logits:\n",
        "    dlogits = softmax(logits) - one_hot(target)\n",
        "    \"\"\"\n",
        "    probs = softmax_stable(logits)\n",
        "    one_hot = torch.zeros_like(logits)\n",
        "    one_hot[target] = 1.0\n",
        "    return probs - one_hot\n",
        "\n",
        "\n",
        "# Manual gradient vs autograd\n",
        "logits = torch.tensor([2.0, 1.0, 0.1], dtype=torch.float32, requires_grad=True)\n",
        "target = 0\n",
        "\n",
        "loss = F.cross_entropy(logits.unsqueeze(0), torch.tensor([target]))\n",
        "loss.backward()\n",
        "\n",
        "grad_autograd = logits.grad\n",
        "grad_manual = cross_entropy_softmax_gradient(logits.detach(), target)\n",
        "\n",
        "print(\"Logits:\", logits.detach().tolist())\n",
        "print(\"Target:\", target)\n",
        "print(\"Gradient (manual):\", grad_manual.tolist())\n",
        "print(\"Gradient (autograd):\", grad_autograd.tolist())\n",
        "print(\"Match?\", torch.allclose(grad_manual, grad_autograd))\n",
        "\n",
        "# Batch version\n",
        "logits_batch = torch.tensor([[2.0, 1.0, 0.1], [0.5, 2.5, 0.3]], dtype=torch.float32, requires_grad=True)\n",
        "targets_batch = torch.tensor([0, 1])\n",
        "\n",
        "loss_batch = F.cross_entropy(logits_batch, targets_batch)\n",
        "loss_batch.backward()\n",
        "\n",
        "grad_autograd_batch = logits_batch.grad\n",
        "grad_manual_batch = torch.stack([\n",
        "    cross_entropy_softmax_gradient(logits_batch[i].detach(), targets_batch[i].item())\n",
        "    for i in range(2)\n",
        "])\n",
        "\n",
        "print(\"\\nBatch gradients match?\", torch.allclose(grad_manual_batch, grad_autograd_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Why This Gradient is Beautiful\n",
        "\n",
        "The gradient of cross-entropy + softmax w.r.t. logits is simply:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial z_i} = p_i - \\mathbb{1}[i = y]$$\n",
        "\n",
        "That is: **prediction minus truth**.\n",
        "\n",
        "- For the **target class** $y$: gradient is $p_y - 1$ (negative). Gradient descent *increases* logit $z_y$ → higher $p_y$ → correct.\n",
        "- For **non-target classes** $i \\neq y$: gradient is $p_i - 0 = p_i$ (positive). Gradient descent *decreases* logits $z_i$ → lower $p_i$ → correct.\n",
        "\n",
        "The magnitude of the error is proportional to the prediction: if we're very wrong ($p_y$ small), we get a large gradient; if we're correct ($p_y \\approx 1$), we get a tiny gradient. This is why neural nets learn — they push predictions toward targets proportionally to the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Building LLMs from Scratch** — [Day 14: Cross-Entropy, Softmax & Classification Gradients](https://omkarray.com/llm-day14.html) | [← Prev](llm_day13.ipynb) | [Next →](llm_day15.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}