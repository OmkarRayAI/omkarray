{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Intro to Language Modeling — The Bigram Model\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "From autograd to language: we shift gears to **language modeling** — predicting the next token (character or word) given previous context. The simplest model is the **bigram model**: we predict the next character given only the previous one.\n",
    "\n",
    "This notebook builds a character-level bigram model from scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "\n",
    "We use a small hardcoded list of common names. Each name is a sequence of characters we'll model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'mia', 'charlotte', 'amelia', 'harper', 'evelyn',\n",
    "         'abigail', 'emily', 'ella', 'elizabeth', 'camila', 'luna', 'sofia', 'avery', 'mila', 'aria']\n",
    "\n",
    "print(f\"Dataset: {len(words)} names\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character Mappings\n",
    "\n",
    "Build `stoi` (string → int) and `itos` (int → string) mappings. We use `.` as the special start/end token so every name is wrapped as `.name.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['.'] + [chr(i) for i in range(ord('a'), ord('z') + 1)]  # 27 chars: . + a-z\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)} (including '.' as start/end token)\")\n",
    "print(f\"stoi: {stoi}\")\n",
    "print(f\"itos: {itos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Bigram Count Matrix\n",
    "\n",
    "Count all bigrams (pairs of consecutive characters). We use a 27×27 matrix: 26 letters + `.` (index 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for c1, c2 in zip(chs[:-1], chs[1:]):\n",
    "        ix1, ix2 = stoi[c1], stoi[c2]\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "print(\"Bigram count matrix (rows=first char, cols=second char):\")\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Matrix\n",
    "\n",
    "Plot the count matrix with matplotlib. Rows and columns are labeled by character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [itos[i] for i in range(27)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(N, cmap='Blues')\n",
    "\n",
    "ax.set_xticks(range(27))\n",
    "ax.set_yticks(range(27))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Next character')\n",
    "ax.set_ylabel('Current character')\n",
    "ax.set_title('Bigram Count Matrix')\n",
    "\n",
    "# Annotate top values (counts > 0)\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        if N[i, j].item() > 0:\n",
    "            ax.text(j, i, int(N[i, j].item()), ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Converting to Probabilities\n",
    "\n",
    "Normalize each row to get P(next | current). We use **add-1 smoothing** to avoid zeros: `P = (N + 1).float()` then divide by row sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()  # add-1 smoothing\n",
    "P = P / P.sum(1, keepdim=True)\n",
    "\n",
    "print(\"Probability matrix P[next | current] (sample rows):\")\n",
    "print(f\"P[0,:] (after '.'): {P[0].tolist()}\")\n",
    "print(f\"Row sums: {P.sum(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sampling from the Model\n",
    "\n",
    "Generate new names by sampling from the bigram distribution. Start with `.`, sample next char, repeat until we hit `.` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def sample_name():\n",
    "    out = []\n",
    "    ix = 0  # start with '.'\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    return ''.join(out)\n",
    "\n",
    "print(\"Generated names:\")\n",
    "for _ in range(10):\n",
    "    print(sample_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating with NLL\n",
    "\n",
    "Compute **negative log-likelihood** on the training data. Lower NLL = better model. For each bigram (ix1, ix2), we use -log P[ix1, ix2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for c1, c2 in zip(chs[:-1], chs[1:]):\n",
    "        ix1, ix2 = stoi[c1], stoi[c2]\n",
    "        log_likelihood += torch.log(P[ix1, ix2]).item()\n",
    "        n += 1\n",
    "\n",
    "nll = -log_likelihood / n\n",
    "print(f\"Negative Log-Likelihood (mean): {nll:.4f}\")\n",
    "print(f\"(Lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: vectorized NLL using tensors\n",
    "ix1_list, ix2_list = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for c1, c2 in zip(chs[:-1], chs[1:]):\n",
    "        ix1_list.append(stoi[c1])\n",
    "        ix2_list.append(stoi[c2])\n",
    "\n",
    "ix1 = torch.tensor(ix1_list)\n",
    "ix2 = torch.tensor(ix2_list)\n",
    "nll_vec = -torch.log(P[ix1, ix2]).mean()\n",
    "print(f\"Vectorized NLL: {nll_vec.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Blog:** [Day 6 — Bigram Model](https://omkarray.com/llm-day6.html)\n",
    "\n",
    "**Prev:** [Day 5 — Training the MLP](llm_day05_training.ipynb) · **Next:** [Day 7 — MLP Language Model](llm_day07_mlp_lm.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
