{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 13: Becoming a Backprop Ninja — Manual Tensor Backprop\n",
        "\n",
        "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Today we **manually compute gradients** for tensor operations: matrix multiply, batch norm, and cross-entropy. No autograd — we derive every gradient by hand and verify against PyTorch. This is the \"backprop ninja\" exercise: understanding exactly how gradients flow through each operation.\n",
        "\n",
        "We'll build a small network with:\n",
        "- Linear layer: \\( h = XW + b \\)\n",
        "- Batch normalization: normalize, then scale \\( \\gamma \\) and shift \\( \\beta \\)\n",
        "- Output layer: \\( \\text{logits} = h_{bn} W_2 + b_2 \\)\n",
        "- Cross-entropy loss\n",
        "\n",
        "For each step, we'll compute the gradients manually and compare with `torch.autograd`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup\n",
        "\n",
        "Import torch, set seed, create a small dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Small dataset: X (4, 10), Y (4,) with targets 0-4\n",
        "X = torch.randn(4, 10)\n",
        "Y = torch.randint(0, 5, (4,))  # 5 classes: 0, 1, 2, 3, 4\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"Y shape: {Y.shape}\")\n",
        "print(f\"Y values: {Y.tolist()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Forward Pass\n",
        "\n",
        "Build the full forward pass: linear → batch norm → output layer → cross-entropy.\n",
        "\n",
        "We use `unbiased=False` for variance to match typical BatchNorm (divide by N, not N-1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Parameters (all require_grad for later autograd verification)\n",
        "W = torch.randn(10, 20, requires_grad=True)\n",
        "b = torch.randn(20, requires_grad=True)\n",
        "gamma = torch.ones(20, requires_grad=True)\n",
        "beta = torch.zeros(20, requires_grad=True)\n",
        "W2 = torch.randn(20, 5, requires_grad=True)\n",
        "b2 = torch.randn(5, requires_grad=True)\n",
        "\n",
        "eps = 1e-5\n",
        "\n",
        "# Forward pass\n",
        "h = X @ W + b                                    # (4, 20)\n",
        "h_mean = h.mean(0)                               # (20,)\n",
        "h_var = h.var(0, unbiased=False)                 # (20,) - BatchNorm uses N, not N-1\n",
        "h_norm = (h - h_mean) / (h_var + eps).sqrt()     # (4, 20)\n",
        "h_bn = gamma * h_norm + beta                     # (4, 20)\n",
        "logits = h_bn @ W2 + b2                          # (4, 5)\n",
        "loss = F.cross_entropy(logits, Y)\n",
        "\n",
        "# Retain gradients on intermediates for manual backprop verification\n",
        "h.retain_grad()\n",
        "h_norm.retain_grad()\n",
        "h_bn.retain_grad()\n",
        "logits.retain_grad()\n",
        "\n",
        "print(f\"h shape: {h.shape}\")\n",
        "print(f\"h_bn shape: {h_bn.shape}\")\n",
        "print(f\"logits shape: {logits.shape}\")\n",
        "print(f\"loss: {loss.item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Manual Gradients for Matrix Multiply\n",
        "\n",
        "For \\( h = X @ W + b \\), the chain rule gives:\n",
        "- \\( \\frac{\\partial L}{\\partial W} = X^T @ \\frac{\\partial L}{\\partial h} \\)\n",
        "- \\( \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial h} @ W^T \\)\n",
        "- \\( \\frac{\\partial L}{\\partial b} = \\sum_i \\frac{\\partial L}{\\partial h_i} \\) (sum over batch)\n",
        "\n",
        "We'll compute `dh` from downstream (BatchNorm), then derive `dW`, `dX`, `db` by hand and verify against autograd."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Manual Gradients for BatchNorm\n",
        "\n",
        "For \\( h_{bn} = \\gamma \\cdot h_{norm} + \\beta \\):\n",
        "- \\( d\\gamma = (dh_{bn} \\cdot h_{norm}).sum(0) \\)\n",
        "- \\( d\\beta = dh_{bn}.sum(0) \\)\n",
        "- \\( dh_{norm} = dh_{bn} \\cdot \\gamma \\)\n",
        "\n",
        "For the normalization \\( h_{norm} = (h - \\mu) / \\sqrt{\\sigma^2 + \\epsilon} \\), the backward through mean and variance gives:\n",
        "\\( dh = \\frac{1}{N \\cdot \\sigma} \\left( N \\cdot dh_{norm} - dh_{norm}.sum(0) - h_{norm} \\cdot (dh_{norm} * h_{norm}).sum(0) \\right) \\)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Manual Gradients for Cross-Entropy\n",
        "\n",
        "For softmax + cross-entropy combined, the gradient is elegant:\n",
        "$$ \\frac{\\partial L}{\\partial \\text{logits}} = \\frac{\\text{softmax(logits)} - \\text{one\\_hot}(Y)}{N} $$\n",
        "\n",
        "where \\( N \\) is the batch size (since `F.cross_entropy` uses `reduction='mean'` by default)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run autograd to get reference gradients (retain_grad was set in forward pass)\n",
        "loss.backward()\n",
        "\n",
        "# --- Manual: Matrix Multiply (h = X @ W + b) ---\n",
        "# Upstream gradient comes from BatchNorm: dh = gradient flowing into h\n",
        "dh = h.grad  # from autograd (we'll verify our manual dh later)\n",
        "\n",
        "dW_manual = X.T @ dh\n",
        "dX_manual = dh @ W.T\n",
        "db_manual = dh.sum(0)\n",
        "\n",
        "print(\"=== Matrix Multiply Gradients ===\")\n",
        "print(f\"dW match: {torch.allclose(dW_manual, W.grad, atol=1e-5)}\")\n",
        "print(f\"db match: {torch.allclose(db_manual, b.grad, atol=1e-5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Manual: BatchNorm (h_bn = gamma * h_norm + beta) ---\n",
        "dh_bn = h_bn.grad  # from autograd\n",
        "\n",
        "dgamma_manual = (dh_bn * h_norm).sum(0)\n",
        "dbeta_manual = dh_bn.sum(0)\n",
        "dh_norm_manual = dh_bn * gamma\n",
        "\n",
        "# Backward through normalization: h_norm = (h - h_mean) / std\n",
        "N = h.shape[0]\n",
        "std = (h_var + eps).sqrt()\n",
        "dh_manual_bn = (1 / (N * std)) * (\n",
        "    N * dh_norm_manual\n",
        "    - dh_norm_manual.sum(0)\n",
        "    - h_norm * (dh_norm_manual * h_norm).sum(0)\n",
        ")\n",
        "\n",
        "print(\"=== BatchNorm Gradients ===\")\n",
        "print(f\"dgamma match: {torch.allclose(dgamma_manual, gamma.grad, atol=1e-5)}\")\n",
        "print(f\"dbeta match: {torch.allclose(dbeta_manual, beta.grad, atol=1e-5)}\")\n",
        "print(f\"dh (from BN) match: {torch.allclose(dh_manual_bn, h.grad, atol=1e-5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Manual: Cross-Entropy ---\n",
        "# dlogits = (softmax(logits) - one_hot(Y)) / N\n",
        "probs = F.softmax(logits, dim=1)\n",
        "one_hot = F.one_hot(Y, num_classes=5).float()\n",
        "dlogits_manual = (probs - one_hot) / X.shape[0]\n",
        "\n",
        "print(\"=== Cross-Entropy Gradient ===\")\n",
        "print(f\"dlogits match: {torch.allclose(dlogits_manual, logits.grad, atol=1e-5)}\")\n",
        "print(\"\\nElegant formula: dlogits = (softmax(logits) - one_hot(Y)) / batch_size\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Full Verification\n",
        "\n",
        "Re-run the full forward pass, then compute *all* manual gradients from scratch (without using any .grad from intermediates). Compare every parameter gradient with `torch.allclose(manual, auto, atol=1e-5)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full verification: compute ALL gradients manually from dlogits downward\n",
        "# We need dlogits first (from cross-entropy), then propagate backward\n",
        "\n",
        "# 1. Cross-entropy: dlogits\n",
        "probs = F.softmax(logits, dim=1)\n",
        "one_hot = F.one_hot(Y, num_classes=5).float()\n",
        "dlogits = (probs - one_hot) / X.shape[0]\n",
        "\n",
        "# 2. Output layer: logits = h_bn @ W2 + b2\n",
        "dh_bn = dlogits @ W2.T\n",
        "dW2_manual = h_bn.T @ dlogits\n",
        "db2_manual = dlogits.sum(0)\n",
        "\n",
        "# 3. BatchNorm: h_bn = gamma * h_norm + beta\n",
        "dgamma = (dh_bn * h_norm).sum(0)\n",
        "dbeta = dh_bn.sum(0)\n",
        "dh_norm = dh_bn * gamma\n",
        "\n",
        "# 4. Normalization: h_norm = (h - mean) / std\n",
        "N = h.shape[0]\n",
        "std = (h_var + eps).sqrt()\n",
        "dh = (1 / (N * std)) * (\n",
        "    N * dh_norm - dh_norm.sum(0) - h_norm * (dh_norm * h_norm).sum(0)\n",
        ")\n",
        "\n",
        "# 5. Linear: h = X @ W + b\n",
        "dW_manual = X.T @ dh\n",
        "db_manual = dh.sum(0)\n",
        "\n",
        "# Verify all (autograd already ran in previous cells)\n",
        "checks = [\n",
        "    (\"W\", dW_manual, W.grad),\n",
        "    (\"b\", db_manual, b.grad),\n",
        "    (\"gamma\", dgamma, gamma.grad),\n",
        "    (\"beta\", dbeta, beta.grad),\n",
        "    (\"W2\", dW2_manual, W2.grad),\n",
        "    (\"b2\", db2_manual, b2.grad),\n",
        "]\n",
        "print(\"=== Full Verification ===\")\n",
        "all_ok = True\n",
        "for name, manual, auto in checks:\n",
        "    ok = torch.allclose(manual, auto, atol=1e-5)\n",
        "    all_ok = all_ok and ok\n",
        "    print(f\"  {name}: {ok}\")\n",
        "print(f\"\\nAll gradients match: {all_ok}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Building LLMs from Scratch** — [Day 13: Manual Backprop](https://omkarray.com/llm-day13.html) | [← Prev](llm_day12_batchnorm.ipynb) | [Next →](llm_day14.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}