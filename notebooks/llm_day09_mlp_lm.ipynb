{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9: MLP Language Model (Bengio et al. 2003)\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We move from **bigram** models to a more powerful **MLP (Multi-Layer Perceptron) language model** following the seminal work of Bengio et al. (2003). Key improvements:\n",
    "\n",
    "- **Context windows**: Instead of conditioning on a single previous character, we use a fixed context of multiple characters (e.g., 3 chars → predict next)\n",
    "- **Learned embeddings**: Each character is mapped to a dense vector via an embedding lookup table, allowing the model to learn meaningful representations\n",
    "- **Non-linear hidden layer**: A tanh-activated hidden layer captures complex interactions between context characters\n",
    "\n",
    "This architecture is a precursor to modern neural language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Use the hardcoded names list. Build `stoi` (string → index) and `itos` (index → string) with `'.'` as token 0 (start/end marker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'mia', 'charlotte', 'amelia', 'harper', 'evelyn',\n",
    "         'abigail', 'emily', 'ella', 'elizabeth', 'camila', 'luna', 'sofia', 'avery', 'mila', 'aria']\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {'.': 0, **{c: i + 1 for i, c in enumerate(chars)}}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "print(f\"Dataset: {len(words)} names\")\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "print(f\"stoi: {stoi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Dataset\n",
    "\n",
    "Create training examples with `block_size=3` (context window). For each word, create (context, target) pairs where context is 3 consecutive chars and target is the next char. Store as `X` (n, block_size) and `Y` (n,) tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs) - block_size):\n",
    "        context = chs[i:i + block_size]\n",
    "        target = chs[i + block_size]\n",
    "        X.append([stoi[c] for c in context])\n",
    "        Y.append(stoi[target])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(f\"Training examples: {len(X)}\")\n",
    "print(f\"X shape: {X.shape}\")  # (n, block_size)\n",
    "print(f\"Y shape: {Y.shape}\")  # (n,)\n",
    "print(f\"Example: context {[itos[x.item()] for x in X[0]]} -> target {itos[Y[0].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The MLP Architecture\n",
    "\n",
    "Parameters:\n",
    "- `C`: embedding lookup table (27 chars, 10-dim embeddings)\n",
    "- `W1`, `b1`: hidden layer (3×10=30 input, 200 hidden)\n",
    "- `W2`, `b2`: output layer (200 → 27 logits)\n",
    "\n",
    "All parameters have `requires_grad=True` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "C = torch.randn((27, 10), requires_grad=True)   # embedding lookup table\n",
    "W1 = torch.randn((30, 200), requires_grad=True)  # 3*10=30 input, 200 hidden\n",
    "b1 = torch.randn(200, requires_grad=True)\n",
    "W2 = torch.randn((200, 27), requires_grad=True)  # output layer\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"C: {C.shape}, W1: {W1.shape}, b1: {b1.shape}, W2: {W2.shape}, b2: {b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Pass\n",
    "\n",
    "1. **Embedding lookup**: `emb = C[X]` — each context char gets a 10-dim vector\n",
    "2. **Flatten**: `emb.view(-1, 30)` — concatenate 3 context embeddings into 30-dim input\n",
    "3. **Hidden layer**: `h = tanh(emb @ W1 + b1)`\n",
    "4. **Output layer**: `logits = h @ W2 + b2`\n",
    "5. **Loss**: `F.cross_entropy(logits, Y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def forward(X, Y):\n",
    "    emb = C[X]                    # (n, block_size, 10)\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)  # (n, 200)\n",
    "    logits = h @ W2 + b2          # (n, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss\n",
    "\n",
    "# Sanity check\n",
    "loss = forward(X, Y)\n",
    "print(f\"Initial loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Mini-batch training: sample 32 random indices per step, forward, backward, update. Run 10000 steps, print loss every 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn((27, 10), requires_grad=True)\n",
    "W1 = torch.randn((30, 200), requires_grad=True)\n",
    "b1 = torch.randn(200, requires_grad=True)\n",
    "W2 = torch.randn((200, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "step_size = 0.1\n",
    "n = X.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "losses = []\n",
    "for step in range(10000):\n",
    "    # Mini-batch\n",
    "    ix = torch.randint(0, n, (batch_size,))\n",
    "    Xb, Yb = X[ix], Y[ix]\n",
    "    \n",
    "    # Forward\n",
    "    emb = C[Xb]\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    for p in parameters:\n",
    "        p.data -= step_size * p.grad\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}: loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sampling\n",
    "\n",
    "Generate 10 names from the trained model. Start with '.' context, sample next char, shift context, repeat until '.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "generated = []\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0, 0, 0]  # ['.', '.', '.']\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]  # (1, 3, 10)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    generated.append(''.join(out))\n",
    "\n",
    "print(\"Generated names:\")\n",
    "for name in generated:\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Curve\n",
    "\n",
    "Plot training loss over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, alpha=0.8, linewidth=0.5)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP Language Model — Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 9: MLP LM](https://omkarray.com/llm-day9.html) | [← Prev](llm_day08_training_loops.ipynb) | [Next →](llm_day10_mlp_refactor.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
