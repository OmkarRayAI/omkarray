{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Tensors, Broadcasting & torch.Tensor Deep Dive\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Understanding PyTorch tensors: shapes, broadcasting, and the operations that make neural nets work. Tensors are the fundamental data structure—scalars, vectors, matrices, and higher-dimensional arrays. Broadcasting lets us write concise, efficient code without explicit loops. Mastering these concepts is essential for building and debugging neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor Basics\n",
    "\n",
    "Create scalars, vectors, matrices, and 3D tensors. Inspect `.shape`, `.dtype`, and `.device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar (0-dimensional)\n",
    "s = torch.tensor(3.14)\n",
    "print(\"Scalar:\", s, \"| shape:\", s.shape, \"| dtype:\", s.dtype)\n",
    "\n",
    "# Vector (1D)\n",
    "v = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Vector:\", v, \"| shape:\", v.shape)\n",
    "\n",
    "# Matrix (2D)\n",
    "M = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "print(\"Matrix shape:\", M.shape, \"→\", M.shape[0], \"rows ×\", M.shape[1], \"cols\")\n",
    "\n",
    "# 3D tensor (e.g., batch of matrices)\n",
    "T = torch.randn(2, 3, 4)\n",
    "print(\"3D tensor shape:\", T.shape)\n",
    "\n",
    "# Device (CPU by default)\n",
    "print(\"Device:\", M.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Broadcasting Rules\n",
    "\n",
    "Broadcasting automatically expands smaller tensors to match larger ones for element-wise ops. Rules: (1) align shapes from the right, (2) dimensions are compatible if equal or one is 1, (3) missing dims are treated as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Vector + scalar: scalar broadcasts to every element\n",
    "v = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Vector + scalar:\", v + 10)\n",
    "\n",
    "# Matrix + row vector: row broadcasts to every row\n",
    "M = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "row = torch.tensor([100.0, 200.0, 300.0])  # shape (3,)\n",
    "print(\"Matrix + row vector:\")\n",
    "print(M + row)\n",
    "\n",
    "# Matrix + column vector: column broadcasts to every column\n",
    "col = torch.tensor([[10.0], [20.0]])  # shape (2, 1)\n",
    "print(\"Matrix + column vector:\")\n",
    "print(M + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting FAILS: incompatible shapes\n",
    "try:\n",
    "    A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])  # (2, 2)\n",
    "    B = torch.tensor([1.0, 2.0, 3.0])          # (3,) — incompatible!\n",
    "    C = A + B\n",
    "except RuntimeError as e:\n",
    "    print(\"Expected error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Row-wise Normalization\n",
    "\n",
    "Normalize each row to sum to 1. Use `keepdim=True` so the sum has shape `(n, 1)` and broadcasts correctly when dividing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "M = torch.rand(3, 4)\n",
    "print(\"Original matrix:\")\n",
    "print(M)\n",
    "print(\"Row sums (before norm):\", M.sum(1))\n",
    "\n",
    "# With keepdim=True: sum has shape (3, 1) → broadcasts for division\n",
    "row_sums = M.sum(1, keepdim=True)\n",
    "M_norm = M / row_sums\n",
    "print(\"\\nNormalized (each row sums to 1):\")\n",
    "print(M_norm)\n",
    "print(\"Row sums (after norm):\", M_norm.sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-Hot Encoding\n",
    "\n",
    "Use `torch.nn.functional.one_hot` to encode integers into a binary matrix. Each integer becomes a row with a 1 at its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encode indices 0, 2, 1 with vocabulary size 4\n",
    "indices = torch.tensor([0, 2, 1])\n",
    "one_hot = F.one_hot(indices, num_classes=4)\n",
    "print(\"Indices:\", indices)\n",
    "print(\"One-hot encoding (each row = one sample):\")\n",
    "print(one_hot)\n",
    "print(\"Shape:\", one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matrix Multiply for Neural Nets\n",
    "\n",
    "The `xenc @ W` pattern: one-hot encoded input times weight matrix = logits. This is the core forward pass of a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 5 samples, each is an index in vocab of size 27 (e.g., characters)\n",
    "ix = torch.tensor([0, 5, 13, 0, 1])\n",
    "xenc = F.one_hot(ix, num_classes=27).float()  # (5, 27)\n",
    "\n",
    "# Weight matrix: 27 inputs → 27 outputs (logits per class)\n",
    "W = torch.randn(27, 27)\n",
    "\n",
    "# Forward: xenc @ W = logits\n",
    "logits = xenc @ W  # (5, 27) @ (27, 27) → (5, 27)\n",
    "print(\"xenc shape:\", xenc.shape)\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"\\nLogits (first 2 rows):\")\n",
    "print(logits[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Softmax\n",
    "\n",
    "Implement softmax manually: `counts = logits.exp(); probs = counts / counts.sum(1, keepdim=True)`. Converts logits to probabilities that sum to 1 per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([[1.0, 2.0, 3.0], [0.5, 1.0, 0.1]])\n",
    "\n",
    "# Softmax: exp(logits) / sum(exp(logits)) per row\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "print(\"Logits:\")\n",
    "print(logits)\n",
    "print(\"\\nProbabilities (each row sums to 1):\")\n",
    "print(probs)\n",
    "print(\"Row sums:\", probs.sum(1))\n",
    "\n",
    "# Compare with F.softmax\n",
    "probs_ref = torch.softmax(logits, dim=1)\n",
    "print(\"\\nMatch F.softmax:\", torch.allclose(probs, probs_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Pitfalls\n",
    "\n",
    "Watch out for: missing `keepdim=True`, integer vs float tensors, and in-place operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Pitfall 1: Missing keepdim=True — wrong shape for broadcasting\n",
    "M = torch.rand(3, 4)\n",
    "s_wrong = M.sum(1)       # shape (3,) — can cause subtle bugs\n",
    "s_right = M.sum(1, keepdim=True)  # shape (3, 1) — broadcasts correctly\n",
    "print(\"Without keepdim:\", s_wrong.shape)\n",
    "print(\"With keepdim:\", s_right.shape)\n",
    "\n",
    "# Pitfall 2: Integer tensors — many ops require float\n",
    "x_int = torch.tensor([1, 2, 3])\n",
    "# x_int.exp()  # would fail: exp not for integers\n",
    "x_float = x_int.float()\n",
    "print(\"\\nInteger .float():\", x_float.dtype)\n",
    "\n",
    "# Pitfall 3: In-place ops — breaks autograd, use sparingly\n",
    "a = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "# a.add_(1)  # in-place: dangerous with requires_grad\n",
    "b = a + 1   # out-of-place: safe\n",
    "print(\"\\nOut-of-place is safer for autograd.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Blog:** [Day 7 — Tensors, Broadcasting & torch.Tensor Deep Dive](https://omkarray.com/llm-day7.html)\n",
    "\n",
    "**Prev:** [Day 6 — Bigram Model](llm_day06_bigram.ipynb) · **Next:** [Day 8 — MLP Language Model](llm_day08_mlp_lm.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
