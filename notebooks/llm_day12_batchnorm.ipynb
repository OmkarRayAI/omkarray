{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 12: Batch Normalization — Taming Internal Covariate Shift\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**Batch Normalization** (Ioffe & Szegedy, 2015) addresses **internal covariate shift**: as parameters update during training, the distribution of activations at each layer shifts, making it harder for deeper layers to learn. BatchNorm normalizes activations to zero mean and unit variance (per feature dimension), then applies learned scale (γ) and shift (β). This:\n",
    "\n",
    "- **Stabilizes training**: Gradients flow more smoothly; higher learning rates become feasible\n",
    "- **Reduces sensitivity to initialization**: Networks converge faster and more reliably\n",
    "- **Acts as mild regularization**: Batch statistics add noise during training\n",
    "\n",
    "We implement BatchNorm1d from scratch and compare MLPs with and without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BatchNorm1d from Scratch\n",
    "\n",
    "Implement a `BatchNorm1d` class:\n",
    "- `gamma` (scale), `beta` (shift): learnable parameters\n",
    "- `running_mean`, `running_var`: exponential moving averages for inference\n",
    "- **Training**: use batch mean/var, update running stats\n",
    "- **Eval**: use running stats (no batch dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # Learnable scale and shift\n",
    "        self.gamma = torch.ones(dim, requires_grad=True)\n",
    "        self.beta = torch.zeros(dim, requires_grad=True)\n",
    "        # Running statistics (not learned, updated during training)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x, training=True):\n",
    "        # x: (batch, dim)\n",
    "        if training:\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0, unbiased=False)\n",
    "            # Update running stats: EMA\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "            mean, var = batch_mean, batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "bn = BatchNorm1d(10)\n",
    "x = torch.randn(32, 10)\n",
    "out = bn(x, training=True)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Output mean (per dim): {out.mean(0).tolist()[:5]}...\")\n",
    "print(f\"Output std (per dim): {out.std(0).tolist()[:5]}...\")\n",
    "print(f\"Parameters: {[p.shape for p in bn.parameters()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & Model\n",
    "\n",
    "Same names dataset as before. Build an MLP with BatchNorm after each hidden layer (before tanh). We'll also define a version without BatchNorm for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'mia', 'charlotte', 'amelia', 'harper', 'evelyn',\n",
    "         'abigail', 'emily', 'ella', 'elizabeth', 'camila', 'luna', 'sofia', 'avery', 'mila', 'aria']\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {'.': 0, **{c: i + 1 for i, c in enumerate(chars)}}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs) - block_size):\n",
    "        context = chs[i:i + block_size]\n",
    "        target = chs[i + block_size]\n",
    "        X.append([stoi[c] for c in context])\n",
    "        Y.append(stoi[target])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "n, block_size = X.shape[0], X.shape[1]\n",
    "vocab_size = len(stoi)\n",
    "emb_dim = 10\n",
    "hidden = 200\n",
    "\n",
    "print(f\"Dataset: {len(words)} names, {n} examples\")\n",
    "print(f\"block_size={block_size}, vocab_size={vocab_size}, hidden={hidden}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(use_bn=False):\n",
    "    \"\"\"Initialize MLP params. use_bn=True adds BatchNorm layers.\"\"\"\n",
    "    C = torch.randn(vocab_size, emb_dim, requires_grad=True)\n",
    "    W1 = torch.randn(block_size * emb_dim, hidden, requires_grad=True)\n",
    "    b1 = torch.randn(hidden, requires_grad=True)\n",
    "    W2 = torch.randn(hidden, vocab_size, requires_grad=True)\n",
    "    b2 = torch.randn(vocab_size, requires_grad=True)\n",
    "    params = [C, W1, b1, W2, b2]\n",
    "    bn1 = BatchNorm1d(hidden) if use_bn else None\n",
    "    if use_bn:\n",
    "        params.extend(bn1.parameters())\n",
    "    return C, W1, b1, W2, b2, bn1, params\n",
    "\n",
    "\n",
    "def forward_no_bn(X, Y, C, W1, b1, W2, b2, return_h=False):\n",
    "    emb = C[X].view(-1, block_size * emb_dim)\n",
    "    h = torch.tanh(emb @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return (loss, h) if return_h else loss\n",
    "\n",
    "\n",
    "def forward_bn(X, Y, C, W1, b1, bn1, W2, b2, training=True, return_h=False):\n",
    "    emb = C[X].view(-1, block_size * emb_dim)\n",
    "    preact = emb @ W1 + b1\n",
    "    h = torch.tanh(bn1(preact, training=training))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return (loss, h, preact) if return_h else loss\n",
    "\n",
    "\n",
    "# Quick test\n",
    "C, W1, b1, W2, b2, bn1, params_bn = init_params(use_bn=True)\n",
    "loss = forward_bn(X, Y, C, W1, b1, bn1, W2, b2, training=True)\n",
    "print(f\"Forward (with BN): loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Before vs After BatchNorm\n",
    "\n",
    "Train two models: one without BN, one with BN. Compare activation distributions and training loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(use_bn, steps=5000, batch_size=32, lr=0.1):\n",
    "    torch.manual_seed(42)\n",
    "    C, W1, b1, W2, b2, bn1, params = init_params(use_bn=use_bn)\n",
    "    losses = []\n",
    "    for step in range(steps):\n",
    "        ix = torch.randint(0, n, (batch_size,))\n",
    "        Xb, Yb = X[ix], Y[ix]\n",
    "        if use_bn:\n",
    "            loss = forward_bn(Xb, Yb, C, W1, b1, bn1, W2, b2, training=True)\n",
    "        else:\n",
    "            loss = forward_no_bn(Xb, Yb, C, W1, b1, W2, b2)\n",
    "        losses.append(loss.item())\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        for p in params:\n",
    "            p.data -= lr * p.grad\n",
    "    return losses, (C, W1, b1, bn1, W2, b2) if use_bn else (C, W1, b1, None, W2, b2)\n",
    "\n",
    "\n",
    "print(\"Training without BatchNorm...\")\n",
    "losses_no_bn, model_no_bn = train_model(use_bn=False)\n",
    "print(\"Training with BatchNorm...\")\n",
    "losses_bn, model_bn = train_model(use_bn=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].plot(losses_no_bn, alpha=0.8, label='No BatchNorm')\n",
    "axes[0].plot(losses_bn, alpha=0.8, label='With BatchNorm')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(losses_no_bn[-500:], alpha=0.8, label='No BatchNorm')\n",
    "axes[1].plot(losses_bn[-500:], alpha=0.8, label='With BatchNorm')\n",
    "axes[1].set_xlabel('Step (last 500)')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss (Zoomed)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation distributions: collect activations from a fresh model at init and after a few steps\n",
    "def get_activation_histograms(use_bn, num_steps=100, num_batches=20):\n",
    "    torch.manual_seed(42)\n",
    "    C, W1, b1, W2, b2, bn1, params = init_params(use_bn=use_bn)\n",
    "    acts_init, acts_after = [], []\n",
    "    for _ in range(num_batches):\n",
    "        ix = torch.randint(0, n, (32,))\n",
    "        Xb, Yb = X[ix], Y[ix]\n",
    "        if use_bn:\n",
    "            _, h, preact = forward_bn(Xb, Yb, C, W1, b1, bn1, W2, b2, training=True, return_h=True)\n",
    "        else:\n",
    "            _, h = forward_no_bn(Xb, Yb, C, W1, b1, W2, b2, return_h=True)\n",
    "        acts_init.append(h.detach().flatten())\n",
    "    acts_init = torch.cat(acts_init)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        ix = torch.randint(0, n, (32,))\n",
    "        Xb, Yb = X[ix], Y[ix]\n",
    "        if use_bn:\n",
    "            loss = forward_bn(Xb, Yb, C, W1, b1, bn1, W2, b2, training=True)\n",
    "            _, h, _ = forward_bn(Xb, Yb, C, W1, b1, bn1, W2, b2, training=True, return_h=True)\n",
    "        else:\n",
    "            loss = forward_no_bn(Xb, Yb, C, W1, b1, W2, b2)\n",
    "            _, h = forward_no_bn(Xb, Yb, C, W1, b1, W2, b2, return_h=True)\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        for p in params:\n",
    "            p.data -= 0.1 * p.grad\n",
    "    for _ in range(num_batches):\n",
    "        ix = torch.randint(0, n, (32,))\n",
    "        Xb, Yb = X[ix], Y[ix]\n",
    "        if use_bn:\n",
    "            _, h, _ = forward_bn(Xb, Yb, C, W1, b1, bn1, W2, b2, training=True, return_h=True)\n",
    "        else:\n",
    "            _, h = forward_no_bn(Xb, Yb, C, W1, b1, W2, b2, return_h=True)\n",
    "        acts_after.append(h.detach().flatten())\n",
    "    acts_after = torch.cat(acts_after)\n",
    "    return acts_init, acts_after\n",
    "\n",
    "\n",
    "acts_no_bn_init, acts_no_bn_after = get_activation_histograms(use_bn=False)\n",
    "acts_bn_init, acts_bn_after = get_activation_histograms(use_bn=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "axes[0, 0].hist(acts_no_bn_init.numpy(), bins=50, alpha=0.7, color='steelblue', edgecolor='black', density=True)\n",
    "axes[0, 0].set_title('No BN: Activations at Init')\n",
    "axes[0, 0].set_xlabel('Activation value')\n",
    "\n",
    "axes[0, 1].hist(acts_no_bn_after.numpy(), bins=50, alpha=0.7, color='steelblue', edgecolor='black', density=True)\n",
    "axes[0, 1].set_title('No BN: Activations After 100 Steps')\n",
    "axes[0, 1].set_xlabel('Activation value')\n",
    "\n",
    "axes[1, 0].hist(acts_bn_init.numpy(), bins=50, alpha=0.7, color='coral', edgecolor='black', density=True)\n",
    "axes[1, 0].set_title('With BN: Activations at Init')\n",
    "axes[1, 0].set_xlabel('Activation value')\n",
    "\n",
    "axes[1, 1].hist(acts_bn_after.numpy(), bins=50, alpha=0.7, color='coral', edgecolor='black', density=True)\n",
    "axes[1, 1].set_title('With BN: Activations After 100 Steps')\n",
    "axes[1, 1].set_xlabel('Activation value')\n",
    "\n",
    "plt.suptitle('Activation Distributions at Hidden Layer')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train vs Eval Mode\n",
    "\n",
    "During **training**: BatchNorm uses batch mean and variance. During **eval**: it uses `running_mean` and `running_var`. Switching matters — if you forget to set eval mode, inference can behave differently (especially with small batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model with BN\n",
    "C, W1, b1, bn1, W2, b2 = model_bn\n",
    "\n",
    "# Single example (batch_size=1) — batch stats would be degenerate!\n",
    "x_single = X[:1]\n",
    "y_single = Y[:1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss_train_mode = forward_bn(x_single, y_single, C, W1, b1, bn1, W2, b2, training=True)\n",
    "    loss_eval_mode = forward_bn(x_single, y_single, C, W1, b1, bn1, W2, b2, training=False)\n",
    "\n",
    "print(f\"Batch size 1:\")\n",
    "print(f\"  training=True  (batch stats): loss = {loss_train_mode.item():.4f}\")\n",
    "print(f\"  training=False (running stats): loss = {loss_eval_mode.item():.4f}\")\n",
    "print(f\"  Difference: {abs(loss_train_mode.item() - loss_eval_mode.item()):.4f}\")\n",
    "\n",
    "# With full batch, they should be closer\n",
    "with torch.no_grad():\n",
    "    loss_train_full = forward_bn(X, Y, C, W1, b1, bn1, W2, b2, training=True)\n",
    "    loss_eval_full = forward_bn(X, Y, C, W1, b1, bn1, W2, b2, training=False)\n",
    "\n",
    "print(f\"\\nFull batch:\")\n",
    "print(f\"  training=True:  loss = {loss_train_full.item():.4f}\")\n",
    "print(f\"  training=False: loss = {loss_eval_full.item():.4f}\")\n",
    "print(f\"  Difference: {abs(loss_train_full.item() - loss_eval_full.item()):.4f}\")\n",
    "print(\"\\n→ Always use training=False at inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Running Mean Update\n",
    "\n",
    "Running stats are updated via exponential moving average:\n",
    "$$\\text{running\\_mean} = (1 - \\text{momentum}) \\cdot \\text{running\\_mean} + \\text{momentum} \\cdot \\text{batch\\_mean}$$\n",
    "\n",
    "Visualize how `running_mean` converges during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "C, W1, b1, W2, b2, bn1, params = init_params(use_bn=True)\n",
    "\n",
    "momentum = 0.1\n",
    "running_means = []  # Store first 5 dims of running_mean over time\n",
    "batch_means_history = []\n",
    "\n",
    "for step in range(500):\n",
    "    ix = torch.randint(0, n, (32,))\n",
    "    Xb, Yb = X[ix], Y[ix]\n",
    "    emb = C[Xb].view(-1, block_size * emb_dim)\n",
    "    preact = emb @ W1 + b1\n",
    "    batch_mean = preact.mean(dim=0)\n",
    "    _ = bn1(preact, training=True)\n",
    "    running_means.append(bn1.running_mean[:5].clone().detach())\n",
    "    batch_means_history.append(batch_mean[:5].clone().detach())\n",
    "    loss = forward_bn(Xb, Yb, C, W1, b1, bn1, W2, b2, training=True)\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data -= 0.1 * p.grad\n",
    "\n",
    "running_means = torch.stack(running_means)  # (500, 5)\n",
    "batch_means_history = torch.stack(batch_means_history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "for d in range(5):\n",
    "    axes[0].plot(running_means[:, d].numpy(), label=f'dim {d}', alpha=0.8)\n",
    "axes[0].set_ylabel('running_mean')\n",
    "axes[0].set_title('Running Mean Convergence (first 5 dims)')\n",
    "axes[0].legend(loc='upper right', fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for d in range(5):\n",
    "    axes[1].plot(batch_means_history[:, d].numpy(), label=f'dim {d}', alpha=0.8)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('batch_mean')\n",
    "axes[1].set_title('Batch Mean (noisy per step)')\n",
    "axes[1].legend(loc='upper right', fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Running mean smooths out batch statistics → stable inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 12: Batch Normalization](https://omkarray.com/llm-day12.html) | [← Prev](llm_day11.ipynb) | [Next →](llm_day13.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
