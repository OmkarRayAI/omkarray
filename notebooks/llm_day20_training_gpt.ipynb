{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 20: Training GPT — From Random Noise to Coherent Text\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We have a complete GPT. Today we train it on real text and watch it learn to generate coherent output — the capstone of the series.\n",
    "\n",
    "**What we cover:**\n",
    "1. Preparing a character-level dataset\n",
    "2. The training loop with AdamW and a cosine LR schedule\n",
    "3. Overfitting diagnostics — train vs val loss\n",
    "4. Generation: temperature, top-k, greedy\n",
    "5. Scaling laws — why bigger + more data = better\n",
    "6. The path from this GPT to GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use tiny shakespeare or a fallback text\n",
    "try:\n",
    "    with open('../input.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    print(f\"Loaded dataset: {len(text):,} characters\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback: short public domain text (first lines of Hamlet)\n",
    "    text = \"\"\"\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To die, to sleep,\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to: 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep, perchance to dream. Ay, there's the rub,\n",
    "For in that sleep of death what dreams may come,\n",
    "When we have shuffled off this mortal coil,\n",
    "Must give us pause. There's the respect\n",
    "That makes calamity of so long life.\n",
    "\"\"\" * 50  # repeat to get enough data\n",
    "    print(f\"Using fallback text: {len(text):,} characters\")\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join(itos[i] for i in l)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Chars: {''.join(chars[:20])}...\")\n",
    "\n",
    "# Train/val split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]\n",
    "print(f\"Train tokens: {len(train_data):,}, Val tokens: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, block_size=64, batch_size=32):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(f\"x shape: {x.shape}  (batch_size, block_size)\")\n",
    "print(f\"y shape: {y.shape}  (targets are x shifted by 1)\")\n",
    "print(f\"\\nSample input:  '{decode(x[0, :20].tolist())}'\")\n",
    "print(f\"Sample target: '{decode(y[0, :20].tolist())}'\")\n",
    "print(\"(Target is input shifted by 1 — predict next character at every position)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT Model (from Day 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 65\n",
    "    block_size: int = 64\n",
    "    n_layer:    int = 4\n",
    "    n_head:     int = 4\n",
    "    n_embd:     int = 128\n",
    "    dropout:    float = 0.1\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head, self.n_embd = config.n_head, config.n_embd\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.attn_drop = nn.Dropout(config.dropout)\n",
    "        self.resid_drop = nn.Dropout(config.dropout)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape; H, hs = self.n_head, self.head_size\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q = q.view(B,T,H,hs).transpose(1,2); k = k.view(B,T,H,hs).transpose(1,2); v = v.view(B,T,H,hs).transpose(1,2)\n",
    "        att = self.attn_drop(F.softmax((q@k.transpose(-2,-1))*(hs**-0.5).masked_fill(self.bias[:T,:T]==0,float('-inf')), dim=-1))\n",
    "        return self.resid_drop(self.c_proj((att@v).transpose(1,2).contiguous().view(B,T,C)))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(config.n_embd, 4*config.n_embd), nn.GELU(),\n",
    "                                  nn.Linear(4*config.n_embd, config.n_embd), nn.Dropout(config.dropout))\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd); self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd); self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)); x = x + self.mlp(self.ln2(x)); return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        for p in self.parameters():\n",
    "            if p.dim() >= 2: nn.init.normal_(p, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        x = self.transformer.drop(self.transformer.wte(idx) + self.transformer.wpe(pos))\n",
    "        for block in self.transformer.h: x = block(x)\n",
    "        logits = self.lm_head(self.transformer.ln_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx[:, -self.config.block_size:])\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            idx = torch.cat([idx, torch.multinomial(F.softmax(logits, dim=-1), 1)], dim=1)\n",
    "        return idx\n",
    "\n",
    "config = GPTConfig(vocab_size=vocab_size)\n",
    "model = GPT(config)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Cosine LR Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=50):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split in ['train', 'val']:\n",
    "        ls = []\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split, block_size=config.block_size)\n",
    "            _, loss = model(x, y)\n",
    "            ls.append(loss.item())\n",
    "        losses[split] = sum(ls) / len(ls)\n",
    "    model.train()\n",
    "    return losses\n",
    "\n",
    "\n",
    "# AdamW with cosine LR schedule (GPT-2 style)\n",
    "max_lr    = 3e-4\n",
    "min_lr    = max_lr / 10\n",
    "max_steps = 3000\n",
    "warmup    = 100\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup:\n",
    "        return max_lr * step / warmup\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    ratio = (step - warmup) / (max_steps - warmup)\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * ratio))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "\n",
    "train_losses, val_losses, lrs = [], [], []\n",
    "eval_every = 300\n",
    "\n",
    "for step in range(max_steps + 1):\n",
    "    # Set LR\n",
    "    lr = get_lr(step)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    if step % eval_every == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        lrs.append(lr)\n",
    "        print(f\"step {step:4d} | train {losses['train']:.4f} | val {losses['val']:.4f} | lr {lr:.2e}\")\n",
    "\n",
    "    if step == max_steps:\n",
    "        break\n",
    "\n",
    "    x, y = get_batch('train', block_size=config.block_size)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # gradient clipping\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_eval = list(range(0, max_steps + 1, eval_every))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(steps_eval, train_losses, label='Train', color='steelblue', linewidth=2)\n",
    "axes[0].plot(steps_eval, val_losses,   label='Val',   color='tomato',    linewidth=2)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Train vs Validation Loss\\n(gap = overfitting)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(steps_eval, lrs, color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Cosine LR Schedule\\n(warmup → cosine decay → min_lr)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {train_losses[0]:.4f} (expected ≈ {math.log(vocab_size):.4f} = log(vocab_size))\")\n",
    "print(f\"Final train:  {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val:    {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Generated text (temperature=1.0, top_k=40):\")\n",
    "print(\"=\" * 60)\n",
    "context = torch.zeros(1, 1, dtype=torch.long)\n",
    "generated = model.generate(context, max_new_tokens=300, temperature=1.0, top_k=40)\n",
    "print(decode(generated[0].tolist()))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"More deterministic (temperature=0.5):\")\n",
    "print(\"=\" * 60)\n",
    "generated = model.generate(torch.zeros(1,1,dtype=torch.long), max_new_tokens=200, temperature=0.5, top_k=10)\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scaling Laws — The Path to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinchilla scaling law: loss ≈ A/N^alpha + B/D^beta + C\n",
    "# Optimal: N params, D = 20*N tokens (Chinchilla ratio)\n",
    "\n",
    "models = [\n",
    "    (\"Our GPT (today)\",     0.4e6,     1e6,      \"~4.0\"),\n",
    "    (\"GPT-2 Small\",        124e6,     9e9,       \"~3.3\"),\n",
    "    (\"GPT-2 XL\",          1542e6,    40e9,       \"~2.9\"),\n",
    "    (\"GPT-3\",            175e9,    300e9,        \"~2.0\"),\n",
    "    (\"Chinchilla (70B)\",   70e9,   1400e9,       \"~1.9\"),\n",
    "    (\"Llama 3 (70B)\",      70e9,  15000e9,       \"~1.7\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<25} {'Params':>12} {'Tokens':>14} {'Est. Loss':>12}\")\n",
    "print(\"-\" * 68)\n",
    "for name, params, tokens, loss in models:\n",
    "    def fmt(n):\n",
    "        if n >= 1e12: return f\"{n/1e12:.0f}T\"\n",
    "        if n >= 1e9:  return f\"{n/1e9:.0f}B\"\n",
    "        if n >= 1e6:  return f\"{n/1e6:.0f}M\"\n",
    "        return f\"{n/1e3:.0f}K\"\n",
    "    print(f\"{name:<25} {fmt(params):>12} {fmt(tokens):>14} {loss:>12}\")\n",
    "\n",
    "print(\"\\nKey insight: Loss scales predictably with compute (Hoffmann et al., 2022)\")\n",
    "print(\"More params + proportionally more data = consistently lower loss\")\n",
    "\n",
    "# Visualize (illustrative, not exact)\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "names = [m[0] for m in models]\n",
    "params_list = [math.log10(m[1]) for m in models]\n",
    "losses = [float(m[3].replace('~','')) for m in models]\n",
    "\n",
    "scatter = ax.scatter(params_list, losses, s=150, c=range(len(models)), cmap='viridis', zorder=3)\n",
    "ax.plot(params_list, losses, '--', color='gray', alpha=0.5)\n",
    "for i, name in enumerate(names):\n",
    "    ax.annotate(name.split('(')[0].strip(), (params_list[i], losses[i]),\n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "ax.set_xlabel('log₁₀(Parameters)')\n",
    "ax.set_ylabel('Estimated Loss (lower = better)')\n",
    "ax.set_title('Scaling Laws: More Parameters → Lower Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Series Summary\n",
    "\n",
    "| Day | Topic | Key Concept |\n",
    "|-----|-------|-------------|\n",
    "| 1-5 | Micrograd | Autograd from scratch |\n",
    "| 6-8 | Bigram + Tensors | Language modeling basics |\n",
    "| 9-10 | MLP + Embeddings | Neural LM, lookup tables |\n",
    "| 11-12 | Activations + BatchNorm | Training stability |\n",
    "| 13-14 | Backprop + Cross-Entropy | Gradient mechanics |\n",
    "| 15 | WaveNet | Hierarchical context fusion |\n",
    "| 16 | Self-Attention | Query-key-value mechanism |\n",
    "| 17 | Multi-Head + Positional Enc | Parallel attention, order |\n",
    "| 18 | Transformer Block | Residuals + LayerNorm |\n",
    "| 19 | GPT Architecture | Full model assembly |\n",
    "| **20** | **Training GPT** | **LR schedule, generation** |\n",
    "\n",
    "The full GPT we built today has the **same architecture** as GPT-2. The only differences between our ~400K parameter baby GPT and GPT-4 are:\n",
    "- **Scale**: 400K → 1.7 trillion parameters\n",
    "- **Data**: 1M tokens → 13 trillion tokens  \n",
    "- **RLHF**: Reinforcement Learning from Human Feedback for alignment\n",
    "- **Compute**: a MacBook → thousands of A100s for months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 20: Training GPT](https://omkarray.com/llm-day20.html) | [← Prev](llm_day19_gpt.ipynb)\n",
    "\n",
    "*Series complete. You built a GPT from scratch — from a single Value node all the way to an autoregressive transformer.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
