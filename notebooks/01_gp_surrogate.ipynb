{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Surrogate — From Prior to Posterior\n",
    "\n",
    "**Bayesian Optimisation Series · Notebook 1 of 3**\n",
    "\n",
    "This notebook implements a Gaussian Process from scratch using only NumPy, then demonstrates:\n",
    "1. Sampling functions from the GP prior\n",
    "2. Conditioning on observations to get the posterior\n",
    "3. How uncertainty collapses near observed data\n",
    "4. The effect of noise on the posterior\n",
    "\n",
    "No libraries beyond NumPy and Matplotlib. Everything is explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (12, 5)\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The RBF Kernel\n",
    "\n",
    "The squared exponential (RBF) kernel:\n",
    "\n",
    "$$k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{|x - x'|^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "Two hyperparameters:\n",
    "- **Length-scale** $\\ell$: how far apart inputs must be before function values become uncorrelated\n",
    "- **Signal variance** $\\sigma_f^2$: the prior variance of function values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, length_scale=1.0, signal_var=1.0):\n",
    "    \"\"\"Compute the RBF (squared exponential) kernel matrix.\"\"\"\n",
    "    sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + \\\n",
    "             np.sum(X2**2, axis=1).reshape(1, -1) - \\\n",
    "             2 * X1 @ X2.T\n",
    "    return signal_var * np.exp(-0.5 * sqdist / length_scale**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling from the GP Prior\n",
    "\n",
    "A GP prior with mean $m(x) = 0$ and kernel $k$ says: for any finite set of test points $X_*$, the function values are jointly Gaussian:\n",
    "\n",
    "$$\\mathbf{f}_* \\sim \\mathcal{N}(\\mathbf{0}, K(X_*, X_*))$$\n",
    "\n",
    "Each sample is one possible function that the GP considers plausible *before seeing any data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "K_prior = rbf_kernel(X_test, X_test, length_scale=1.0, signal_var=1.0)\n",
    "K_prior += 1e-8 * np.eye(len(X_test))  # numerical stability\n",
    "\n",
    "L_prior = np.linalg.cholesky(K_prior)\n",
    "prior_samples = L_prior @ np.random.randn(len(X_test), 5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot(X_test, prior_samples[:, i], alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax.fill_between(X_test.ravel(), -2, 2, alpha=0.1, color='steelblue', label='±2σ prior')\n",
    "ax.axhline(0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('GP Prior — 5 sample functions (RBF kernel, ℓ=1.0, σ²_f=1.0)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GP Posterior — Conditioning on Observations\n",
    "\n",
    "Given $n$ observations $D_n = \\{(x_i, y_i)\\}$ with $y_i = f(x_i) + \\varepsilon_i$, the posterior at test point $x_*$ is:\n",
    "\n",
    "$$\\mu_n(x_*) = k(x_*, X) [K(X,X) + \\sigma_n^2 I]^{-1} \\mathbf{y}$$\n",
    "\n",
    "$$\\sigma_n^2(x_*) = k(x_*, x_*) - k(x_*, X) [K(X,X) + \\sigma_n^2 I]^{-1} k(X, x_*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_posterior(X_train, y_train, X_test, length_scale=1.0, signal_var=1.0, noise_var=1e-6):\n",
    "    \"\"\"Compute GP posterior mean and variance at test points.\"\"\"\n",
    "    K = rbf_kernel(X_train, X_train, length_scale, signal_var) + noise_var * np.eye(len(X_train))\n",
    "    K_s = rbf_kernel(X_train, X_test, length_scale, signal_var)\n",
    "    K_ss = rbf_kernel(X_test, X_test, length_scale, signal_var)\n",
    "\n",
    "    L = np.linalg.cholesky(K)\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y_train))\n",
    "    mu = K_s.T @ alpha\n",
    "\n",
    "    v = np.linalg.solve(L, K_s)\n",
    "    var = np.diag(K_ss) - np.sum(v**2, axis=0)\n",
    "    var = np.maximum(var, 1e-10)\n",
    "\n",
    "    return mu.ravel(), var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prior → Posterior: Watching Uncertainty Collapse\n",
    "\n",
    "We'll observe the same function at 0, 3, and 7 points — exactly matching the three-panel progression from the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_fn = lambda x: np.sin(x) + 0.5 * np.cos(2.5 * x)\n",
    "\n",
    "X_obs_all = np.array([-3.5, -2.0, -0.5, 0.8, 2.0, 3.2, 4.0]).reshape(-1, 1)\n",
    "y_obs_all = true_fn(X_obs_all.ravel())\n",
    "\n",
    "observation_counts = [0, 3, 7]\n",
    "titles = ['Prior (n=0)', 'Posterior (n=3)', 'Posterior (n=7, converging)']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "\n",
    "for idx, (n_obs, title) in enumerate(zip(observation_counts, titles)):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    if n_obs == 0:\n",
    "        mu = np.zeros(len(X_test))\n",
    "        var = np.ones(len(X_test))\n",
    "    else:\n",
    "        X_train = X_obs_all[:n_obs]\n",
    "        y_train = y_obs_all[:n_obs]\n",
    "        mu, var = gp_posterior(X_train, y_train, X_test, length_scale=1.0, signal_var=1.0, noise_var=1e-4)\n",
    "\n",
    "    std = np.sqrt(var)\n",
    "    ax.fill_between(X_test.ravel(), mu - 2*std, mu + 2*std, alpha=0.2, color='steelblue', label='±2σ')\n",
    "    ax.plot(X_test, mu, color='steelblue', linewidth=2, label='posterior mean μₙ(x)')\n",
    "    ax.plot(X_test, true_fn(X_test.ravel()), 'k--', alpha=0.3, linewidth=1, label='true f(x)')\n",
    "\n",
    "    if n_obs > 0:\n",
    "        ax.scatter(X_train, y_train, c='black', s=50, zorder=5, label=f'{n_obs} observations')\n",
    "\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('f(x)')\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "\n",
    "fig.suptitle('GP Prior → Posterior: How Observations Collapse Uncertainty', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Effect of Observation Noise\n",
    "\n",
    "When $\\sigma_n^2 > 0$, the posterior doesn't interpolate the data exactly — it smooths through noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = [1e-4, 0.1, 0.5]\n",
    "noise_labels = ['Near-zero noise (σ²_n=0.0001)', 'Moderate noise (σ²_n=0.1)', 'High noise (σ²_n=0.5)']\n",
    "\n",
    "X_train = X_obs_all\n",
    "y_noisy = true_fn(X_train.ravel()) + 0.3 * np.random.randn(len(X_train))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "\n",
    "for ax, noise_var, label in zip(axes, noise_levels, noise_labels):\n",
    "    mu, var = gp_posterior(X_train, y_noisy, X_test, length_scale=1.0, signal_var=1.0, noise_var=noise_var)\n",
    "    std = np.sqrt(var)\n",
    "\n",
    "    ax.fill_between(X_test.ravel(), mu - 2*std, mu + 2*std, alpha=0.2, color='steelblue')\n",
    "    ax.plot(X_test, mu, color='steelblue', linewidth=2)\n",
    "    ax.scatter(X_train, y_noisy, c='black', s=50, zorder=5)\n",
    "    ax.plot(X_test, true_fn(X_test.ravel()), 'k--', alpha=0.3, linewidth=1)\n",
    "    ax.set_title(label, fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "axes[0].set_ylabel('f(x)')\n",
    "fig.suptitle('Effect of Noise Variance on GP Posterior', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Computational Cost: O(n³) Matrix Inversion\n",
    "\n",
    "The bottleneck is the Cholesky decomposition of the $n \\times n$ kernel matrix. Let's time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sizes = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "times = []\n",
    "\n",
    "for n in sizes:\n",
    "    X = np.random.randn(n, 1)\n",
    "    K = rbf_kernel(X, X) + 1e-6 * np.eye(n)\n",
    "    start = time.time()\n",
    "    np.linalg.cholesky(K)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.loglog(sizes, times, 'o-', color='steelblue', linewidth=2, markersize=8)\n",
    "ax.loglog(sizes, [times[0] * (n/sizes[0])**3 for n in sizes], '--', color='gray', alpha=0.5, label='O(n³) reference')\n",
    "ax.set_xlabel('Number of observations n')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Cholesky Decomposition Time — The O(n³) Bottleneck')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for n, t in zip(sizes, times):\n",
    "    print(f'  n={n:>5d}  →  {t:.4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next:** [Notebook 2 — Kernel Design & Function Priors](./02_kernels.ipynb) · [Notebook 3 — Acquisition Functions & Full BO Loop](./03_acquisition_functions.ipynb)\n",
    "\n",
    "**Back to article:** [Bayesian Optimisation — Mathematical Deep Dive](https://omkarray.com/bayesian-optimization.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}