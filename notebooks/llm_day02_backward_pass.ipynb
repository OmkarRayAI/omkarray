{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 — The Backward Pass & Chain Rule\n",
    "\n",
    "**Building LLMs from Scratch · Following Andrej Karpathy's micrograd**\n",
    "\n",
    "Today we implement **backpropagation**: computing gradients via the chain rule. The forward pass builds a computation graph; the backward pass propagates derivatives from output to every input. Each gradient answers: *\"How much of the final result is my fault?\"*\n",
    "\n",
    "**What you'll build:**\n",
    "- Extend the Value class with `_backward` closures for `+` and `×`\n",
    "- Manually run the backward pass step-by-step\n",
    "- Understand how addition *distributes* and multiplication *swaps* gradients\n",
    "- See the `+=` vs `=` trap when the same variable appears twice\n",
    "- Verify all gradients against PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Value Class with `_backward`\n",
    "\n",
    "We extend the Value class from Day 1 with `_backward` closures. Each operation stores a function that knows how to propagate the upstream gradient to its children.\n",
    "\n",
    "- **Addition:** $\\frac{\\partial(a+b)}{\\partial a} = 1$ → both inputs get the full gradient (distributes)\n",
    "- **Multiplication:** $\\frac{\\partial(a \\times b)}{\\partial a} = b$ → each input gets the *other's* value × gradient (swaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None  # default: no-op for leaf nodes\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad   # ∂(a+b)/∂a = 1 → pass through\n",
    "            other.grad += out.grad  # ∂(a+b)/∂b = 1 → pass through\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad   # ∂(a*b)/∂a = b\n",
    "            other.grad += self.data * out.grad   # ∂(a*b)/∂b = a\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "print(\"Value class with _backward for + and * defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual Backward Pass\n",
    "\n",
    "Build the graph $L = (a \\times b) + c$, then manually call `_backward()` in **reverse topological order** (output → inputs). We must call L's _backward first, then d's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "d = a * b   # d = -6.0\n",
    "L = d + c   # L = 4.0\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  a={a.data}, b={b.data}, c={c.data}\")\n",
    "print(f\"  d = a*b = {d.data}\")\n",
    "print(f\"  L = d+c = {L.data}\")\n",
    "print()\n",
    "\n",
    "# Manual backward: seed L.grad = 1.0, then call _backward in reverse order\n",
    "L.grad = 1.0\n",
    "L._backward()   # propagates to d and c\n",
    "d._backward()   # propagates to a and b\n",
    "\n",
    "print(\"Backward pass (manual _backward calls):\")\n",
    "print(f\"  L.grad = {L.grad}\")\n",
    "print(f\"  d.grad = {d.grad}\")\n",
    "print(f\"  c.grad = {c.grad}\")\n",
    "print(f\"  a.grad = {a.grad}\")\n",
    "print(f\"  b.grad = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Chain Rule in Action\n",
    "\n",
    "**Addition distributes:** Both children receive the full upstream gradient. If $L = d + c$, then $\\frac{\\partial L}{\\partial d} = \\frac{\\partial L}{\\partial c} = 1$.\n",
    "\n",
    "**Multiplication swaps:** Each child gets the *other's* value times the upstream gradient. If $d = a \\times b$, then $\\frac{\\partial d}{\\partial a} = b$ and $\\frac{\\partial d}{\\partial b} = a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chain rule intuition:\")\n",
    "print(\"  Addition:  ∂(a+b)/∂a = 1  →  grad passes through unchanged to both inputs\")\n",
    "print(\"  Addition:  ∂(a+b)/∂b = 1  →  same\")\n",
    "print(\"  Multiplication: ∂(a*b)/∂a = b  →  a gets (b's value) × upstream_grad\")\n",
    "print(\"  Multiplication: ∂(a*b)/∂b = a  →  b gets (a's value) × upstream_grad\")\n",
    "print()\n",
    "print(\"For L = (a*b) + c with a=2, b=-3, c=10:\")\n",
    "print(\"  L._backward() → d.grad += 1, c.grad += 1  (addition distributes)\")\n",
    "print(\"  d._backward() → a.grad += (-3)*1 = -3, b.grad += 2*1 = 2  (multiplication swaps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Accumulation — The `+=` vs `=` Trap\n",
    "\n",
    "When the same variable appears **twice** (e.g. $L = a + a$), its gradient is the **sum** of contributions from each use. Each path contributes +1, so $a.grad = 2.0$.\n",
    "\n",
    "Using `=` instead of `+=` would **overwrite** the first contribution — producing wrong gradients. Always use `+=` in _backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0)\n",
    "L = a + a   # L = 6.0, both inputs are the same node\n",
    "\n",
    "L.grad = 1.0\n",
    "L._backward()  # addition: both children get += 1.0 → a gets 1 + 1 = 2.0\n",
    "\n",
    "print(\"L = a + a  (same variable used twice)\")\n",
    "print(f\"  L.data = {L.data}\")\n",
    "print(f\"  a.grad = {a.grad}  ← should be 2.0 (both paths contribute +1)\")\n",
    "print()\n",
    "print(\"If we had used '=' instead of '+=' in _backward, a.grad would be 1.0 (WRONG).\")\n",
    "print(\"The += ensures gradients from multiple paths accumulate correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification with PyTorch\n",
    "\n",
    "Rebuild the same graph $L = (a \\times b) + c$ in PyTorch with `requires_grad=True`, call `.backward()`, and compare gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a_t = torch.tensor(2.0, requires_grad=True)\n",
    "b_t = torch.tensor(-3.0, requires_grad=True)\n",
    "c_t = torch.tensor(10.0, requires_grad=True)\n",
    "d_t = a_t * b_t\n",
    "L_t = d_t + c_t\n",
    "\n",
    "L_t.backward()\n",
    "\n",
    "print(\"PyTorch gradients:\")\n",
    "print(f\"  a.grad = {a_t.grad.item()}\")\n",
    "print(f\"  b.grad = {b_t.grad.item()}\")\n",
    "print(f\"  c.grad = {c_t.grad.item()}\")\n",
    "print()\n",
    "print(\"Our micrograd (from Section 2): a.grad=-3, b.grad=2, c.grad=1\")\n",
    "print(\"Match:\", a_t.grad.item() == -3.0 and b_t.grad.item() == 2.0 and c_t.grad.item() == 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complex Expression — $L = (a \\times b + c) \\times (a + b)$\n",
    "\n",
    "Compute gradients for a multi-branch expression. We need to reset gradients (or use fresh Values) since we're reusing the Value class. Then verify against PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grads(*values):\n",
    "    for v in values:\n",
    "        v.grad = 0.0\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "\n",
    "# L = (a*b + c) * (a + b)\n",
    "ab = a * b\n",
    "ab_plus_c = ab + c\n",
    "a_plus_b = a + b\n",
    "L = ab_plus_c * a_plus_b\n",
    "\n",
    "print(\"Forward: L = (a*b + c) * (a + b)\")\n",
    "print(f\"  ab = {ab.data}, ab+c = {ab_plus_c.data}, a+b = {a_plus_b.data}\")\n",
    "print(f\"  L = {L.data}\")\n",
    "print()\n",
    "\n",
    "# Manual backward: reverse topological order\n",
    "# Order: L → (ab_plus_c, a_plus_b) → (ab, c) and (a, b) → (a, b)\n",
    "L.grad = 1.0\n",
    "L._backward()       # → ab_plus_c, a_plus_b\n",
    "ab_plus_c._backward()  # → ab, c\n",
    "a_plus_b._backward()   # → a, b\n",
    "ab._backward()         # → a, b\n",
    "\n",
    "print(\"Our gradients:\")\n",
    "print(f\"  a.grad = {a.grad}\")\n",
    "print(f\"  b.grad = {b.grad}\")\n",
    "print(f\"  c.grad = {c.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch verification for L = (a*b + c) * (a + b)\n",
    "a_t = torch.tensor(2.0, requires_grad=True)\n",
    "b_t = torch.tensor(-3.0, requires_grad=True)\n",
    "c_t = torch.tensor(10.0, requires_grad=True)\n",
    "\n",
    "ab_t = a_t * b_t\n",
    "ab_plus_c_t = ab_t + c_t\n",
    "a_plus_b_t = a_t + b_t\n",
    "L_t = ab_plus_c_t * a_plus_b_t\n",
    "\n",
    "L_t.backward()\n",
    "\n",
    "print(\"PyTorch gradients:\")\n",
    "print(f\"  a.grad = {a_t.grad.item()}\")\n",
    "print(f\"  b.grad = {b_t.grad.item()}\")\n",
    "print(f\"  c.grad = {c_t.grad.item()}\")\n",
    "print()\n",
    "print(\"Match:\", abs(a.grad - a_t.grad.item()) < 1e-6 and abs(b.grad - b_t.grad.item()) < 1e-6 and abs(c.grad - c_t.grad.item()) < 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** · Day 2 of 80\n",
    "\n",
    "| [← Day 1: Forward Pass](llm_day01_forward_pass.ipynb) | [Day 2 article](https://omkarray.com/llm-day2.html) | [Day 3: Topological Sort →](llm_day03_topological_sort.ipynb) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
