{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 18: The Transformer Block — LayerNorm, FFN & Residual Connections\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "A **Transformer block** = Multi-Head Attention + Feed-Forward Network, held together by two key ingredients:\n",
    "\n",
    "1. **Residual connections**: `x = x + sublayer(x)` — gradients flow directly from output to input, allowing very deep networks to train\n",
    "2. **Layer Normalization**: stabilizes activations at each layer, replacing BatchNorm in sequence models\n",
    "\n",
    "The full block (\"Pre-LN\" variant used by GPT-2+):\n",
    "```\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```\n",
    "\n",
    "Stack $N$ of these blocks and you have a GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer Normalization vs Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Illustrate the difference\n",
    "B, T, C = 4, 6, 8  # batch, seq_len, channels\n",
    "x = torch.randn(B, T, C) * 3 + 1  # non-zero mean, non-unit std\n",
    "\n",
    "# BatchNorm: normalize across the BATCH dimension (per feature)\n",
    "# Problem: statistics depend on other items in the batch — bad for inference\n",
    "# LayerNorm: normalize across FEATURES (per token, per sample)\n",
    "# Statistics only depend on the current token — works at inference time\n",
    "\n",
    "layer_norm = nn.LayerNorm(C)\n",
    "x_ln = layer_norm(x)\n",
    "\n",
    "print(\"Input stats:\")\n",
    "print(f\"  mean: {x.mean():.3f}, std: {x.std():.3f}\")\n",
    "print(f\"\\nAfter LayerNorm (normalized over last {C} dims):\")\n",
    "print(f\"  mean per token: {x_ln[0, 0].mean():.6f}  (≈ 0)\")\n",
    "print(f\"  std  per token: {x_ln[0, 0].std():.6f}   (≈ 1)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Before\n",
    "axes[0].hist(x.flatten().detach().numpy(), bins=50, color='tomato', alpha=0.7)\n",
    "axes[0].set_title(f'Before LayerNorm\\nmean={x.mean():.2f}, std={x.std():.2f}')\n",
    "axes[0].set_xlabel('Activation value')\n",
    "\n",
    "# After\n",
    "axes[1].hist(x_ln.flatten().detach().numpy(), bins=50, color='steelblue', alpha=0.7)\n",
    "axes[1].set_title(f'After LayerNorm\\nmean≈0, std≈1')\n",
    "axes[1].set_xlabel('Activation value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why Residual Connections Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient flow with and without residuals\n",
    "# Without residuals: gradient must pass through every layer\n",
    "# With residuals: gradient has a highway directly back to early layers\n",
    "\n",
    "class DeepNetNoResidual(nn.Module):\n",
    "    def __init__(self, dim=32, depth=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[nn.Linear(dim, dim) for _ in range(depth)])\n",
    "    def forward(self, x): return self.layers(x)\n",
    "\n",
    "class DeepNetResidual(nn.Module):\n",
    "    def __init__(self, dim=32, depth=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(dim, dim) for _ in range(depth)])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = x + torch.tanh(layer(x))  # residual!\n",
    "        return x\n",
    "\n",
    "dim = 32\n",
    "x = torch.randn(4, dim, requires_grad=True)\n",
    "\n",
    "net_no_res = DeepNetNoResidual(dim)\n",
    "net_res    = DeepNetResidual(dim)\n",
    "\n",
    "# Forward + backward\n",
    "loss_no_res = net_no_res(x).sum()\n",
    "loss_no_res.backward()\n",
    "grad_no_res = x.grad.norm().item()\n",
    "\n",
    "x.grad = None\n",
    "loss_res = net_res(x).sum()\n",
    "loss_res.backward()\n",
    "grad_res = x.grad.norm().item()\n",
    "\n",
    "print(f\"Gradient norm (no residuals): {grad_no_res:.6f}\")\n",
    "print(f\"Gradient norm (with residuals): {grad_res:.6f}\")\n",
    "print(f\"\\nResiduals amplify gradient flow by {grad_res / max(grad_no_res, 1e-8):.1f}x\")\n",
    "print(\"Without residuals: gradients can vanish (or explode) through 10 layers\")\n",
    "print(\"With residuals: gradient has a direct shortcut to the input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feed-Forward Network (FFN)\n",
    "\n",
    "The FFN in a Transformer block:\n",
    "1. Expands dimension by 4x (classic ratio from \"Attention is All You Need\")\n",
    "2. Applies a nonlinearity (GELU in GPT-2, ReLU in original)\n",
    "3. Projects back to embed_dim\n",
    "\n",
    "This is where most of the model's \"memory\" lives — knowledge stored in FFN weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"FFN: expand 4x -> GELU -> contract. Applied position-wise.\"\"\"\n",
    "    def __init__(self, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "# GELU vs ReLU: GELU is smoother, better empirically for LLMs\n",
    "x_plot = torch.linspace(-3, 3, 200)\n",
    "gelu = F.gelu(x_plot)\n",
    "relu = F.relu(x_plot)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_plot.numpy(), relu.numpy(), label='ReLU', linestyle='--', color='tomato')\n",
    "plt.plot(x_plot.numpy(), gelu.numpy(), label='GELU', color='steelblue', linewidth=2)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('GELU vs ReLU\\nGELU is smooth near 0 — better gradient flow for small activations')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('activation(x)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Full Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim, head_size, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "        scores = q @ k.transpose(-2,-1) * self.head_size**-0.5\n",
    "        scores = scores.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        return self.dropout(F.softmax(scores, dim=-1)) @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_dim, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Transformer block (Pre-LN variant):\n",
    "      x = x + MHA(LayerNorm(x))\n",
    "      x = x + FFN(LayerNorm(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1  = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, block_size, dropout)\n",
    "        self.ln2  = nn.LayerNorm(embed_dim)\n",
    "        self.ff   = FeedForward(embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))  # attention residual\n",
    "        x = x + self.ff(self.ln2(x))    # FFN residual\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test\n",
    "embed_dim, num_heads, block_size = 64, 4, 32\n",
    "block = TransformerBlock(embed_dim, num_heads, block_size)\n",
    "x = torch.randn(2, 16, embed_dim)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Input:  {x.shape}\")\n",
    "print(f\"Output: {out.shape}  (shape preserved by residuals)\")\n",
    "print(f\"Parameters per block: {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stacking Blocks — Depth vs Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-scale parameter counts for different configs\n",
    "def count_params(n_layers, embed_dim, num_heads, block_size, vocab_size):\n",
    "    blocks = nn.Sequential(*[TransformerBlock(embed_dim, num_heads, block_size) for _ in range(n_layers)])\n",
    "    emb = nn.Embedding(vocab_size, embed_dim)\n",
    "    head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    total = sum(p.numel() for p in blocks.parameters())\n",
    "    total += sum(p.numel() for p in emb.parameters())\n",
    "    total += sum(p.numel() for p in head.parameters())\n",
    "    return total\n",
    "\n",
    "configs = [\n",
    "    (\"Baby GPT (today)\",    2,   64,  4,  32,  65),\n",
    "    (\"GPT-2 Small\",        12,  768, 12, 1024, 50257),\n",
    "    (\"GPT-2 Medium\",       24, 1024, 16, 1024, 50257),\n",
    "    (\"GPT-2 Large\",        36, 1280, 20, 1024, 50257),\n",
    "    (\"GPT-2 XL\",           48, 1600, 25, 1024, 50257),\n",
    "]\n",
    "\n",
    "print(f\"{'Config':<25} {'Layers':>8} {'dim':>6} {'Heads':>6} {'Params':>15}\")\n",
    "print(\"-\" * 65)\n",
    "for name, n_layers, dim, heads, bs, vocab in configs:\n",
    "    params = count_params(n_layers, dim, heads, min(bs, 64), vocab)\n",
    "    print(f\"{name:<25} {n_layers:>8} {dim:>6} {heads:>6} {params:>15,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Activation Statistics Through Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor how activations evolve through stacked blocks\n",
    "n_blocks = 4\n",
    "blocks = nn.Sequential(*[TransformerBlock(64, 4, 32) for _ in range(n_blocks)])\n",
    "\n",
    "x = torch.randn(4, 16, 64)\n",
    "stats = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    h = x\n",
    "    stats.append(('input', h.std().item(), h.mean().item()))\n",
    "    for i, block in enumerate(blocks):\n",
    "        h = block(h)\n",
    "        stats.append((f'block {i+1}', h.std().item(), h.mean().item()))\n",
    "\n",
    "names, stds, means = zip(*stats)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(stds, 'o-', color='steelblue')\n",
    "axes[0].set_xticks(range(len(names)))\n",
    "axes[0].set_xticklabels(names, rotation=30, ha='right')\n",
    "axes[0].set_ylabel('Standard deviation')\n",
    "axes[0].set_title('Activation Std Through Blocks\\n(residuals keep it stable)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(means, 'o-', color='tomato')\n",
    "axes[1].set_xticks(range(len(names)))\n",
    "axes[1].set_xticklabels(names, rotation=30, ha='right')\n",
    "axes[1].set_ylabel('Mean')\n",
    "axes[1].set_title('Activation Mean Through Blocks\\n(LayerNorm keeps mean ≈ 0)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 18: Transformer Block](https://omkarray.com/llm-day18.html) | [← Prev](llm_day17_multihead_attention.ipynb) | [Next →](llm_day19_gpt.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
