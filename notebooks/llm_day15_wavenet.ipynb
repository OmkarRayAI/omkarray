{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 15: WaveNet — Hierarchical Language Models with Dilated Convolutions\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Our MLP language model from Day 9 concatenated a fixed context window of embeddings and fed them into a single linear layer. WaveNet (DeepMind, 2016) introduced a smarter architecture: **hierarchically fuse** adjacent pairs of tokens through multiple layers, building up a richer representation at each level.\n",
    "\n",
    "Instead of flattening the full context window at once, we progressively merge:\n",
    "- Layer 1: fuse pairs (token $i$, token $i+1$) → half as many features\n",
    "- Layer 2: fuse pairs of the fused features → quarter as many\n",
    "- ...\n",
    "- Final layer: single rich representation → logits\n",
    "\n",
    "This is a **dilated causal convolution** pattern — each layer has access to an exponentially growing receptive field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup — Names Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Build vocabulary from names dataset (or fallback to synthetic)\n",
    "try:\n",
    "    with open('../names.txt') as f:\n",
    "        words = f.read().splitlines()\n",
    "except FileNotFoundError:\n",
    "    # Synthetic fallback: 1000 random short 'names'\n",
    "    import string\n",
    "    words = [''.join(random.choices(string.ascii_lowercase, k=random.randint(3,8))) for _ in range(1000)]\n",
    "\n",
    "chars = sorted(set(''.join(words)))\n",
    "vocab_size = len(chars) + 1  # +1 for '.' pad token\n",
    "stoi = {c: i+1 for i, c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "print(f\"Words: {len(words)}, Vocab: {vocab_size}\")\n",
    "print(f\"Sample: {words[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Dataset — Context Window of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # WaveNet works better with larger context\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(words)\n",
    "n1, n2 = int(0.8 * len(words)), int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "print(f\"Train: {Xtr.shape}, Dev: {Xdev.shape}, Test: {Xte.shape}\")\n",
    "print(f\"Input shape: (batch, block_size={block_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WaveNet Architecture\n",
    "\n",
    "We define a hierarchy of **FlattenConsecutive + Linear** layers.\n",
    "\n",
    "Each `FlattenConsecutive(n)` takes `(B, T, C)` and reshapes to `(B, T//n, n*C)` — merging `n` adjacent tokens into one. After a linear layer, each position now captures `n` tokens worth of context. Stack this and you get a tree-like fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive(nn.Module):\n",
    "    \"\"\"Reshape (B, T, C) -> (B, T//n, n*C) by merging n adjacent timesteps.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T // self.n, C * self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)  # collapse time dim when T=1\n",
    "        return x\n",
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=24, hidden=128, block_size=8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # 3 levels: 8 tokens -> 4 -> 2 -> 1\n",
    "        # Each level: flatten 2 adjacent -> linear -> batchnorm -> tanh\n",
    "        self.layers = nn.Sequential(\n",
    "            FlattenConsecutive(2), nn.Linear(embed_dim * 2, hidden, bias=False),\n",
    "            nn.BatchNorm1d(hidden), nn.Tanh(),\n",
    "\n",
    "            FlattenConsecutive(2), nn.Linear(hidden * 2, hidden, bias=False),\n",
    "            nn.BatchNorm1d(hidden), nn.Tanh(),\n",
    "\n",
    "            FlattenConsecutive(2), nn.Linear(hidden * 2, hidden, bias=False),\n",
    "            nn.BatchNorm1d(hidden), nn.Tanh(),\n",
    "\n",
    "            nn.Linear(hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, block_size) int\n",
    "        x = self.embed(x)  # (B, 8, embed_dim)\n",
    "        x = self.layers(x) # (B, vocab_size)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = WaveNet(vocab_size=vocab_size, embed_dim=24, hidden=128, block_size=8)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward\n",
    "x_test = Xtr[:4]\n",
    "logits = model(x_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.3)\n",
    "\n",
    "losses = []\n",
    "for step in range(15000):\n",
    "    # Minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        losses.append(loss.item())\n",
    "        print(f\"step {step:5d} | loss {loss.item():.4f}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step (x1000)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('WaveNet Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate & Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, X, Y):\n",
    "    model.eval()\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "print(f\"Train loss: {evaluate(model, Xtr, Ytr):.4f}\")\n",
    "print(f\"Dev loss:   {evaluate(model, Xdev, Ydev):.4f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, n=10):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for _ in range(n):\n",
    "        context = [0] * block_size\n",
    "        name = []\n",
    "        while True:\n",
    "            x = torch.tensor([context])\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            ix = torch.multinomial(probs, 1).item()\n",
    "            if ix == 0:\n",
    "                break\n",
    "            name.append(itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "        out.append(''.join(name))\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "print(\"\\nSampled names:\")\n",
    "for name in sample(model, 15):\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why Hierarchical Fusion Works\n",
    "\n",
    "Compare the receptive field of our flat MLP vs WaveNet:\n",
    "\n",
    "| Architecture | Layer | Tokens in context |\n",
    "|---|---|---|\n",
    "| Flat MLP | 1 layer | 8 (all at once, flat) |\n",
    "| WaveNet | Layer 1 | 2 |\n",
    "| WaveNet | Layer 2 | 4 |\n",
    "| WaveNet | Layer 3 | 8 |\n",
    "\n",
    "WaveNet learns **local structure first** (bigrams), then **medium-range** (4-grams), then **full context** (8-grams). This inductive bias — short-range before long-range — matches how language actually works: adjacent characters form syllables, syllables form words, words form phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hierarchical fusion\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Positions of 8 input tokens\n",
    "n_tokens = 8\n",
    "levels = 4  # input + 3 layers\n",
    "colors = ['#4C72B0', '#DD8452', '#55A868', '#C44E52']\n",
    "\n",
    "for level in range(levels):\n",
    "    n = n_tokens // (2 ** level)\n",
    "    stride = 2 ** level\n",
    "    for i in range(n):\n",
    "        x = i * stride + stride / 2 - 0.5\n",
    "        ax.scatter(x, level, s=200, color=colors[level], zorder=3)\n",
    "        if level > 0:\n",
    "            # Draw lines to children\n",
    "            child_l = (i * 2) * (stride // 2) + (stride // 2) / 2 - 0.5\n",
    "            child_r = (i * 2 + 1) * (stride // 2) + (stride // 2) / 2 - 0.5\n",
    "            ax.plot([x, child_l], [level, level - 1], 'k-', alpha=0.4)\n",
    "            ax.plot([x, child_r], [level, level - 1], 'k-', alpha=0.4)\n",
    "\n",
    "ax.set_yticks(range(levels))\n",
    "ax.set_yticklabels(['Input (8 tokens)', 'Layer 1 (4 nodes)', 'Layer 2 (2 nodes)', 'Layer 3 (1 node)'])\n",
    "ax.set_xticks([])\n",
    "ax.set_title('WaveNet Hierarchical Fusion — Each Node Fuses 2 Children')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 15: WaveNet](https://omkarray.com/llm-day15.html) | [← Prev](llm_day14_cross_entropy.ipynb) | [Next →](llm_day16_self_attention.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
