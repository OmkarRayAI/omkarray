{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 17: Multi-Head Attention & Positional Encoding\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "A single attention head can only learn **one type of relationship** between tokens — e.g., \"what comes before me syntactically\". Real language has many simultaneous relationships: grammatical agreement, semantic similarity, coreference, positional proximity.\n",
    "\n",
    "**Multi-head attention** runs $h$ attention heads in **parallel**, each learning different relationship patterns, then concatenates their outputs:\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$.\n",
    "\n",
    "**Positional encoding** solves the fact that attention is **permutation-invariant** — without it, \"dog bites man\" and \"man bites dog\" would produce identical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single attention head.\"\"\"\n",
    "    def __init__(self, embed_dim, head_size, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        scores = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n",
    "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = self.dropout(F.softmax(scores, dim=-1))\n",
    "        return weights @ v, weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"h parallel attention heads, then project back to embed_dim.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(embed_dim, head_size, block_size, dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)   # output projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run all heads in parallel\n",
    "        head_outs, all_weights = zip(*[h(x) for h in self.heads])\n",
    "        out = torch.cat(head_outs, dim=-1)  # (B, T, embed_dim)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out, list(all_weights)\n",
    "\n",
    "\n",
    "# Test\n",
    "embed_dim, num_heads, block_size = 32, 4, 16\n",
    "mha = MultiHeadAttention(embed_dim, num_heads, block_size)\n",
    "x = torch.randn(2, 8, embed_dim)\n",
    "out, weights = mha(x)\n",
    "print(f\"Input:   {x.shape}\")\n",
    "print(f\"Output:  {out.shape}  (same as input — embed_dim preserved)\")\n",
    "print(f\"Each head size: {embed_dim // num_heads}  ({num_heads} heads x {embed_dim // num_heads} = {embed_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What Different Heads Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 4 heads' attention patterns on the same input\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (w, ax) in enumerate(zip(weights, axes)):\n",
    "    im = ax.imshow(w[0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key position')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Query position')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Multi-Head Attention — Each Head Learns Different Patterns', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Each head attends to different positions — this is the power of multi-head attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding — Why We Need It\n",
    "\n",
    "Self-attention treats the input as a **set**, not a sequence — \"dog bites man\" = \"man bites dog\" without positional info. We add a positional signal to each token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate permutation-invariance problem\n",
    "embed_dim = 8\n",
    "vocab_size = 5\n",
    "emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# \"dog bites man\" = tokens [1, 2, 3]\n",
    "# \"man bites dog\" = tokens [3, 2, 1]\n",
    "seq1 = torch.tensor([[1, 2, 3]])  # dog bites man\n",
    "seq2 = torch.tensor([[3, 2, 1]])  # man bites dog\n",
    "\n",
    "e1 = emb(seq1)  # (1, 3, 8)\n",
    "e2 = emb(seq2)  # (1, 3, 8)\n",
    "\n",
    "# Sum of embeddings is same regardless of order\n",
    "print(f\"Sum of embeddings (dog bites man): {e1.sum(dim=1)[0].tolist()}\")\n",
    "print(f\"Sum of embeddings (man bites dog): {e2.sum(dim=1)[0].tolist()}\")\n",
    "print(f\"Same? {torch.allclose(e1.sum(1), e2.sum(1))}  <- problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learned vs Sinusoidal Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Learned positional embedding (GPT-style)\n",
    "class LearnedPosEncoding(nn.Module):\n",
    "    def __init__(self, block_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        pos = torch.arange(T, device=x.device)  # [0, 1, ..., T-1]\n",
    "        return x + self.pos_emb(pos)             # broadcast over batch\n",
    "\n",
    "\n",
    "# Option B: Sinusoidal positional encoding (original Transformer)\n",
    "def sinusoidal_pos_encoding(T, embed_dim):\n",
    "    \"\"\"Fixed sinusoidal encoding: PE[pos, 2i] = sin(pos/10000^(2i/d))\"\"\"\n",
    "    pe = torch.zeros(T, embed_dim)\n",
    "    pos = torch.arange(T).unsqueeze(1).float()           # (T, 1)\n",
    "    div = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000) / embed_dim))\n",
    "    pe[:, 0::2] = torch.sin(pos * div)\n",
    "    pe[:, 1::2] = torch.cos(pos * div[:embed_dim//2])\n",
    "    return pe\n",
    "\n",
    "\n",
    "# Visualize sinusoidal encoding\n",
    "T, d = 50, 64\n",
    "pe = sinusoidal_pos_encoding(T, d)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im = axes[0].imshow(pe.numpy().T, aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "axes[0].set_title('Sinusoidal Positional Encoding\\n(each row = one dimension, alternating sin/cos)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Show a few dimensions\n",
    "for dim in [0, 1, 4, 10, 20]:\n",
    "    axes[1].plot(pe[:, dim].numpy(), label=f'dim {dim}')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Encoding value')\n",
    "axes[1].set_title('Sinusoidal Encoding per Dimension\\n(low dims = high freq, high dims = low freq)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Positional Encoding Makes Order Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With positional encoding, dog bites man ≠ man bites dog\n",
    "T = 3\n",
    "pos_enc = sinusoidal_pos_encoding(T, embed_dim)  # (3, 8)\n",
    "\n",
    "e1_pos = e1 + pos_enc  # dog(pos0) + bites(pos1) + man(pos2)\n",
    "e2_pos = e2 + pos_enc  # man(pos0) + bites(pos1) + dog(pos2)\n",
    "\n",
    "print(f\"With positional encoding:\")\n",
    "print(f\"Sum (dog bites man): {e1_pos.sum(dim=1)[0].tolist()}\")\n",
    "print(f\"Sum (man bites dog): {e2_pos.sum(dim=1)[0].tolist()}\")\n",
    "print(f\"Same? {torch.allclose(e1_pos.sum(1), e2_pos.sum(1))}  <- fixed!\")\n",
    "\n",
    "# Similarity between positions: positions close together should be similar\n",
    "pe = sinusoidal_pos_encoding(20, 64)\n",
    "sim = pe @ pe.T  # cosine-like similarity\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.imshow(sim.numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Positional Encoding Similarity\\n(nearby positions are more similar)')\n",
    "plt.xlabel('Position j')\n",
    "plt.ylabel('Position i')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Efficient Multi-Head Attention (Batched)\n",
    "\n",
    "The parallel head implementation above runs each head sequentially in Python. The standard efficient approach reshapes Q, K, V to run all heads **in one batched matrix multiply**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Efficient MHA: all heads computed in a single batched matmul.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embed_dim // num_heads\n",
    "\n",
    "        self.qkv  = nn.Linear(embed_dim, 3 * embed_dim, bias=False)  # Q, K, V in one shot\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        H, hs = self.num_heads, self.head_size\n",
    "\n",
    "        # Project to Q, K, V all at once\n",
    "        qkv = self.qkv(x)                              # (B, T, 3*C)\n",
    "        q, k, v = qkv.split(self.embed_dim, dim=2)     # each (B, T, C)\n",
    "\n",
    "        # Reshape to (B, H, T, head_size)\n",
    "        q = q.view(B, T, H, hs).transpose(1, 2)\n",
    "        k = k.view(B, T, H, hs).transpose(1, 2)\n",
    "        v = v.view(B, T, H, hs).transpose(1, 2)\n",
    "\n",
    "        # Attention scores: (B, H, T, T)\n",
    "        scores = q @ k.transpose(-2, -1) * (hs ** -0.5)\n",
    "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = self.dropout(F.softmax(scores, dim=-1))\n",
    "\n",
    "        # Aggregate: (B, H, T, hs) -> (B, T, C)\n",
    "        out = (weights @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "# Verify same output as naive version (with same weights)\n",
    "emha = EfficientMultiHeadAttention(embed_dim=32, num_heads=4, block_size=16)\n",
    "x = torch.randn(2, 8, 32)\n",
    "out = emha(x)\n",
    "print(f\"Efficient MHA output: {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in emha.parameters()):,}\")\n",
    "\n",
    "# Speed comparison\n",
    "import time\n",
    "x_large = torch.randn(32, 64, 128)\n",
    "emha_large = EfficientMultiHeadAttention(128, 8, 64)\n",
    "mha_large  = MultiHeadAttention(128, 8, 64)\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(100): emha_large(x_large)\n",
    "t1 = time.time()\n",
    "for _ in range(100): mha_large(x_large)\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"\\nEfficient (batched): {(t1-t0)*1000:.1f} ms for 100 forward passes\")\n",
    "print(f\"Naive (loop):        {(t2-t1)*1000:.1f} ms for 100 forward passes\")\n",
    "print(f\"Speedup: {(t2-t1)/(t1-t0):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** — [Day 17: Multi-Head Attention](https://omkarray.com/llm-day17.html) | [← Prev](llm_day16_self_attention.ipynb) | [Next →](llm_day18_transformer_block.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
