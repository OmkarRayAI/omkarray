{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Sort & Full Autograd\n",
    "\n",
    "**Building LLMs from Scratch · Day 3** (following Andrej Karpathy's micrograd)\n",
    "\n",
    "So far we've built a `Value` class that can compute gradients via the chain rule. But we had to call `_backward()` on each node **manually** in the correct order—one mistake and gradients are wrong.\n",
    "\n",
    "Today we automate this: we'll use **topological sort** to determine the correct backward order automatically, then call `backward()` once on the loss and let the engine propagate gradients through the entire computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "**Automating backpropagation with topological sort**\n",
    "\n",
    "The computation graph is a directed acyclic graph (DAG): each `Value` is a node, and edges go from inputs to outputs (e.g. `a` → `a*b`). To backpropagate correctly, we must call `_backward()` on a node only after we've called it on all nodes that depend on it (i.e. its children in the graph).\n",
    "\n",
    "A **topological ordering** of a DAG visits nodes in an order such that for every edge `u → v`, `u` is visited before `v`. So in the *forward* direction: parents before children.\n",
    "\n",
    "For **backward**, we need the reverse: parents after children. So we do a DFS post-order traversal (visit children first, then parent), collect nodes, then reverse the list. That gives us the correct order to call `_backward()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem with Manual Backward\n",
    "\n",
    "With manual backward, we must call `_backward()` in the right order by hand. One wrong order and gradients are wrong.\n",
    "\n",
    "Example: `L = (a*b + c) * (a + b)`. The correct order depends on the graph structure—easy to get wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragile manual approach: must call _backward in the right order\n",
    "def manual_backward_example():\n",
    "    \"\"\"Simplified Value class for manual backward demo.\"\"\"\n",
    "    class Value:\n",
    "        def __init__(self, data, _prev=()):\n",
    "            self.data = data\n",
    "            self.grad = 0.0\n",
    "            self._prev = set(_prev)\n",
    "            self._backward = lambda: None\n",
    "\n",
    "    a, b, c = Value(2.0), Value(3.0), Value(1.0)\n",
    "    ab = Value(a.data * b.data, (a, b))\n",
    "    ab._backward = lambda: (\n",
    "        setattr(a, 'grad', a.grad + b.data * ab.grad),\n",
    "        setattr(b, 'grad', b.grad + a.data * ab.grad)\n",
    "    )\n",
    "    abc = Value(ab.data + c.data, (ab, c))\n",
    "    abc._backward = lambda: (\n",
    "        setattr(ab, 'grad', ab.grad + abc.grad),\n",
    "        setattr(c, 'grad', c.grad + abc.grad)\n",
    "    )\n",
    "    apb = Value(a.data + b.data, (a, b))\n",
    "    apb._backward = lambda: (\n",
    "        setattr(a, 'grad', a.grad + apb.grad),\n",
    "        setattr(b, 'grad', b.grad + apb.grad)\n",
    "    )\n",
    "    L = Value(abc.data * apb.data, (abc, apb))\n",
    "    L._backward = lambda: (\n",
    "        setattr(abc, 'grad', abc.grad + apb.data * L.grad),\n",
    "        setattr(apb, 'grad', apb.grad + abc.data * L.grad)\n",
    "    )\n",
    "\n",
    "    L.grad = 1.0\n",
    "    # Manual order: must call L before abc, apb; abc before ab, c; apb before a, b; ab before a, b\n",
    "    L._backward()\n",
    "    abc._backward()\n",
    "    apb._backward()\n",
    "    ab._backward()\n",
    "    # c, a, b are leaves—no _backward\n",
    "\n",
    "    print(\"Manual backward:\", a.grad, b.grad, c.grad)\n",
    "\n",
    "manual_backward_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topological Sort via DFS\n",
    "\n",
    "**Implement `build_topo` with DFS post-order traversal**\n",
    "\n",
    "We traverse the graph in DFS order. For each node, we first recurse into its children (`_prev`), then append the node. This gives us a **post-order** traversal: children before parents.\n",
    "\n",
    "Post-order yields [children..., parents...]. For backward, we must process a node only *after* all nodes that depend on it (its children in the graph). So we need children before parents—exactly what post-order gives. We iterate in **reversed post-order** (root first, then its dependencies) and call `_backward()` on each node; by the time we reach a node, all nodes that depend on it have already propagated their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topo(v, visited, topo):\n",
    "    \"\"\"\n",
    "    DFS post-order traversal of the computation graph.\n",
    "    Appends nodes to topo so that children appear before parents.\n",
    "    Reversed topo = parents before children = correct order for backward.\n",
    "    \"\"\"\n",
    "    if v in visited:\n",
    "        return\n",
    "    visited.add(v)\n",
    "    for child in v._prev:\n",
    "        build_topo(child, visited, topo)\n",
    "    topo.append(v)\n",
    "\n",
    "\n",
    "# Quick test: build a simple graph\n",
    "class SimpleValue:\n",
    "    def __init__(self, data, _prev=()):\n",
    "        self.data = data\n",
    "        self._prev = set(_prev)\n",
    "\n",
    "a, b = SimpleValue(1.0), SimpleValue(2.0)\n",
    "c = SimpleValue(a.data + b.data, (a, b))\n",
    "d = SimpleValue(c.data * 2, (c,))\n",
    "\n",
    "topo = []\n",
    "build_topo(d, set(), topo)\n",
    "print(\"Post-order (children before parents):\", [id(x) for x in topo])\n",
    "print(\"Reversed (for backward):\", [id(x) for x in reversed(topo)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Complete Value Class\n",
    "\n",
    "Full implementation with:\n",
    "- `__init__` (data, grad, _backward, _prev, _op)\n",
    "- `__add__` with _backward closure (grad += out.grad)\n",
    "- `__mul__` with _backward closure (grad += other.data * out.grad)\n",
    "- `__repr__`\n",
    "- `backward()` method using topological sort\n",
    "- Handle `isinstance` check for int/float operands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"Scalar value with automatic differentiation via topological sort.\"\"\"\n",
    "\n",
    "    def __init__(self, data, _prev=(), _op=\"\"):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_prev)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v in visited:\n",
    "                return\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "                build_topo(child)\n",
    "            topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Engine\n",
    "\n",
    "Build expression `L = (a*b + c) * (a + b)`, call `L.backward()`, print all gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = Value(1.0)\n",
    "\n",
    "L = (a * b + c) * (a + b)\n",
    "L.backward()\n",
    "\n",
    "print(\"L =\", L.data)\n",
    "print(\"dL/da =\", a.grad)\n",
    "print(\"dL/db =\", b.grad)\n",
    "print(\"dL/dc =\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Manual vs Automated\n",
    "\n",
    "Side by side: old manual 5-line backward vs new single `L.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Manual (fragile): ===\")\n",
    "a1, b1, c1 = Value(2.0), Value(3.0), Value(1.0)\n",
    "L1 = (a1 * b1 + c1) * (a1 + b1)\n",
    "L1.grad = 1.0\n",
    "# Must call in exact order:\n",
    "L1._backward()\n",
    "for child in L1._prev:\n",
    "    child._backward()\n",
    "for child in L1._prev:\n",
    "    for grandchild in child._prev:\n",
    "        grandchild._backward()\n",
    "# ... gets messy quickly for nested graphs\n",
    "print(\"Manual dL/da:\", a1.grad)\n",
    "\n",
    "print(\"\\n=== Automated (one line): ===\")\n",
    "a2, b2, c2 = Value(2.0), Value(3.0), Value(1.0)\n",
    "L2 = (a2 * b2 + c2) * (a2 + b2)\n",
    "L2.backward()\n",
    "print(\"Automated dL/da:\", a2.grad)\n",
    "print(\"\\nSame result:\", abs(a1.grad - a2.grad) < 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Zero-Gradient Trap\n",
    "\n",
    "Calling `backward()` twice accumulates gradients. Gradients are added with `+=`, so without zeroing, the second run adds to the first. Fix: zero grads before each backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(v, visited=None):\n",
    "    \"\"\"Zero all gradients in the graph.\"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if v in visited:\n",
    "        return\n",
    "    visited.add(v)\n",
    "    v.grad = 0.0\n",
    "    for child in v._prev:\n",
    "        zero_grad(child, visited)\n",
    "\n",
    "\n",
    "a, b, c = Value(2.0), Value(3.0), Value(1.0)\n",
    "L = (a * b + c) * (a + b)\n",
    "\n",
    "print(\"\\n--- First backward ---\")\n",
    "L.backward()\n",
    "print(\"a.grad:\", a.grad)\n",
    "\n",
    "print(\"\\n--- Second backward (without zeroing) ---\")\n",
    "L.backward()  # Wrong! Gradients accumulate\n",
    "print(\"a.grad (doubled!):\", a.grad)\n",
    "\n",
    "print(\"\\n--- Second backward (with zero_grad first) ---\")\n",
    "zero_grad(L)\n",
    "L.backward()\n",
    "print(\"a.grad (correct):\", a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verification with PyTorch\n",
    "\n",
    "Same expression with torch tensors, compare gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cde55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    HAS_TORCH = True\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "if HAS_TORCH:\n",
    "    a = torch.tensor(2.0, requires_grad=True)\n",
    "    b = torch.tensor(3.0, requires_grad=True)\n",
    "    c = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "    L = (a * b + c) * (a + b)\n",
    "    L.backward()\n",
    "\n",
    "    print(\"PyTorch:\")\n",
    "    print(\"  dL/da =\", a.grad.item())\n",
    "    print(\"  dL/db =\", b.grad.item())\n",
    "    print(\"  dL/dc =\", c.grad.item())\n",
    "    print(\"\\nOur micrograd:\")\n",
    "    a2, b2, c2 = Value(2.0), Value(3.0), Value(1.0)\n",
    "    L2 = (a2 * b2 + c2) * (a2 + b2)\n",
    "    L2.backward()\n",
    "    print(\"  dL/da =\", a2.grad)\n",
    "    print(\"  dL/db =\", b2.grad)\n",
    "    print(\"  dL/dc =\", c2.grad)\n",
    "    print(\"\\nMatch:\", torch.allclose(torch.tensor([a.grad, b.grad, c.grad]), torch.tensor([a2.grad, b2.grad, c2.grad])))\n",
    "else:\n",
    "    print(\"PyTorch not installed. Run: pip install torch\")\n",
    "    print(\"Our micrograd:\")\n",
    "    a2, b2, c2 = Value(2.0), Value(3.0), Value(1.0)\n",
    "    L2 = (a2 * b2 + c2) * (a2 + b2)\n",
    "    L2.backward()\n",
    "    print(\"  dL/da =\", a2.grad, \"dL/db =\", b2.grad, \"dL/dc =\", c2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Building LLMs from Scratch** · [Day 3: Topological Sort & Autograd](https://omkarray.com/llm-day3.html)\n",
    "\n",
    "← Prev: [Day 2](https://omkarray.com/llm-day2.html) · Next: [Day 4](https://omkarray.com/llm-day4.html) →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
