{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 11: Activations & Gradients — The Fragility of Deep Nets\n",
        "\n",
        "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Why are deep networks fragile? As we stack more layers, two problems emerge:\n",
        "\n",
        "- **Vanishing gradients**: Gradients shrink exponentially as they flow backward through many layers. Early layers receive almost no signal and barely update.\n",
        "- **Exploding gradients**: Sometimes gradients grow exponentially instead, causing numerical instability and NaNs.\n",
        "- **Dead neurons**: With activations like tanh, neurons can \"saturate\" (outputs near ±1). In the saturated region, the gradient is ≈ 0, so the neuron stops learning.\n",
        "\n",
        "Today we build a deeper MLP, inspect activation statistics, and fix the problem with **proper weight initialization** (Kaiming)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset & Model Setup\n",
        "\n",
        "Same names dataset from Day 6. Build a deeper MLP: embedding C(27, 10), then 3 hidden layers of size 100 with tanh activations, output layer to 27 classes. All parameters use `requires_grad=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'mia', 'charlotte', 'amelia', 'harper', 'evelyn',\n",
        "         'abigail', 'emily', 'ella', 'elizabeth', 'camila', 'luna', 'sofia', 'avery', 'mila', 'aria']\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {'.': 0, **{c: i + 1 for i, c in enumerate(chars)}}\n",
        "itos = {i: c for c, i in stoi.items()}\n",
        "\n",
        "# Build bigram pairs\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for c1, c2 in zip(chs[:-1], chs[1:]):\n",
        "        xs.append(stoi[c1])\n",
        "        ys.append(stoi[c2])\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "print(f\"Dataset: {len(words)} names, {len(xs)} bigram pairs\")\n",
        "print(f\"Vocabulary: {len(stoi)} chars\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_mlp(use_kaiming=False):\n",
        "    \"\"\"Build MLP: embed(27,10) -> 3x hidden(100, tanh) -> out(27).\"\"\"\n",
        "    C = torch.randn(27, 10, requires_grad=True)\n",
        "    W1 = torch.randn(10, 100, requires_grad=True)\n",
        "    b1 = torch.randn(100, requires_grad=True)\n",
        "    W2 = torch.randn(100, 100, requires_grad=True)\n",
        "    b2 = torch.randn(100, requires_grad=True)\n",
        "    W3 = torch.randn(100, 100, requires_grad=True)\n",
        "    b3 = torch.randn(100, requires_grad=True)\n",
        "    W4 = torch.randn(100, 27, requires_grad=True)\n",
        "    b4 = torch.randn(27, requires_grad=True)\n",
        "\n",
        "    if use_kaiming:\n",
        "        # Kaiming init: W *= sqrt(2/fan_in) for tanh\n",
        "        C.data *= (2.0 / 27) ** 0.5\n",
        "        W1.data *= (2.0 / 10) ** 0.5\n",
        "        W2.data *= (2.0 / 100) ** 0.5\n",
        "        W3.data *= (2.0 / 100) ** 0.5\n",
        "        W4.data *= (2.0 / 100) ** 0.5\n",
        "        # Biases: keep small (often 0)\n",
        "        b1.data.zero_()\n",
        "        b2.data.zero_()\n",
        "        b3.data.zero_()\n",
        "        b4.data.zero_()\n",
        "\n",
        "    return {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4}\n",
        "\n",
        "\n",
        "def forward(params, xs, return_activations=False):\n",
        "    \"\"\"Forward pass. Returns logits and optionally list of pre-activations per hidden layer.\"\"\"\n",
        "    emb = params['C'][xs]  # (N, 10)\n",
        "    h1_pre = emb @ params['W1'] + params['b1']\n",
        "    h1 = torch.tanh(h1_pre)\n",
        "    h2_pre = h1 @ params['W2'] + params['b2']\n",
        "    h2 = torch.tanh(h2_pre)\n",
        "    h3_pre = h2 @ params['W3'] + params['b3']\n",
        "    h3 = torch.tanh(h3_pre)\n",
        "    logits = h3 @ params['W4'] + params['b4']\n",
        "\n",
        "    if return_activations:\n",
        "        return logits, [h1, h2, h3]\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The Saturation Problem\n",
        "\n",
        "Forward pass with **random init**. For each hidden layer, print mean, std, and fraction of activations > 0.99 or < -0.99 (saturated tanh). With bad init, most activations are saturated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(42)\n",
        "params_bad = build_mlp(use_kaiming=False)\n",
        "logits, activations = forward(params_bad, xs, return_activations=True)\n",
        "\n",
        "print(\"Activation statistics (random init — BAD):\")\n",
        "print(\"-\" * 55)\n",
        "for i, h in enumerate(activations):\n",
        "    mean = h.mean().item()\n",
        "    std = h.std().item()\n",
        "    saturated = ((h > 0.99) | (h < -0.99)).float().mean().item()\n",
        "    print(f\"Layer {i+1}: mean={mean:+.4f}, std={std:.4f}, saturated (|x|>0.99): {saturated*100:.1f}%\")\n",
        "\n",
        "print(\"\\n→ Most activations are saturated! tanh gradient ≈ 0 in saturated region.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizing Activation Distributions\n",
        "\n",
        "Plot histograms of activations for each layer — before (bad init) and after (Kaiming) fix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_activation_histograms(params, title, axs):\n",
        "    _, activations = forward(params, xs, return_activations=True)\n",
        "    for i, h in enumerate(activations):\n",
        "        axs[i].hist(h.detach().flatten().numpy(), bins=50, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "        axs[i].set_title(f\"Layer {i+1}\")\n",
        "        axs[i].set_xlabel(\"Activation value\")\n",
        "        axs[i].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
        "        axs[i].axvline(0.99, color='orange', linestyle=':', alpha=0.7)\n",
        "        axs[i].axvline(-0.99, color='orange', linestyle=':', alpha=0.7)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "plot_activation_histograms(params_bad, \"Random init (bad)\", axs)\n",
        "fig.suptitle(\"Activation distributions — Random init (saturated)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(42)\n",
        "params_kaiming = build_mlp(use_kaiming=True)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "plot_activation_histograms(params_kaiming, \"Kaiming init (good)\", axs)\n",
        "fig.suptitle(\"Activation distributions — Kaiming init (well-distributed)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Kaiming Initialization\n",
        "\n",
        "Kaiming init: `W *= (2.0 / fan_in)**0.5` for tanh. This keeps the variance of activations roughly constant across layers. Show that activations are now well-distributed (std ≈ 0.6–0.7 across layers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Activation statistics (Kaiming init — GOOD):\")\n",
        "print(\"-\" * 55)\n",
        "_, activations = forward(params_kaiming, xs, return_activations=True)\n",
        "for i, h in enumerate(activations):\n",
        "    mean = h.mean().item()\n",
        "    std = h.std().item()\n",
        "    saturated = ((h > 0.99) | (h < -0.99)).float().mean().item()\n",
        "    print(f\"Layer {i+1}: mean={mean:+.4f}, std={std:.4f}, saturated (|x|>0.99): {saturated*100:.1f}%\")\n",
        "\n",
        "print(\"\\n→ std ≈ 0.6–0.7 across layers. Activations are well-distributed, gradients can flow.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Gradient Flow\n",
        "\n",
        "After backward pass, plot the mean absolute gradient for each layer's weights. Compare: vanishing gradients with bad init vs healthy gradients with Kaiming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_grad_stats(params):\n",
        "    \"\"\"Run forward+backward, return mean |grad| per weight layer.\"\"\"\n",
        "    logits = forward(params, xs)\n",
        "    loss = F.cross_entropy(logits, ys)\n",
        "    for p in params.values():\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    grad_stats = []\n",
        "    for name in ['W1', 'W2', 'W3', 'W4']:\n",
        "        g = params[name].grad\n",
        "        grad_stats.append(g.abs().mean().item())\n",
        "    return grad_stats\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "params_bad = build_mlp(use_kaiming=False)\n",
        "grads_bad = get_grad_stats(params_bad)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "params_kaiming = build_mlp(use_kaiming=True)\n",
        "grads_kaiming = get_grad_stats(params_kaiming)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "x = ['W1', 'W2', 'W3', 'W4']\n",
        "ax.bar([i - 0.2 for i in range(4)], grads_bad, width=0.4, label='Random init', color='coral', alpha=0.8)\n",
        "ax.bar([i + 0.2 for i in range(4)], grads_kaiming, width=0.4, label='Kaiming init', color='steelblue', alpha=0.8)\n",
        "ax.set_xticks(range(4))\n",
        "ax.set_xticklabels(x)\n",
        "ax.set_ylabel(\"Mean |gradient|\")\n",
        "ax.set_title(\"Gradient flow: Bad init → vanishing; Kaiming → healthy\")\n",
        "ax.legend()\n",
        "ax.set_yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Random init:\", [f\"{g:.2e}\" for g in grads_bad])\n",
        "print(\"Kaiming init:\", [f\"{g:.2e}\" for g in grads_kaiming])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Xavier vs Kaiming\n",
        "\n",
        "**Xavier (Glorot)**: `1/sqrt(fan_in)` — assumes linear activations (symmetric, zero-centered). Good for tanh/sigmoid when layers are not too deep.\n",
        "\n",
        "**Kaiming (He)**: `sqrt(2/fan_in)` — designed for ReLU (half the activations are zero). For tanh, we use the same formula because tanh is also zero-centered and we want to preserve variance through the nonlinearity.\n",
        "\n",
        "**When to use which:**\n",
        "- **Xavier**: tanh, sigmoid, linear — when activations are symmetric.\n",
        "- **Kaiming**: ReLU, LeakyReLU — when half (or more) of activations are zero. Kaiming accounts for the \"dead\" half.\n",
        "\n",
        "For our tanh MLP, both work; Kaiming often gives slightly better gradient flow in deeper nets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_mlp_xavier():\n",
        "    C = torch.randn(27, 10, requires_grad=True)\n",
        "    W1 = torch.randn(10, 100, requires_grad=True)\n",
        "    b1 = torch.randn(100, requires_grad=True)\n",
        "    W2 = torch.randn(100, 100, requires_grad=True)\n",
        "    b2 = torch.randn(100, requires_grad=True)\n",
        "    W3 = torch.randn(100, 100, requires_grad=True)\n",
        "    b3 = torch.randn(100, requires_grad=True)\n",
        "    W4 = torch.randn(100, 27, requires_grad=True)\n",
        "    b4 = torch.randn(27, requires_grad=True)\n",
        "\n",
        "    # Xavier: 1/sqrt(fan_in)\n",
        "    C.data *= (1.0 / 27) ** 0.5\n",
        "    W1.data *= (1.0 / 10) ** 0.5\n",
        "    W2.data *= (1.0 / 100) ** 0.5\n",
        "    W3.data *= (1.0 / 100) ** 0.5\n",
        "    W4.data *= (1.0 / 100) ** 0.5\n",
        "    b1.data.zero_(); b2.data.zero_(); b3.data.zero_(); b4.data.zero_()\n",
        "\n",
        "    return {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4}\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "params_xavier = build_mlp_xavier()\n",
        "_, acts_xavier = forward(params_xavier, xs, return_activations=True)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "params_k = build_mlp(use_kaiming=True)\n",
        "_, acts_kaiming = forward(params_k, xs, return_activations=True)\n",
        "\n",
        "print(\"Xavier vs Kaiming — activation std per layer:\")\n",
        "print(\"-\" * 45)\n",
        "for i in range(3):\n",
        "    sx = acts_xavier[i].std().item()\n",
        "    sk = acts_kaiming[i].std().item()\n",
        "    print(f\"Layer {i+1}: Xavier std={sx:.4f}, Kaiming std={sk:.4f}\")\n",
        "\n",
        "print(\"\\nBoth keep activations in a reasonable range. Kaiming tends to be slightly larger (std ~0.6–0.7).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Blog:** [Day 11 — Activations & Gradients](https://omkarray.com/llm-day11.html)\n",
        "\n",
        "**Prev:** [Day 10 — Embeddings & LR](llm_day10_embeddings_lr.ipynb) · **Next:** [Day 12 — BatchNorm](llm_day12_batchnorm.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}