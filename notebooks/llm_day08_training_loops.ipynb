{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8: Training Loops, Loss Functions & Evaluation Splits\n",
    "\n",
    "**Building LLMs from Scratch** — Following Andrej Karpathy's makemore lectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Yesterday's bigram model *counted* character pairs and normalized. Today we replace counting with a **neural bigram model**: a 27×27 weight matrix trained via gradient descent. We build proper training infrastructure — forward pass, loss, backward, parameter updates — and introduce **train/dev/test splits** for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup\n",
    "\n",
    "Use the same names list from Day 6. Build bigram pairs `(xs, ys)` as integer tensors: `xs` = current character index, `ys` = next character index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'mia', 'charlotte', 'amelia', 'harper', 'evelyn',\n",
    "         'abigail', 'emily', 'ella', 'elizabeth', 'camila', 'luna', 'sofia', 'avery', 'mila', 'aria']\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {'.': 0, **{c: i + 1 for i, c in enumerate(chars)}}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "print(f\"Dataset: {len(words)} names\")\n",
    "print(f\"Vocabulary size: {len(stoi)}\")\n",
    "\n",
    "# Build bigram pairs (xs, ys) as integer tensors\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for c1, c2 in zip(chs[:-1], chs[1:]):\n",
    "        xs.append(stoi[c1])\n",
    "        ys.append(stoi[c2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f\"Bigram pairs: {len(xs)}\")\n",
    "print(f\"xs (current): {xs[:10].tolist()}...\")\n",
    "print(f\"ys (next):    {ys[:10].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Bigram Model\n",
    "\n",
    "One-hot encode inputs, multiply by weight matrix W (27×27), apply softmax to get probabilities. We use `torch.nn.functional.one_hot` and manual softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "\n",
    "# Forward: one-hot -> W -> logits -> softmax -> probs\n",
    "def forward(xs, W):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()  # (N, 27)\n",
    "    logits = xenc @ W                             # (N, 27)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return logits, probs\n",
    "\n",
    "logits, probs = forward(xs, W)\n",
    "print(f\"Input shape: xs {xs.shape}\")\n",
    "print(f\"One-hot shape: (N, 27)\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Probs shape: {probs.shape}\")\n",
    "print(f\"Probs sum per row: {probs.sum(1)[:5].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Training Loop\n",
    "\n",
    "Full loop: forward → NLL loss (using `F.cross_entropy`, equivalent to manual NLL) → backward → update. Add L2 regularization. Run 200 iterations, print loss every 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "lr = 50.0\n",
    "reg = 0.01\n",
    "\n",
    "for step in range(200):\n",
    "    # Forward\n",
    "    logits, probs = forward(xs, W)\n",
    "    \n",
    "    # Loss: cross_entropy(logits, ys) = -mean(log(softmax(logits)[i, ys[i]]))\n",
    "    # Equivalent to: -mean(log(probs[range(N), ys]))\n",
    "    loss_ce = F.cross_entropy(logits, ys)\n",
    "    loss_reg = reg * (W ** 2).mean()\n",
    "    loss = loss_ce + loss_reg\n",
    "    \n",
    "    # Backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    W.data -= lr * W.grad\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        print(f\"step {step:3d}: loss = {loss.item():.4f} (ce={loss_ce.item():.4f}, reg={loss_reg.item():.4f})\")\n",
    "\n",
    "print(f\"\\nFinal loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: F.cross_entropy is equivalent to manual NLL\n",
    "logits, probs = forward(xs, W)\n",
    "manual_nll = -torch.log(probs[torch.arange(len(ys)), ys]).mean()\n",
    "ce_loss = F.cross_entropy(logits, ys)\n",
    "print(f\"Manual NLL: {manual_nll.item():.4f}\")\n",
    "print(f\"F.cross_entropy: {ce_loss.item():.4f}\")\n",
    "print(f\"Same: {torch.allclose(manual_nll, ce_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Dev/Test Split\n",
    "\n",
    "Split data 80/10/10. Evaluate loss on each split to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices for 80/10/10 split\n",
    "torch.manual_seed(42)\n",
    "perm = torch.randperm(len(xs))\n",
    "\n",
    "n = len(xs)\n",
    "n_train = int(0.8 * n)\n",
    "n_dev = int(0.1 * n)\n",
    "n_test = n - n_train - n_dev\n",
    "\n",
    "train_idx = perm[:n_train]\n",
    "dev_idx = perm[n_train:n_train + n_dev]\n",
    "test_idx = perm[n_train + n_dev:]\n",
    "\n",
    "xs_train = xs[train_idx]\n",
    "ys_train = ys[train_idx]\n",
    "xs_dev = xs[dev_idx]\n",
    "ys_dev = ys[dev_idx]\n",
    "xs_test = xs[test_idx]\n",
    "ys_test = ys[test_idx]\n",
    "\n",
    "print(f\"Train: {len(xs_train)}, Dev: {len(xs_dev)}, Test: {len(xs_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on train split only, evaluate on all splits\n",
    "torch.manual_seed(42)\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "lr = 50.0\n",
    "reg = 0.01\n",
    "\n",
    "for step in range(200):\n",
    "    logits, _ = forward(xs_train, W)\n",
    "    loss_ce = F.cross_entropy(logits, ys_train)\n",
    "    loss_reg = reg * (W ** 2).mean()\n",
    "    loss = loss_ce + loss_reg\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data -= lr * W.grad\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            train_loss = F.cross_entropy(forward(xs_train, W)[0], ys_train).item()\n",
    "            dev_loss = F.cross_entropy(forward(xs_dev, W)[0], ys_dev).item()\n",
    "            test_loss = F.cross_entropy(forward(xs_test, W)[0], ys_test).item()\n",
    "        print(f\"step {step:3d}: train={train_loss:.4f} dev={dev_loss:.4f} test={test_loss:.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = F.cross_entropy(forward(xs_train, W)[0], ys_train).item()\n",
    "    dev_loss = F.cross_entropy(forward(xs_dev, W)[0], ys_dev).item()\n",
    "    test_loss = F.cross_entropy(forward(xs_test, W)[0], ys_test).item()\n",
    "print(f\"\\nFinal: train={train_loss:.4f} dev={dev_loss:.4f} test={test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sampling\n",
    "\n",
    "Generate names from the trained model. Start with `.`, sample next char from softmax, repeat until `.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_name(W, itos):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "            logits = xenc @ W\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    return ''.join(out)\n",
    "\n",
    "print(\"Generated names:\")\n",
    "for _ in range(10):\n",
    "    print(sample_name(W, itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing to Count-Based\n",
    "\n",
    "The neural model trained with gradient descent should converge to similar probabilities as the count-based bigram from Day 6. Build the count-based P matrix and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Count-based bigram (from Day 6)\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for c1, c2 in zip(chs[:-1], chs[1:]):\n",
    "        ix1, ix2 = stoi[c1], stoi[c2]\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "P_count = (N + 1).float()  # add-1 smoothing\n",
    "P_count = P_count / P_count.sum(1, keepdim=True)\n",
    "\n",
    "# Neural bigram: softmax(W) per row\n",
    "with torch.no_grad():\n",
    "    P_neural = F.softmax(W, dim=1)\n",
    "\n",
    "print(\"Count-based P (sample row 0, first 10):\")\n",
    "print(P_count[0, :10].tolist())\n",
    "print(\"\\nNeural P (sample row 0, first 10):\")\n",
    "print(P_neural[0, :10].tolist())\n",
    "\n",
    "# NLL comparison\n",
    "nll_count = -torch.log(P_count[xs, ys]).mean().item()\n",
    "nll_neural = F.cross_entropy(forward(xs, W)[0], ys).item()\n",
    "print(f\"\\nCount-based NLL: {nll_count:.4f}\")\n",
    "print(f\"Neural NLL: {nll_neural:.4f}\")\n",
    "print(f\"\\nBoth should be ~2.45 — neural model learns the same distribution via optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Blog:** [Day 8 — Training Loops & Evaluation Splits](https://omkarray.com/llm-day8.html)\n",
    "\n",
    "**Prev:** [Day 7 — MLP Language Model](llm_day07_mlp_lm.ipynb) · **Next:** [Day 9 — Bengio 2003 MLP](llm_day09_bengio.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
