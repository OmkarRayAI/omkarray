{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Functions & the Full BO Loop\n",
    "\n",
    "**Bayesian Optimisation Series · Notebook 3 of 3**\n",
    "\n",
    "This notebook:\n",
    "1. Implements EI, PI, and UCB acquisition functions from scratch\n",
    "2. Visualises all three on the same GP posterior — showing how they pick different next points\n",
    "3. Runs a complete Bayesian Optimisation loop on a multi-modal test function\n",
    "4. Compares BO against random search\n",
    "5. Demonstrates the exploration–exploitation trade-off via the UCB β parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (12, 5)\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GP Infrastructure (from Notebook 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, l=1.0, sf=1.0):\n",
    "    sq = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1).reshape(1, -1) - 2 * X1 @ X2.T\n",
    "    return sf**2 * np.exp(-0.5 * sq / l**2)\n",
    "\n",
    "\n",
    "def gp_posterior(X_train, y_train, X_test, l=1.0, sf=1.0, noise=1e-6):\n",
    "    K = rbf_kernel(X_train, X_train, l, sf) + noise * np.eye(len(X_train))\n",
    "    K_s = rbf_kernel(X_train, X_test, l, sf)\n",
    "    K_ss = rbf_kernel(X_test, X_test, l, sf)\n",
    "\n",
    "    L = np.linalg.cholesky(K)\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y_train))\n",
    "    mu = (K_s.T @ alpha).ravel()\n",
    "\n",
    "    v = np.linalg.solve(L, K_s)\n",
    "    var = np.diag(K_ss) - np.sum(v**2, axis=0)\n",
    "    var = np.maximum(var, 1e-10)\n",
    "\n",
    "    return mu, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Three Canonical Acquisition Functions\n",
    "\n",
    "All three are derived from the GP posterior $\\mathcal{N}(\\mu_n(x), \\sigma_n^2(x))$ and the current best $f^* = \\max\\{y_1, \\ldots, y_n\\}$.\n",
    "\n",
    "**PI:** $\\alpha_{PI}(x) = \\Phi\\left(\\frac{\\mu_n(x) - f^* - \\xi}{\\sigma_n(x)}\\right)$\n",
    "\n",
    "**EI:** $\\alpha_{EI}(x) = (\\mu_n(x) - f^*) \\Phi(Z) + \\sigma_n(x) \\phi(Z)$, where $Z = \\frac{\\mu_n(x) - f^*}{\\sigma_n(x)}$\n",
    "\n",
    "**UCB:** $\\alpha_{UCB}(x) = \\mu_n(x) + \\sqrt{\\beta} \\cdot \\sigma_n(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of_improvement(mu, var, f_best, xi=0.01):\n",
    "    sigma = np.sqrt(var)\n",
    "    Z = (mu - f_best - xi) / (sigma + 1e-10)\n",
    "    return norm.cdf(Z)\n",
    "\n",
    "\n",
    "def expected_improvement(mu, var, f_best, xi=0.01):\n",
    "    sigma = np.sqrt(var)\n",
    "    Z = (mu - f_best - xi) / (sigma + 1e-10)\n",
    "    ei = (mu - f_best - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-10] = 0.0\n",
    "    return ei\n",
    "\n",
    "\n",
    "def upper_confidence_bound(mu, var, beta=2.0):\n",
    "    return mu + np.sqrt(beta) * np.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualising All Three on the Same GP Posterior\n",
    "\n",
    "Same data, same GP — three different strategies for picking the next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_fn = lambda x: np.sin(x) * np.cos(0.5 * x) + 0.3 * np.sin(3 * x)\n",
    "\n",
    "X_test = np.linspace(-5, 5, 500).reshape(-1, 1)\n",
    "X_train = np.array([-3.0, -0.5, 2.5]).reshape(-1, 1)\n",
    "y_train = true_fn(X_train.ravel())\n",
    "\n",
    "mu, var = gp_posterior(X_train, y_train, X_test, l=1.0, sf=1.0, noise=1e-4)\n",
    "std = np.sqrt(var)\n",
    "f_best = np.max(y_train)\n",
    "\n",
    "pi_vals = probability_of_improvement(mu, var, f_best)\n",
    "ei_vals = expected_improvement(mu, var, f_best)\n",
    "ucb_vals = upper_confidence_bound(mu, var, beta=2.0)\n",
    "\n",
    "acq_fns = [\n",
    "    ('PI — Probability of Improvement', pi_vals, '#d4a017', 'exploits mean'),\n",
    "    ('EI — Expected Improvement', ei_vals, '#2ca02c', 'balances mean + σ'),\n",
    "    ('UCB — Upper Confidence Bound', ucb_vals, '#7b2d8e', 'high mean + σ'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 9), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "for col, (name, acq, color, note) in enumerate(acq_fns):\n",
    "    ax_top = axes[0, col]\n",
    "    ax_bot = axes[1, col]\n",
    "\n",
    "    ax_top.fill_between(X_test.ravel(), mu - 2*std, mu + 2*std, alpha=0.15, color='steelblue')\n",
    "    ax_top.plot(X_test, mu, color='steelblue', linewidth=2, label='μₙ(x)')\n",
    "    ax_top.plot(X_test, true_fn(X_test.ravel()), 'k--', alpha=0.3, linewidth=1, label='true f(x)')\n",
    "    ax_top.scatter(X_train, y_train, c='black', s=60, zorder=5)\n",
    "    ax_top.axhline(f_best, color='#d62728', linestyle='--', alpha=0.5, label='f*')\n",
    "\n",
    "    x_next_idx = np.argmax(acq)\n",
    "    x_next = X_test[x_next_idx, 0]\n",
    "    ax_top.axvline(x_next, color=color, linestyle=':', linewidth=2, alpha=0.8)\n",
    "    ax_top.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax_top.legend(fontsize=8)\n",
    "\n",
    "    acq_norm = (acq - acq.min()) / (acq.max() - acq.min() + 1e-10)\n",
    "    ax_bot.fill_between(X_test.ravel(), 0, acq_norm, alpha=0.3, color=color)\n",
    "    ax_bot.plot(X_test, acq_norm, color=color, linewidth=2)\n",
    "    ax_bot.axvline(x_next, color=color, linestyle=':', linewidth=2, alpha=0.8)\n",
    "    ax_bot.scatter([x_next], [1.0], color=color, s=80, zorder=5, marker='v')\n",
    "    ax_bot.set_xlabel('x')\n",
    "    ax_bot.set_ylabel('α(x) normalised')\n",
    "    ax_bot.text(x_next + 0.2, 0.85, f'x_next = {x_next:.2f}\\n({note})', fontsize=9, color=color)\n",
    "\n",
    "fig.suptitle('Same GP Posterior, Three Acquisition Functions → Three Different Next Points', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Bayesian Optimisation Loop\n",
    "\n",
    "We run BO on a multi-modal test function using EI as the acquisition function, starting from 2 random initial points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimisation(f, bounds, n_init=2, n_iter=15, acq='ei', l=1.0, sf=1.0, beta=2.0):\n",
    "    X_grid = np.linspace(bounds[0], bounds[1], 1000).reshape(-1, 1)\n",
    "\n",
    "    X_obs = np.random.uniform(bounds[0], bounds[1], size=(n_init, 1))\n",
    "    y_obs = f(X_obs.ravel())\n",
    "\n",
    "    history = [{'X': X_obs.copy(), 'y': y_obs.copy(), 'best': np.max(y_obs)}]\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        mu, var = gp_posterior(X_obs, y_obs, X_grid, l=l, sf=sf, noise=1e-4)\n",
    "        f_best = np.max(y_obs)\n",
    "\n",
    "        if acq == 'ei':\n",
    "            acq_vals = expected_improvement(mu, var, f_best)\n",
    "        elif acq == 'pi':\n",
    "            acq_vals = probability_of_improvement(mu, var, f_best)\n",
    "        elif acq == 'ucb':\n",
    "            acq_vals = upper_confidence_bound(mu, var, beta=beta)\n",
    "\n",
    "        x_next = X_grid[np.argmax(acq_vals)].reshape(1, 1)\n",
    "        y_next = f(x_next.ravel())\n",
    "\n",
    "        X_obs = np.vstack([X_obs, x_next])\n",
    "        y_obs = np.append(y_obs, y_next)\n",
    "\n",
    "        history.append({'X': X_obs.copy(), 'y': y_obs.copy(), 'best': np.max(y_obs)})\n",
    "\n",
    "    return X_obs, y_obs, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn = lambda x: -(x - 2)**2 * np.sin(3*x) + 2 * np.exp(-0.5 * (x + 1)**2)\n",
    "\n",
    "bounds = (-5, 5)\n",
    "X_obs, y_obs, history = bayesian_optimisation(test_fn, bounds, n_init=2, n_iter=15, acq='ei', l=1.0)\n",
    "\n",
    "X_grid = np.linspace(bounds[0], bounds[1], 1000).reshape(-1, 1)\n",
    "y_true = test_fn(X_grid.ravel())\n",
    "x_true_best = X_grid[np.argmax(y_true), 0]\n",
    "y_true_best = np.max(y_true)\n",
    "\n",
    "print(f'True optimum:  x* = {x_true_best:.3f},  f(x*) = {y_true_best:.3f}')\n",
    "print(f'BO found:      x̂* = {X_obs[np.argmax(y_obs), 0]:.3f},  f(x̂*) = {np.max(y_obs):.3f}')\n",
    "print(f'Gap (simple regret): {y_true_best - np.max(y_obs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualising the BO Loop — Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_to_show = [0, 2, 5, 10, 15]\n",
    "fig, axes = plt.subplots(1, len(steps_to_show), figsize=(20, 4), sharey=True)\n",
    "\n",
    "for ax, step in zip(axes, steps_to_show):\n",
    "    h = history[step]\n",
    "    X_t, y_t = h['X'], h['y']\n",
    "\n",
    "    if len(X_t) >= 2:\n",
    "        mu, var = gp_posterior(X_t, y_t, X_grid, l=1.0, sf=1.0, noise=1e-4)\n",
    "        std = np.sqrt(var)\n",
    "        ax.fill_between(X_grid.ravel(), mu - 2*std, mu + 2*std, alpha=0.15, color='steelblue')\n",
    "        ax.plot(X_grid, mu, color='steelblue', linewidth=2)\n",
    "\n",
    "    ax.plot(X_grid, y_true, 'k--', alpha=0.3, linewidth=1)\n",
    "    ax.scatter(X_t, y_t, c='black', s=40, zorder=5)\n",
    "\n",
    "    best_idx = np.argmax(y_t)\n",
    "    ax.scatter(X_t[best_idx], y_t[best_idx], c='#d62728', s=100, zorder=6, marker='*', label=f'best = {y_t[best_idx]:.2f}')\n",
    "\n",
    "    n_total = len(X_t)\n",
    "    ax.set_title(f'n = {n_total} points', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(fontsize=8, loc='lower left')\n",
    "\n",
    "axes[0].set_ylabel('f(x)')\n",
    "fig.suptitle('Bayesian Optimisation with EI — Progressive Convergence', fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BO vs Random Search — Convergence Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "n_total = 17\n",
    "\n",
    "bo_bests = []\n",
    "rand_bests = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    np.random.seed(run)\n",
    "    _, _, hist = bayesian_optimisation(test_fn, bounds, n_init=2, n_iter=15, acq='ei', l=1.0)\n",
    "    bo_bests.append([h['best'] for h in hist])\n",
    "\n",
    "    np.random.seed(run)\n",
    "    X_rand = np.random.uniform(bounds[0], bounds[1], size=n_total)\n",
    "    y_rand = test_fn(X_rand)\n",
    "    rand_bests.append([np.max(y_rand[:i+1]) for i in range(n_total)])\n",
    "\n",
    "bo_bests = np.array(bo_bests)\n",
    "rand_bests = np.array(rand_bests)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "iters = np.arange(n_total)\n",
    "ax.fill_between(iters, np.percentile(bo_bests, 25, axis=0), np.percentile(bo_bests, 75, axis=0),\n",
    "                alpha=0.2, color='steelblue')\n",
    "ax.plot(iters, np.median(bo_bests, axis=0), linewidth=2.5, color='steelblue', label='BO (EI) — median')\n",
    "\n",
    "ax.fill_between(iters, np.percentile(rand_bests, 25, axis=0), np.percentile(rand_bests, 75, axis=0),\n",
    "                alpha=0.2, color='#d62728')\n",
    "ax.plot(iters, np.median(rand_bests, axis=0), linewidth=2.5, color='#d62728', label='Random search — median')\n",
    "\n",
    "ax.axhline(y_true_best, color='black', linestyle='--', alpha=0.5, label=f'True optimum = {y_true_best:.2f}')\n",
    "\n",
    "ax.set_xlabel('Number of evaluations')\n",
    "ax.set_ylabel('Best f(x) found')\n",
    "ax.set_title('BO vs Random Search — 20 runs, shaded IQR')\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploration–Exploitation: The UCB β Spectrum\n",
    "\n",
    "β controls the width of the confidence bound:\n",
    "- β → 0: pure exploitation (follow the mean)\n",
    "- β → ∞: pure exploration (follow uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [0.01, 0.5, 2.0, 5.0, 20.0]\n",
    "beta_labels = ['β=0.01\\n(exploit)', 'β=0.5\\n(mild)', 'β=2.0\\n(balanced)', 'β=5.0\\n(explore)', 'β=20.0\\n(max explore)']\n",
    "colors = ['#d62728', '#d4a017', '#2ca02c', '#1f77b4', '#7b2d8e']\n",
    "\n",
    "mu, var = gp_posterior(X_train, y_train, X_test, l=1.0, sf=1.0, noise=1e-4)\n",
    "std = np.sqrt(var)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 7), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "for col, (beta, label, color) in enumerate(zip(betas, beta_labels, colors)):\n",
    "    ax_top = axes[0, col]\n",
    "    ax_bot = axes[1, col]\n",
    "\n",
    "    ax_top.fill_between(X_test.ravel(), mu - 2*std, mu + 2*std, alpha=0.12, color='steelblue')\n",
    "    ax_top.plot(X_test, mu, color='steelblue', linewidth=1.5)\n",
    "    ax_top.scatter(X_train, y_train, c='black', s=40, zorder=5)\n",
    "\n",
    "    ucb = upper_confidence_bound(mu, var, beta=beta)\n",
    "    ax_top.plot(X_test, ucb, color=color, linewidth=1.5, linestyle='--', alpha=0.7, label=f'μ+√β·σ')\n",
    "\n",
    "    x_next_idx = np.argmax(ucb)\n",
    "    x_next = X_test[x_next_idx, 0]\n",
    "    ax_top.axvline(x_next, color=color, linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax_top.set_title(label, fontsize=10, fontweight='bold')\n",
    "    ax_top.legend(fontsize=7)\n",
    "\n",
    "    ucb_norm = (ucb - ucb.min()) / (ucb.max() - ucb.min() + 1e-10)\n",
    "    ax_bot.fill_between(X_test.ravel(), 0, ucb_norm, alpha=0.3, color=color)\n",
    "    ax_bot.plot(X_test, ucb_norm, color=color, linewidth=1.5)\n",
    "    ax_bot.axvline(x_next, color=color, linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax_bot.set_xlabel('x')\n",
    "\n",
    "fig.suptitle('UCB Exploration–Exploitation Spectrum — Same GP, Different β', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cumulative Regret — Convergence in Practice\n",
    "\n",
    "Simple regret $r_T = f(x^*) - \\max_{t \\leq T} f(x_t)$ and cumulative regret $R_T = \\sum_{t=1}^T [f(x^*) - f(x_t)]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_bo, y_bo, hist_bo = bayesian_optimisation(test_fn, bounds, n_init=2, n_iter=25, acq='ei', l=1.0)\n",
    "\n",
    "simple_regret = [y_true_best - h['best'] for h in hist_bo]\n",
    "cumulative_regret = np.cumsum([y_true_best - y for y in y_bo])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(simple_regret, 'o-', color='steelblue', linewidth=2, markersize=5)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Simple Regret r_T')\n",
    "ax1.set_title('Simple Regret — Gap to True Optimum')\n",
    "ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2.plot(cumulative_regret, 'o-', color='#d62728', linewidth=2, markersize=5)\n",
    "ax2.set_xlabel('Evaluation number t')\n",
    "ax2.set_ylabel('Cumulative Regret R_T')\n",
    "ax2.set_title('Cumulative Regret — Sublinear Growth = No-Regret')\n",
    "\n",
    "t = np.arange(1, len(cumulative_regret) + 1)\n",
    "ax2.plot(t, cumulative_regret[-1] * np.sqrt(t) / np.sqrt(t[-1]), '--', color='gray', alpha=0.5, label='O(√T) reference')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Previous:** [Notebook 1 — GP Surrogate](./01_gp_surrogate.ipynb) · [Notebook 2 — Kernel Design](./02_kernels.ipynb)\n",
    "\n",
    "**Back to article:** [Bayesian Optimisation — Mathematical Deep Dive](https://omkarray.com/bayesian-optimization.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}