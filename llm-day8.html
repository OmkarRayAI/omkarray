<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Day 8 &mdash; Training Loops, Loss Functions &amp; Evaluation Splits</title>
<link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;1,400&family=Source+Serif+4:ital,wght@0,300;0,400;0,600;1,300;1,400&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
<style>
:root{--ink:#1a1a2e;--cream:#faf8f4;--cream-dark:#f0ece4;--accent:#c0392b;--blue:#2c3e6b;--green:#27654a;--gold:#d4a843;--gray:#7f8c8d;--gray-light:#bdc3c7;--purple:#5b2c6f;--orange:#d35400;}
*{margin:0;padding:0;box-sizing:border-box;}
body{background:var(--cream);color:var(--ink);font-family:'Source Serif 4',Georgia,serif;line-height:1.7;}
.progress-strip{position:sticky;top:0;z-index:100;background:var(--ink);padding:0.6rem 2rem;display:flex;align-items:center;gap:0.4rem;font-family:'JetBrains Mono',monospace;font-size:0.65rem;color:#888;}
.p-dot{width:8px;height:8px;border-radius:50%;border:1.5px solid #555;flex-shrink:0;}
.p-dot.done{background:var(--green);border-color:var(--green);}
.p-dot.now{background:var(--accent);border-color:var(--accent);box-shadow:0 0 6px rgba(192,57,43,0.5);}
.p-line{width:24px;height:1.5px;background:#444;flex-shrink:0;}.p-line.done{background:var(--green);}
.p-label{color:#666;margin-left:0.3rem;margin-right:0.5rem;}.p-label.done{color:var(--green);}.p-label.now{color:var(--accent);font-weight:600;}
.back-link{display:block;font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:.18em;text-transform:uppercase;color:var(--gray);text-decoration:none;padding:1.2rem 2rem 0;max-width:1100px;margin:0 auto;transition:color .2s;}.back-link:hover{color:var(--accent);}
.header{padding:2rem 2rem 2rem;max-width:1100px;margin:0 auto;border-bottom:3px double var(--ink);}
.header-meta{font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:0.15em;text-transform:uppercase;color:var(--gray);margin-bottom:0.5rem;}
.phase-tag{background:var(--ink);color:white;padding:0.15rem 0.5rem;border-radius:2px;font-size:0.62rem;margin-right:0.4rem;}
.header h1{font-family:'Playfair Display',serif;font-size:clamp(2rem,5vw,3.1rem);font-weight:700;line-height:1.15;margin-bottom:0.5rem;}
.header h1 span{color:var(--accent);}
.header-sub{font-style:italic;font-size:1.05rem;color:var(--blue);max-width:700px;}
.content{max-width:1100px;margin:0 auto;padding:2rem 2rem 3rem;}
.memo-quote{background:var(--cream-dark);border-left:4px solid var(--accent);padding:1.5rem 2rem;margin:0 0 3rem;font-style:italic;font-size:1.02rem;color:var(--blue);position:relative;}
.memo-quote::before{content:'"\27';font-family:'Playfair Display',serif;font-size:4rem;color:var(--accent);position:absolute;top:-0.5rem;left:0.5rem;opacity:0.25;}
.memo-quote .attr{display:block;margin-top:0.6rem;font-style:normal;font-size:0.8rem;color:var(--gray);font-family:'JetBrains Mono',monospace;}
.sh{font-family:'Playfair Display',serif;font-size:1.55rem;font-weight:700;margin:3rem 0 1.25rem;padding-bottom:0.5rem;border-bottom:1px solid var(--gray-light);}
.sh .n{color:var(--accent);font-style:italic;margin-right:0.2rem;}
.prose{font-size:1rem;max-width:720px;margin-bottom:1.25rem;}
.prose strong{color:var(--blue);}
.prose code,code{font-family:'JetBrains Mono',monospace;font-size:0.83rem;background:var(--cream-dark);padding:0.1rem 0.35rem;border-radius:2px;color:var(--accent);}
.vf{background:white;border:1px solid var(--gray-light);border-radius:2px;padding:2rem 1.5rem 1.5rem;margin:1.5rem 0 2rem;position:relative;overflow:hidden;}
.vf::before{content:'';position:absolute;top:0;left:0;right:0;height:3px;}
.vf.green::before{background:linear-gradient(90deg,var(--green),var(--blue));}
.vf.gold::before{background:linear-gradient(90deg,var(--gold),var(--accent));}
.vf.purple::before{background:linear-gradient(90deg,var(--purple),var(--blue));}
.vf-label{font-family:'JetBrains Mono',monospace;font-size:0.68rem;letter-spacing:0.18em;text-transform:uppercase;color:var(--gray);margin-bottom:1rem;}
svg text{font-family:'Source Serif 4',Georgia,serif;}
svg .m{font-family:'JetBrains Mono',monospace;}
.box{border-radius:2px;padding:1.4rem 1.8rem;margin:2rem 0;position:relative;}
.box::before{position:absolute;top:-0.75rem;left:1rem;font-size:1.2rem;background:var(--cream);padding:0 0.5rem;}
.box h4{font-family:'Playfair Display',serif;margin-bottom:0.4rem;font-size:1rem;}
.box p{font-size:0.93rem;color:#555;}
.box.insight{background:linear-gradient(135deg,#fdf6e8,#fef9f0);border:1px solid var(--gold);}.box.insight::before{content:'\1F4A1';}.box.insight h4{color:var(--gold);}
.box.danger{background:linear-gradient(135deg,#fdf0ef,#fef5f4);border:1px solid var(--accent);}.box.danger::before{content:'\26A0\FE0F';}.box.danger h4{color:var(--accent);}
.code-block{background:#1e1e2e;color:#cdd6f4;border-radius:4px;padding:1.5rem;margin:1.5rem 0;overflow-x:auto;font-family:'JetBrains Mono',monospace;font-size:0.82rem;line-height:1.8;}
.code-block .comment{color:#6c7086;}.code-block .keyword{color:#cba6f7;}.code-block .string{color:#a6e3a1;}.code-block .number{color:#fab387;}.code-block .func{color:#89b4fa;}.code-block .class-name{color:#f9e2af;}.code-block .op{color:#89dceb;}.code-block .self{color:#f38ba8;}
.mx{display:grid;grid-template-columns:auto 1fr 1fr;grid-template-rows:auto 1fr 1fr;gap:0;margin:2rem 0;background:white;border:1px solid var(--gray-light);border-radius:2px;overflow:hidden;}
.mx-corner{background:var(--ink);padding:0.8rem;}
.mx-ch{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;text-align:center;display:flex;align-items:center;justify-content:center;}
.mx-rh{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;writing-mode:vertical-lr;text-orientation:mixed;transform:rotate(180deg);display:flex;align-items:center;justify-content:center;}
.mx-cell{padding:1.2rem;border:1px solid var(--cream-dark);}.mx-cell h4{font-family:'Playfair Display',serif;font-size:0.95rem;margin-bottom:0.4rem;}.mx-cell p{font-size:0.83rem;color:#555;line-height:1.5;}.mx-cell.best{background:#f0faf4;}.mx-cell.worst{background:#fdf0ef;}.mx-cell .e{font-size:1.4rem;display:block;margin-bottom:0.4rem;}
.mx-cell code{font-family:'JetBrains Mono',monospace;font-size:0.8rem;background:var(--cream-dark);padding:0.1rem 0.3rem;border-radius:2px;color:var(--accent);}
.cl{list-style:none;margin:1.5rem 0;}.cl li{padding:0.55rem 0 0.55rem 2rem;position:relative;font-size:0.93rem;border-bottom:1px dotted var(--gray-light);}.cl li::before{content:'\2610';position:absolute;left:0;color:var(--accent);font-size:1.1rem;}.cl li strong{font-family:'JetBrains Mono',monospace;font-size:0.8rem;color:var(--blue);}
.footer{max-width:1100px;margin:0 auto;padding:2rem;border-top:3px double var(--ink);display:flex;justify-content:space-between;align-items:center;font-family:'JetBrains Mono',monospace;font-size:0.68rem;color:var(--gray);text-transform:uppercase;letter-spacing:0.1em;flex-wrap:wrap;gap:0.5rem;}
.fi{opacity:0;transform:translateY(16px);animation:fu 0.5s ease forwards;}
@keyframes fu{to{opacity:1;transform:translateY(0);}}
.fi:nth-child(2){animation-delay:0.08s;}.fi:nth-child(3){animation-delay:0.16s;}.fi:nth-child(4){animation-delay:0.24s;}
@media(max-width:600px){.header{padding:1.5rem 1rem;}.content{padding:1.5rem 1rem;}.footer{flex-direction:column;text-align:center;}}

.notebook-card{margin:2.5rem 0;border:2px solid var(--ink);background:white;overflow:hidden;}
.notebook-card-hdr{padding:14px 20px;background:var(--ink);color:white;display:flex;justify-content:space-between;align-items:center;}
.notebook-card-hdr .nb-title{font-family:'JetBrains Mono',monospace;font-size:11px;letter-spacing:.08em;text-transform:uppercase;font-weight:600;}
.notebook-card-hdr .nb-badge{font-family:'JetBrains Mono',monospace;font-size:9px;padding:3px 10px;border:1px solid rgba(255,255,255,.3);letter-spacing:.08em;text-transform:uppercase;opacity:.7;}
.notebook-card-body{padding:20px;}
.notebook-card-body p{font-size:0.95rem;color:#555;line-height:1.7;margin-bottom:12px;}
.notebook-links{display:flex;gap:12px;flex-wrap:wrap;}
.notebook-links a{font-family:'JetBrains Mono',monospace;font-size:10px;letter-spacing:.06em;text-transform:uppercase;padding:8px 16px;text-decoration:none;border:1.5px solid;transition:all .15s;font-weight:600;}
.nb-colab{background:#f9ab00;color:#1c1c1c;border-color:#f9ab00;}.nb-colab:hover{background:#e09800;border-color:#e09800;}
.nb-github{background:white;color:var(--ink);border-color:var(--ink);}.nb-github:hover{background:var(--ink);color:white;}
.nb-nbviewer{background:white;color:#2c3e6b;border-color:#2c3e6b;}.nb-nbviewer:hover{background:#2c3e6b;color:white;}

</style>
</head>
<body>

<div class="progress-strip">
  <span class="p-dot done"></span><span class="p-label done">1-7</span><span class="p-line done"></span>
  <span class="p-dot now"></span><span class="p-label now">8</span><span class="p-line"></span>
  <span class="p-dot"></span><span class="p-label">9</span><span class="p-line"></span>
  <span class="p-dot"></span><span class="p-label">10</span>
  <span style="color:#555;margin-left:0.5rem;">&middot; &middot; &middot;</span>
  <span class="p-dot" style="margin-left:0.5rem;"></span><span class="p-label">80</span>
</div>

<a href="index.html" class="back-link">&larr; Back to index</a>

<header class="header fi">
  <div class="header-meta"><span class="phase-tag">PHASE 1</span> Foundations &middot; Day 8 of 80 &middot; Neural Networks &amp; Backprop</div>
  <h1>Training Loops, <span>Loss Functions</span> &amp; Evaluation Splits</h1>
  <p class="header-sub">The neural bigram model: replace counting with gradient descent. Split data into train/dev/test. Learn proper model evaluation.</p>
</header>

<main class="content">

  <div class="memo-quote fi">
    You cannot evaluate a fund&rsquo;s returns using the same data that chose the strategy. In-sample performance
    is not out-of-sample truth. The discipline of train/dev/test splits is the machine learning equivalent of
    this principle: you <em>must</em> evaluate on data the model has never seen.
    <span class="attr">&mdash; Day 8 Principle, adapted from the Marks framework</span>
  </div>

  <h2 class="sh fi"><span class="n">I.</span> From Counting to Gradient Descent</h2>
  <p class="prose fi">
    Yesterday&rsquo;s bigram model <em>counted</em> character pairs and normalized. Today we replace counting with a
    <strong>neural network</strong>: a 27&times;27 weight matrix <code>W</code>, trained via gradient descent, that learns
    the same probabilities. The result is numerically identical &mdash; but the method generalizes to any architecture.
  </p>

  <div class="fi">
    <div class="vf green">
      <div class="vf-label">Exhibit A &mdash; Neural Bigram: One-Hot &rarr; W &rarr; Softmax &rarr; Loss</div>
      <svg viewBox="0 0 800 160" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
        <rect x="20" y="45" width="110" height="50" rx="4" fill="#eef5ee" stroke="#27654a" stroke-width="1.5"/>
        <text x="75" y="67" text-anchor="middle" class="m" font-size="9" fill="#27654a">one_hot(x)</text>
        <text x="75" y="83" text-anchor="middle" class="m" font-size="8" fill="#888">[B, 27]</text>

        <line x1="130" y1="70" x2="175" y2="70" stroke="#888" stroke-width="1.2" marker-end="url(#a8)"/>

        <rect x="180" y="45" width="90" height="50" rx="4" fill="#edf3fc" stroke="#2c3e6b" stroke-width="1.5"/>
        <text x="225" y="67" text-anchor="middle" class="m" font-size="9" fill="#2c3e6b">@ W</text>
        <text x="225" y="83" text-anchor="middle" class="m" font-size="8" fill="#888">[27, 27]</text>

        <line x1="270" y1="70" x2="315" y2="70" stroke="#888" stroke-width="1.2" marker-end="url(#a8)"/>
        <text x="293" y="58" class="m" font-size="7" fill="#aaa">logits</text>

        <rect x="320" y="45" width="100" height="50" rx="4" fill="#fdf6e8" stroke="#d4a843" stroke-width="1.5"/>
        <text x="370" y="67" text-anchor="middle" class="m" font-size="9" fill="#d4a843">softmax</text>
        <text x="370" y="83" text-anchor="middle" class="m" font-size="8" fill="#888">[B, 27]</text>

        <line x1="420" y1="70" x2="465" y2="70" stroke="#888" stroke-width="1.2" marker-end="url(#a8)"/>
        <text x="443" y="58" class="m" font-size="7" fill="#aaa">probs</text>

        <rect x="470" y="45" width="130" height="50" rx="4" fill="#fef5f4" stroke="#c0392b" stroke-width="1.5"/>
        <text x="535" y="67" text-anchor="middle" class="m" font-size="9" fill="#c0392b">-log(P[target])</text>
        <text x="535" y="83" text-anchor="middle" class="m" font-size="8" fill="#888">NLL loss</text>

        <line x1="600" y1="70" x2="645" y2="70" stroke="#c0392b" stroke-width="1.2" marker-end="url(#a8r)"/>

        <rect x="650" y="50" width="80" height="40" rx="20" fill="#c0392b"/>
        <text x="690" y="75" text-anchor="middle" class="m" font-size="10" fill="white" font-weight="600">.backward()</text>

        <defs>
          <marker id="a8" markerWidth="7" markerHeight="5" refX="6" refY="2.5" orient="auto"><polygon points="0 0,7 2.5,0 5" fill="#888"/></marker>
          <marker id="a8r" markerWidth="7" markerHeight="5" refX="6" refY="2.5" orient="auto"><polygon points="0 0,7 2.5,0 5" fill="#c0392b"/></marker>
        </defs>
      </svg>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">II.</span> The Code &mdash; Neural Bigram with Gradient Descent</h2>

  <div class="code-block fi">
<span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F

<span class="comment"># Prepare dataset</span>
xs, ys = [], []
<span class="keyword">for</span> w <span class="keyword">in</span> words:
    chs = [<span class="string">&#39;.&#39;</span>] <span class="op">+</span> <span class="func">list</span>(w) <span class="op">+</span> [<span class="string">&#39;.&#39;</span>]
    <span class="keyword">for</span> ch1, ch2 <span class="keyword">in</span> <span class="func">zip</span>(chs, chs[<span class="number">1</span>:]):
        xs.<span class="func">append</span>(stoi[ch1])
        ys.<span class="func">append</span>(stoi[ch2])
xs = torch.<span class="func">tensor</span>(xs)
ys = torch.<span class="func">tensor</span>(ys)
num = xs.<span class="func">nelement</span>()

<span class="comment"># Initialize weight matrix</span>
W = torch.<span class="func">randn</span>((<span class="number">27</span>, <span class="number">27</span>), requires_grad=<span class="keyword">True</span>)

<span class="comment"># Training loop</span>
<span class="keyword">for</span> k <span class="keyword">in</span> <span class="func">range</span>(<span class="number">100</span>):
    <span class="comment"># Forward pass</span>
    xenc = F.<span class="func">one_hot</span>(xs, num_classes=<span class="number">27</span>).<span class="func">float</span>()
    logits = xenc <span class="op">@</span> W              <span class="comment"># [N, 27]</span>
    counts = logits.<span class="func">exp</span>()           <span class="comment"># softmax numerator</span>
    probs = counts <span class="op">/</span> counts.<span class="func">sum</span>(<span class="number">1</span>, keepdim=<span class="keyword">True</span>)

    <span class="comment"># Loss: negative log-likelihood + regularization</span>
    loss = <span class="op">-</span>probs[torch.<span class="func">arange</span>(num), ys].<span class="func">log</span>().<span class="func">mean</span>()
    loss <span class="op">+=</span> <span class="number">0.01</span> <span class="op">*</span> (W<span class="op">**</span><span class="number">2</span>).<span class="func">mean</span>()     <span class="comment"># L2 regularization</span>

    <span class="comment"># Backward + update</span>
    W.grad = <span class="keyword">None</span>
    loss.<span class="func">backward</span>()
    W.data <span class="op">+=</span> <span class="op">-</span><span class="number">50</span> <span class="op">*</span> W.grad          <span class="comment"># larger lr for 27x27 matrix</span>

<span class="func">print</span>(f<span class="string">"final loss: {loss.item():.4f}"</span>)  <span class="comment"># ~2.46, matching counting approach</span>
  </div>

  <h2 class="sh fi"><span class="n">III.</span> Train / Dev / Test Splits &mdash; The Discipline of Evaluation</h2>

  <div class="fi">
    <div class="vf purple">
      <div class="vf-label">Exhibit B &mdash; Data Split Strategy: 80% / 10% / 10%</div>
      <svg viewBox="0 0 700 120" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
        <rect x="20" y="30" width="440" height="50" rx="4" fill="#eef5ee" stroke="#27654a" stroke-width="1.5"/>
        <text x="240" y="60" text-anchor="middle" class="m" font-size="12" fill="#27654a" font-weight="600">TRAIN (80%)</text>

        <rect x="465" y="30" width="105" height="50" rx="4" fill="#edf3fc" stroke="#2c3e6b" stroke-width="1.5"/>
        <text x="517" y="60" text-anchor="middle" class="m" font-size="10" fill="#2c3e6b" font-weight="600">DEV (10%)</text>

        <rect x="575" y="30" width="105" height="50" rx="4" fill="#fdf6e8" stroke="#d4a843" stroke-width="1.5"/>
        <text x="627" y="60" text-anchor="middle" class="m" font-size="10" fill="#d4a843" font-weight="600">TEST (10%)</text>

        <text x="240" y="105" text-anchor="middle" class="m" font-size="8" fill="#888">Fit parameters</text>
        <text x="517" y="105" text-anchor="middle" class="m" font-size="8" fill="#888">Tune hyperparams</text>
        <text x="627" y="105" text-anchor="middle" class="m" font-size="8" fill="#888">Final score (1&times;)</text>
      </svg>
    </div>
  </div>

  <div class="code-block fi">
<span class="keyword">import</span> random
random.<span class="func">shuffle</span>(words)
n1 = <span class="func">int</span>(<span class="number">0.8</span> <span class="op">*</span> <span class="func">len</span>(words))
n2 = <span class="func">int</span>(<span class="number">0.9</span> <span class="op">*</span> <span class="func">len</span>(words))

train_words = words[:n1]
dev_words   = words[n1:n2]
test_words  = words[n2:]

<span class="func">print</span>(f<span class="string">"train: {len(train_words)}, dev: {len(dev_words)}, test: {len(test_words)}"</span>)
  </div>

  <div class="box insight fi">
    <h4>The Loss Should Match</h4>
    <p>The neural bigram with gradient descent should converge to the <em>same</em> NLL (&sim;2.45) as the counting approach from Day 6. If it doesn&rsquo;t, you have a bug. This equivalence is the proof that the neural method is working correctly &mdash; it has learned the exact same probability distribution through optimization.</p>
  </div>

  <h2 class="sh fi"><span class="n">IV.</span> The Matrix &mdash; What Matters Today</h2>
  <div class="mx fi">
    <div class="mx-corner"></div>
    <div class="mx-ch">Builds Deep Intuition</div>
    <div class="mx-ch">Surface-Level Only</div>
    <div class="mx-rh">Quick to Do</div>
    <div class="mx-cell best">
      <span class="e">&#127919;</span>
      <h4>DO FIRST</h4>
      <p>Implement the neural bigram training loop. Verify final NLL matches the counting approach (&sim;2.45).</p>
    </div>
    <div class="mx-cell">
      <span class="e">&#9197;&#65039;</span>
      <h4>DO IF TIME</h4>
      <p>Implement train/dev/test splits. Train on train, evaluate on dev. Confirm they&rsquo;re close (for this simple model).</p>
    </div>
    <div class="mx-rh">Slow but Worth It</div>
    <div class="mx-cell">
      <span class="e">&#128400;</span>
      <h4>DO CAREFULLY</h4>
      <p>Add L2 regularization (<code>0.01 * (W**2).mean()</code>). Understand how it pushes weights toward uniform probabilities when the data is sparse.</p>
    </div>
    <div class="mx-cell worst">
      <span class="e">&#128683;</span>
      <h4>AVOID TODAY</h4>
      <p>Using <code>nn.CrossEntropyLoss</code> or any PyTorch abstractions. Implement softmax and NLL manually to understand what they do.</p>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">V.</span> Today&rsquo;s Deliverables</h2>
  <ul class="cl fi">
    <li><strong>Neural bigram:</strong> One-hot &rarr; <code>W</code> &rarr; softmax &rarr; NLL loss &rarr; backward &rarr; update</li>
    <li><strong>Loss convergence:</strong> Train for 100+ steps, verify NLL &asymp; 2.45</li>
    <li><strong>Data splits:</strong> Implement 80/10/10 train/dev/test split</li>
    <li><strong>Evaluation:</strong> Compute loss separately on train and dev sets</li>
    <li><strong>Regularization:</strong> Add L2 penalty and observe its smoothing effect</li>
    <li><strong>Sampling:</strong> Generate names from the trained neural model, compare quality with Day 6</li>
  </ul>

  <div class="memo-quote fi" style="margin-top:3rem;">
    The neural bigram produces the same result as counting &mdash; but the <em>framework</em> is what matters.
    This framework scales. Tomorrow you replace the single weight matrix with an MLP, add embeddings, and
    suddenly the model can see more than one character of context. That is the leap from bigram to language model.
    <span class="attr">&mdash; Day 8 Closing Principle</span>
  </div>

  <div class="notebook-card fade-in">
    <div class="notebook-card-hdr">
      <span class="nb-title">Day 8 Notebook â€” Training Loops, Loss Functions & Splits</span>
      <span class="nb-badge">Runnable Python</span>
    </div>
    <div class="notebook-card-body">
      <p>Neural bigram with cross-entropy, L2 regularization, train/dev/test splits, and comparison to count-based model.</p>
      <div class="notebook-links">
        <a href="https://colab.research.google.com/github/OmkarRayAI/omkarray/blob/main/notebooks/llm_day08_training_loops.ipynb" target="_blank" class="nb-colab">&#9654; Open in Colab</a>
        <a href="https://github.com/OmkarRayAI/omkarray/blob/main/notebooks/llm_day08_training_loops.ipynb" target="_blank" class="nb-github">View on GitHub</a>
        <a href="https://nbviewer.org/github/OmkarRayAI/omkarray/blob/main/notebooks/llm_day08_training_loops.ipynb" target="_blank" class="nb-nbviewer">nbviewer</a>
      </div>
    </div>
  </div>
</main>

<footer class="footer">
  <span>RAG &amp; LLM Engineer &middot; 80-Day Plan</span>
  <span>Day 8 of 80 &middot; Phase 1: Foundations</span>
  <span>&larr; <a href="llm-day7.html" style="color:inherit;">Day 7</a> &nbsp;|&nbsp; &rarr; Day 9: MLP Language Model</span>
</footer>
</body>
</html>