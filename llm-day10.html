<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Day 10 &mdash; Embeddings, Learning Rate Schedules &amp; Hyperparameters</title>
<link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;1,400&family=Source+Serif+4:ital,wght@0,300;0,400;0,600;1,300;1,400&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
<style>
:root{--ink:#1a1a2e;--cream:#faf8f4;--cream-dark:#f0ece4;--accent:#c0392b;--blue:#2c3e6b;--green:#27654a;--gold:#d4a843;--gray:#7f8c8d;--gray-light:#bdc3c7;--purple:#5b2c6f;--orange:#d35400;}
*{margin:0;padding:0;box-sizing:border-box;}
body{background:var(--cream);color:var(--ink);font-family:'Source Serif 4',Georgia,serif;line-height:1.7;}
.progress-strip{position:sticky;top:0;z-index:100;background:var(--ink);padding:0.6rem 2rem;display:flex;align-items:center;gap:0.4rem;font-family:'JetBrains Mono',monospace;font-size:0.65rem;color:#888;}
.p-dot{width:8px;height:8px;border-radius:50%;border:1.5px solid #555;flex-shrink:0;}
.p-dot.done{background:var(--green);border-color:var(--green);}
.p-dot.now{background:var(--accent);border-color:var(--accent);box-shadow:0 0 6px rgba(192,57,43,0.5);}
.p-line{width:24px;height:1.5px;background:#444;flex-shrink:0;}.p-line.done{background:var(--green);}
.p-label{color:#666;margin-left:0.3rem;margin-right:0.5rem;}.p-label.done{color:var(--green);}.p-label.now{color:var(--accent);font-weight:600;}
.back-link{display:block;font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:.18em;text-transform:uppercase;color:var(--gray);text-decoration:none;padding:1.2rem 2rem 0;max-width:1100px;margin:0 auto;transition:color .2s;}.back-link:hover{color:var(--accent);}
.header{padding:2rem 2rem 2rem;max-width:1100px;margin:0 auto;border-bottom:3px double var(--ink);}
.header-meta{font-family:'JetBrains Mono',monospace;font-size:0.72rem;letter-spacing:0.15em;text-transform:uppercase;color:var(--gray);margin-bottom:0.5rem;}
.phase-tag{background:var(--ink);color:white;padding:0.15rem 0.5rem;border-radius:2px;font-size:0.62rem;margin-right:0.4rem;}
.header h1{font-family:'Playfair Display',serif;font-size:clamp(2rem,5vw,3.1rem);font-weight:700;line-height:1.15;margin-bottom:0.5rem;}
.header h1 span{color:var(--accent);}
.header-sub{font-style:italic;font-size:1.05rem;color:var(--blue);max-width:700px;}
.content{max-width:1100px;margin:0 auto;padding:2rem 2rem 3rem;}
.memo-quote{background:var(--cream-dark);border-left:4px solid var(--accent);padding:1.5rem 2rem;margin:0 0 3rem;font-style:italic;font-size:1.02rem;color:var(--blue);position:relative;}
.memo-quote::before{content:'"\27';font-family:'Playfair Display',serif;font-size:4rem;color:var(--accent);position:absolute;top:-0.5rem;left:0.5rem;opacity:0.25;}
.memo-quote .attr{display:block;margin-top:0.6rem;font-style:normal;font-size:0.8rem;color:var(--gray);font-family:'JetBrains Mono',monospace;}
.sh{font-family:'Playfair Display',serif;font-size:1.55rem;font-weight:700;margin:3rem 0 1.25rem;padding-bottom:0.5rem;border-bottom:1px solid var(--gray-light);}
.sh .n{color:var(--accent);font-style:italic;margin-right:0.2rem;}
.prose{font-size:1rem;max-width:720px;margin-bottom:1.25rem;}
.prose strong{color:var(--blue);}
.prose code,code{font-family:'JetBrains Mono',monospace;font-size:0.83rem;background:var(--cream-dark);padding:0.1rem 0.35rem;border-radius:2px;color:var(--accent);}
.vf{background:white;border:1px solid var(--gray-light);border-radius:2px;padding:2rem 1.5rem 1.5rem;margin:1.5rem 0 2rem;position:relative;overflow:hidden;}
.vf::before{content:'';position:absolute;top:0;left:0;right:0;height:3px;}
.vf.green::before{background:linear-gradient(90deg,var(--green),var(--blue));}
.vf.gold::before{background:linear-gradient(90deg,var(--gold),var(--accent));}
.vf.purple::before{background:linear-gradient(90deg,var(--purple),var(--blue));}
.vf-label{font-family:'JetBrains Mono',monospace;font-size:0.68rem;letter-spacing:0.18em;text-transform:uppercase;color:var(--gray);margin-bottom:1rem;}
svg text{font-family:'Source Serif 4',Georgia,serif;}
svg .m{font-family:'JetBrains Mono',monospace;}
.box{border-radius:2px;padding:1.4rem 1.8rem;margin:2rem 0;position:relative;}
.box::before{position:absolute;top:-0.75rem;left:1rem;font-size:1.2rem;background:var(--cream);padding:0 0.5rem;}
.box h4{font-family:'Playfair Display',serif;margin-bottom:0.4rem;font-size:1rem;}
.box p{font-size:0.93rem;color:#555;}
.box.insight{background:linear-gradient(135deg,#fdf6e8,#fef9f0);border:1px solid var(--gold);}.box.insight::before{content:'\1F4A1';}.box.insight h4{color:var(--gold);}
.box.danger{background:linear-gradient(135deg,#fdf0ef,#fef5f4);border:1px solid var(--accent);}.box.danger::before{content:'\26A0\FE0F';}.box.danger h4{color:var(--accent);}
.code-block{background:#1e1e2e;color:#cdd6f4;border-radius:4px;padding:1.5rem;margin:1.5rem 0;overflow-x:auto;font-family:'JetBrains Mono',monospace;font-size:0.82rem;line-height:1.8;}
.code-block .comment{color:#6c7086;}.code-block .keyword{color:#cba6f7;}.code-block .string{color:#a6e3a1;}.code-block .number{color:#fab387;}.code-block .func{color:#89b4fa;}.code-block .class-name{color:#f9e2af;}.code-block .op{color:#89dceb;}.code-block .self{color:#f38ba8;}
.mx{display:grid;grid-template-columns:auto 1fr 1fr;grid-template-rows:auto 1fr 1fr;gap:0;margin:2rem 0;background:white;border:1px solid var(--gray-light);border-radius:2px;overflow:hidden;}
.mx-corner{background:var(--ink);padding:0.8rem;}
.mx-ch{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;text-align:center;display:flex;align-items:center;justify-content:center;}
.mx-rh{background:var(--ink);color:white;padding:0.8rem;font-family:'JetBrains Mono',monospace;font-size:0.7rem;text-transform:uppercase;letter-spacing:0.1em;writing-mode:vertical-lr;text-orientation:mixed;transform:rotate(180deg);display:flex;align-items:center;justify-content:center;}
.mx-cell{padding:1.2rem;border:1px solid var(--cream-dark);}.mx-cell h4{font-family:'Playfair Display',serif;font-size:0.95rem;margin-bottom:0.4rem;}.mx-cell p{font-size:0.83rem;color:#555;line-height:1.5;}.mx-cell.best{background:#f0faf4;}.mx-cell.worst{background:#fdf0ef;}.mx-cell .e{font-size:1.4rem;display:block;margin-bottom:0.4rem;}
.mx-cell code{font-family:'JetBrains Mono',monospace;font-size:0.8rem;background:var(--cream-dark);padding:0.1rem 0.3rem;border-radius:2px;color:var(--accent);}
.cl{list-style:none;margin:1.5rem 0;}.cl li{padding:0.55rem 0 0.55rem 2rem;position:relative;font-size:0.93rem;border-bottom:1px dotted var(--gray-light);}.cl li::before{content:'\2610';position:absolute;left:0;color:var(--accent);font-size:1.1rem;}.cl li strong{font-family:'JetBrains Mono',monospace;font-size:0.8rem;color:var(--blue);}
.footer{max-width:1100px;margin:0 auto;padding:2rem;border-top:3px double var(--ink);display:flex;justify-content:space-between;align-items:center;font-family:'JetBrains Mono',monospace;font-size:0.68rem;color:var(--gray);text-transform:uppercase;letter-spacing:0.1em;flex-wrap:wrap;gap:0.5rem;}
.fi{opacity:0;transform:translateY(16px);animation:fu 0.5s ease forwards;}
@keyframes fu{to{opacity:1;transform:translateY(0);}}
.fi:nth-child(2){animation-delay:0.08s;}.fi:nth-child(3){animation-delay:0.16s;}.fi:nth-child(4){animation-delay:0.24s;}
@media(max-width:600px){.header{padding:1.5rem 1rem;}.content{padding:1.5rem 1rem;}.footer{flex-direction:column;text-align:center;}}
</style>
</head>
<body>

<div class="progress-strip">
  <span class="p-dot done"></span><span class="p-label done">1-9</span><span class="p-line done"></span>
  <span class="p-dot now"></span><span class="p-label now">10</span><span class="p-line"></span>
  <span class="p-dot"></span><span class="p-label">11</span>
  <span style="color:#555;margin-left:0.5rem;">&middot; &middot; &middot;</span>
  <span class="p-dot" style="margin-left:0.5rem;"></span><span class="p-label">80</span>
</div>

<a href="index.html" class="back-link">&larr; Back to index</a>

<header class="header fi">
  <div class="header-meta"><span class="phase-tag">PHASE 1</span> Foundations &middot; Day 10 of 80 &middot; Neural Networks &amp; Backprop</div>
  <h1>Embeddings, <span>Learning Rate</span> Schedules &amp; Hyperparameters</h1>
  <p class="header-sub">Squeeze the last drop of performance from the MLP: tune embedding dimension, hidden size, learning rate decay, and block size. Phase 1 finale.</p>
</header>

<main class="content">

  <div class="memo-quote fi">
    The difference between a good fund and a great fund is not the strategy &mdash; it is the calibration.
    Position sizing, rebalancing frequency, risk limits. In neural networks, the equivalent calibrations are
    hyperparameters: embedding dimension, hidden layer size, learning rate schedule, context length. Today you
    learn to tune them systematically.
    <span class="attr">&mdash; Day 10 Principle, adapted from the Marks framework</span>
  </div>

  <h2 class="sh fi"><span class="n">I.</span> Learning Rate Finder &mdash; The Log-Space Sweep</h2>
  <p class="prose fi">
    Before training, sweep the learning rate from 10<sup>&minus;3</sup> to 10<sup>0</sup> on a log scale.
    Plot loss vs. learning rate. The optimal initial LR is just before the loss starts increasing &mdash;
    typically where the curve is steepest downward. This 30-second experiment saves hours of wasted training.
  </p>

  <div class="fi">
    <div class="vf gold">
      <div class="vf-label">Exhibit A &mdash; Learning Rate Finder: Loss vs. LR (Log Scale)</div>
      <svg viewBox="0 0 700 200" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
        <line x1="60" y1="20" x2="60" y2="170" stroke="#888" stroke-width="1"/>
        <line x1="60" y1="170" x2="660" y2="170" stroke="#888" stroke-width="1"/>
        <text x="30" y="95" text-anchor="middle" font-size="10" fill="#888" transform="rotate(-90,30,95)">LOSS</text>
        <text x="360" y="195" text-anchor="middle" class="m" font-size="9" fill="#888">LEARNING RATE (log scale)</text>

        <text x="60" y="185" class="m" font-size="7" fill="#aaa">10&sup3;</text>
        <text x="260" y="185" class="m" font-size="7" fill="#aaa">10&sup2;</text>
        <text x="460" y="185" class="m" font-size="7" fill="#aaa">10&sup1;</text>
        <text x="620" y="185" class="m" font-size="7" fill="#aaa">10&sup0;</text>

        <path d="M60,40 Q130,38 200,35 T350,80 T460,100 T520,90" fill="none" stroke="#2c3e6b" stroke-width="2"/>
        <path d="M520,90 Q560,100 600,140 T660,160" fill="none" stroke="#c0392b" stroke-width="2" stroke-dasharray="5"/>

        <line x1="460" y1="100" x2="460" y2="170" stroke="#27654a" stroke-width="1.5" stroke-dasharray="4"/>
        <circle cx="460" cy="100" r="5" fill="#27654a"/>
        <text x="460" y="85" text-anchor="middle" class="m" font-size="9" fill="#27654a" font-weight="600">SWEET SPOT</text>
        <text x="460" y="70" text-anchor="middle" class="m" font-size="8" fill="#27654a">lr &asymp; 0.1</text>

        <text x="600" y="130" class="m" font-size="8" fill="#c0392b">divergence</text>
        <text x="150" y="60" class="m" font-size="8" fill="#888">too slow</text>
      </svg>
    </div>
  </div>

  <div class="code-block fi">
<span class="comment"># Learning rate finder</span>
lre = torch.<span class="func">linspace</span>(<span class="number">-3</span>, <span class="number">0</span>, <span class="number">1000</span>)
lrs = <span class="number">10</span><span class="op">**</span>lre

<span class="keyword">for</span> i <span class="keyword">in</span> <span class="func">range</span>(<span class="number">1000</span>):
    <span class="comment"># ... forward pass, loss computation ...</span>
    loss.<span class="func">backward</span>()
    lr = lrs[i]
    <span class="keyword">for</span> p <span class="keyword">in</span> parameters:
        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad
    lri.<span class="func">append</span>(lre[i])
    lossi.<span class="func">append</span>(loss.<span class="func">item</span>())

<span class="comment"># Plot: plt.plot(lri, lossi) &mdash; pick lr where curve is steepest</span>
  </div>

  <h2 class="sh fi"><span class="n">II.</span> Hyperparameter Sweep &mdash; Embedding &amp; Hidden Dimensions</h2>
  <p class="prose fi">
    The key hyperparameters for the MLP language model are: <strong>embedding dimension</strong> (how many features
    per character), <strong>hidden layer size</strong> (capacity of the nonlinear transform), <strong>block size</strong>
    (how many previous characters to look at), and <strong>batch size</strong> (samples per gradient step).
  </p>

  <div class="fi" style="overflow-x:auto;">
    <table style="width:100%;border-collapse:collapse;font-family:'JetBrains Mono',monospace;font-size:0.82rem;margin:1.5rem 0;">
      <thead>
        <tr style="border-bottom:2px solid var(--ink);">
          <th style="padding:0.6rem;text-align:left;color:var(--blue);">Config</th>
          <th style="padding:0.6rem;text-align:center;">emb_dim</th>
          <th style="padding:0.6rem;text-align:center;">hidden</th>
          <th style="padding:0.6rem;text-align:center;">block</th>
          <th style="padding:0.6rem;text-align:center;">params</th>
          <th style="padding:0.6rem;text-align:center;color:var(--green);">dev NLL</th>
        </tr>
      </thead>
      <tbody>
        <tr style="border-bottom:1px solid var(--cream-dark);">
          <td style="padding:0.6rem;">Baseline</td>
          <td style="padding:0.6rem;text-align:center;">2</td>
          <td style="padding:0.6rem;text-align:center;">100</td>
          <td style="padding:0.6rem;text-align:center;">3</td>
          <td style="padding:0.6rem;text-align:center;">3.5K</td>
          <td style="padding:0.6rem;text-align:center;color:var(--accent);">2.17</td>
        </tr>
        <tr style="border-bottom:1px solid var(--cream-dark);">
          <td style="padding:0.6rem;">Wider embed</td>
          <td style="padding:0.6rem;text-align:center;font-weight:600;">10</td>
          <td style="padding:0.6rem;text-align:center;">200</td>
          <td style="padding:0.6rem;text-align:center;">3</td>
          <td style="padding:0.6rem;text-align:center;">11.7K</td>
          <td style="padding:0.6rem;text-align:center;color:var(--green);">2.08</td>
        </tr>
        <tr style="border-bottom:1px solid var(--cream-dark);">
          <td style="padding:0.6rem;">Longer context</td>
          <td style="padding:0.6rem;text-align:center;">10</td>
          <td style="padding:0.6rem;text-align:center;">200</td>
          <td style="padding:0.6rem;text-align:center;font-weight:600;">8</td>
          <td style="padding:0.6rem;text-align:center;">22K</td>
          <td style="padding:0.6rem;text-align:center;color:var(--green);font-weight:600;">2.03</td>
        </tr>
        <tr style="border-bottom:1px solid var(--cream-dark);">
          <td style="padding:0.6rem;">Overfitting</td>
          <td style="padding:0.6rem;text-align:center;">30</td>
          <td style="padding:0.6rem;text-align:center;">500</td>
          <td style="padding:0.6rem;text-align:center;">8</td>
          <td style="padding:0.6rem;text-align:center;">140K</td>
          <td style="padding:0.6rem;text-align:center;color:var(--accent);">2.10 &#8593;</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="box insight fi">
    <h4>The Overfitting Signal</h4>
    <p>When train loss keeps dropping but dev loss starts rising, the model is memorizing training data rather than learning general patterns. The gap between train and dev loss is the overfitting signal. The &ldquo;best&rdquo; model is the one with the lowest <em>dev</em> loss, not the lowest train loss.</p>
  </div>

  <h2 class="sh fi"><span class="n">III.</span> Learning Rate Decay &mdash; Start Fast, Finish Precise</h2>
  <p class="prose fi">
    The standard pattern: start with a high learning rate (0.1) for fast initial learning, then decay to a lower
    rate (0.01) for fine convergence. In practice, this is implemented as a simple step schedule or the more
    sophisticated cosine annealing that modern LLMs use.
  </p>

  <div class="code-block fi">
<span class="comment"># Step decay: high lr for first 100k steps, low lr for rest</span>
<span class="keyword">for</span> i <span class="keyword">in</span> <span class="func">range</span>(<span class="number">200000</span>):
    <span class="comment"># ... forward + backward ...</span>
    lr = <span class="number">0.1</span> <span class="keyword">if</span> i &lt; <span class="number">100000</span> <span class="keyword">else</span> <span class="number">0.01</span>
    <span class="keyword">for</span> p <span class="keyword">in</span> parameters:
        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad

<span class="comment"># The pattern in modern LLMs: cosine with warmup</span>
<span class="comment"># lr = max_lr * 0.5 * (1 + cos(pi * step / total_steps))</span>
  </div>

  <h2 class="sh fi"><span class="n">IV.</span> The Matrix &mdash; What Matters Today</h2>
  <div class="mx fi">
    <div class="mx-corner"></div>
    <div class="mx-ch">Builds Deep Intuition</div>
    <div class="mx-ch">Surface-Level Only</div>
    <div class="mx-rh">Quick to Do</div>
    <div class="mx-cell best">
      <span class="e">&#127919;</span>
      <h4>DO FIRST</h4>
      <p>Run the LR finder. Find the optimal initial learning rate. Implement step decay. Beat your Day 9 result.</p>
    </div>
    <div class="mx-cell">
      <span class="e">&#9197;&#65039;</span>
      <h4>DO IF TIME</h4>
      <p>Try block_size = 4, 5, 8. More context helps &mdash; but at some point the hidden layer becomes the bottleneck.</p>
    </div>
    <div class="mx-rh">Slow but Worth It</div>
    <div class="mx-cell">
      <span class="e">&#128400;</span>
      <h4>DO CAREFULLY</h4>
      <p>Sweep emb_dim &times; hidden_size. Track train vs. dev loss for each. Find the configuration with lowest dev loss.</p>
    </div>
    <div class="mx-cell worst">
      <span class="e">&#128683;</span>
      <h4>AVOID TODAY</h4>
      <p>Automated hyperparameter search (Optuna, Ray Tune). Do it manually &mdash; build intuition for how each knob affects the model.</p>
    </div>
  </div>

  <h2 class="sh fi"><span class="n">V.</span> Today&rsquo;s Deliverables</h2>
  <ul class="cl fi">
    <li><strong>LR finder:</strong> Sweep 10<sup>&minus;3</sup> to 10<sup>0</sup>, plot loss vs. LR, identify sweet spot</li>
    <li><strong>LR schedule:</strong> Implement step decay (0.1 &rarr; 0.01 at midpoint)</li>
    <li><strong>Hyperparameter sweep:</strong> Test at least 4 configurations of emb_dim &times; hidden_size</li>
    <li><strong>Best model:</strong> Achieve the lowest possible dev NLL (&lt;2.10 target)</li>
    <li><strong>Overfitting check:</strong> Identify one configuration where dev loss &gt; train loss by &gt;0.05</li>
    <li><strong>Phase 1 summary:</strong> Log all NLL baselines: bigram (2.45), MLP baseline (2.17), tuned MLP (your best)</li>
  </ul>

  <div class="memo-quote fi" style="margin-top:3rem;">
    Phase 1 is complete. You built an autograd engine, a neuron, an MLP, a bigram model, and a neural language
    model &mdash; all from scratch. You understand backprop, gradient descent, softmax, cross-entropy, embeddings,
    and data splits at a level that most practitioners never reach. Phase 2 begins with the hard parts:
    <strong>activations, batch normalization, and deep network training</strong>. The foundation you&rsquo;ve built will hold.
    <span class="attr">&mdash; Day 10 Closing Principle &middot; End of Phase 1</span>
  </div>

</main>

<footer class="footer">
  <span>RAG &amp; LLM Engineer &middot; 80-Day Plan</span>
  <span>Day 10 of 80 &middot; Phase 1: Foundations (Final Day)</span>
  <span>&larr; <a href="llm-day9.html" style="color:inherit;">Day 9</a> &nbsp;|&nbsp; &rarr; Day 11: Activations &amp; Gradients</span>
</footer>
</body>
</html>